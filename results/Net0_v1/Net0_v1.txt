     
 ID: Net0_v1 
 parameter number: 8225600
 infomation: baseline -- extract feature then stack 
 Epoch number: 20 
 Batch size: 2 
 =======================

Net_0(
  (feature_extract): FeatureNet(
    (net): Sequential(
      (0): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (1): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (2): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (3): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (4): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (5): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
    )
  )
  (disp_extract): DispNet(
    (net): Sequential(
      (0): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (1): ConvTranspose2d_block(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (2): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (3): ConvTranspose2d_block(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (4): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (5): ConvTranspose2d_block(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (6): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): Softmax()
    )
  )
  (regression): DisparityRegression()
) batch loss: 94.97442626953125 
 batch loss: 94.02816009521484 
 batch loss: 91.8077163696289 
 batch loss: 89.20056915283203 
 batch loss: 88.62312316894531 
 batch loss: 85.37738800048828 
 batch loss: 83.75454711914062 
 batch loss: 82.40404510498047 
 batch loss: 82.2428207397461 
 batch loss: 80.38459014892578 
 batch loss: 79.69247436523438 
 batch loss: 78.56049346923828 
 batch loss: 78.8958740234375 
 batch loss: 78.31785583496094 
 batch loss: 77.8250503540039 
 batch loss: 77.31515502929688 
 batch loss: 77.6176986694336 
 batch loss: 77.09513092041016 
 batch loss: 76.97146606445312 
 batch loss: 76.39201354980469 
 batch loss: 76.04978942871094 
 batch loss: 76.86386108398438 
 batch loss: 75.7059097290039 
 batch loss: 75.7063217163086 
 batch loss: 75.57013702392578 
 batch loss: 75.38050842285156 
 batch loss: 75.18601989746094 
 batch loss: 74.9913330078125 
 batch loss: 74.87987518310547 
 batch loss: 74.98384094238281 
 batch loss: 74.60623931884766 
 batch loss: 74.83134460449219 
 batch loss: 74.42784881591797 
 batch loss: 74.28230285644531 
 batch loss: 74.9172134399414 
 batch loss: 74.0426025390625 
 batch loss: 74.09951782226562 
 batch loss: 73.70936584472656 
 batch loss: 73.71656799316406 
 batch loss: 73.69230651855469 
 batch loss: 73.55049896240234 
 batch loss: 73.22371673583984 
 batch loss: 73.14615631103516 
 batch loss: 73.10740661621094 
 batch loss: 73.20685577392578 
 batch loss: 72.79523468017578 
 batch loss: 72.91357421875 
 batch loss: 72.60166931152344 
 batch loss: 72.69335174560547 
 batch loss: 72.44007110595703 
 batch loss: 72.42859649658203 
 batch loss: 72.15457916259766 
 batch loss: 72.00171661376953 
 batch loss: 71.9711685180664 
 batch loss: 71.69376373291016 
 batch loss: 71.80388641357422 
 batch loss: 71.68109130859375 
 batch loss: 71.39241790771484 
 batch loss: 71.33512115478516 
 batch loss: 71.15580749511719 
 batch loss: 71.28577423095703 
 batch loss: 71.12257385253906 
 batch loss: 70.91227722167969 
 batch loss: 70.85548400878906 
 batch loss: 70.71183776855469 
 batch loss: 70.5463638305664 
 batch loss: 70.46246337890625 
 batch loss: 70.3524398803711 
 batch loss: 70.14263153076172 
 batch loss: 70.0377197265625 
 batch loss: 69.96780395507812 
 batch loss: 69.74549865722656 
 batch loss: 69.73792266845703 
 batch loss: 69.71344757080078 
 batch loss: 69.45443725585938 
 batch loss: 69.29530334472656 
 batch loss: 69.36237335205078 
 batch loss: 69.08750915527344 
 batch loss: 68.95532989501953 
 batch loss: 68.8565444946289 
 batch loss: 68.82689666748047 
 batch loss: 68.77930450439453 
 batch loss: 68.84501647949219 
 batch loss: 68.79574584960938 
 batch loss: 68.78460693359375 
 batch loss: 68.77374267578125 
 batch loss: 68.73871612548828 
 batch loss: 68.69470977783203 
 batch loss: 68.7515869140625 
 batch loss: 68.8578109741211 
 batch loss: 68.74748229980469 
 batch loss: 68.75505065917969 
 batch loss: 68.7603988647461 
 batch loss: 68.77723693847656 
 batch loss: 68.7960433959961 
 batch loss: 68.78126525878906 
 batch loss: 68.91229248046875 
 batch loss: 68.71340942382812 
 batch loss: 68.6723403930664 
 batch loss: 68.7560043334961 
 Epoch 0 |Train loss: 74.96282405853272 |Validation loss: 68.77598304748535 
 batch loss: 68.7877426147461 
 batch loss: 68.73194885253906 
 batch loss: 68.52601623535156 
 batch loss: 68.44454193115234 
 batch loss: 68.24249267578125 
 batch loss: 68.22195434570312 
 batch loss: 67.99397277832031 
 batch loss: 67.98194122314453 
 batch loss: 67.75676727294922 
 batch loss: 67.71315002441406 
 batch loss: 67.65499877929688 
 batch loss: 67.43537139892578 
 batch loss: 67.38540649414062 
 batch loss: 67.22523498535156 
 batch loss: 67.12889099121094 
 batch loss: 67.0569076538086 
 batch loss: 66.88682556152344 
 batch loss: 66.77626037597656 
 batch loss: 66.68386840820312 
 batch loss: 66.61813354492188 
 batch loss: 66.46327209472656 
 batch loss: 66.49906158447266 
 batch loss: 66.31755065917969 
 batch loss: 66.1639633178711 
 batch loss: 66.13922119140625 
 batch loss: 65.9175796508789 
 batch loss: 65.7778549194336 
 batch loss: 65.60392761230469 
 batch loss: 65.51909637451172 
 batch loss: 65.46424102783203 
 batch loss: 65.32332611083984 
 batch loss: 65.1183090209961 
 batch loss: 65.02070617675781 
 batch loss: 64.8446273803711 
 batch loss: 64.71076965332031 
 batch loss: 64.71971130371094 
 batch loss: 64.47038269042969 
 batch loss: 64.28194427490234 
 batch loss: 64.13336944580078 
 batch loss: 63.96942138671875 
 batch loss: 63.752540588378906 
 batch loss: 63.72085189819336 
 batch loss: 63.51665496826172 
 batch loss: 63.42272186279297 
 batch loss: 63.21525955200195 
 batch loss: 64.00178527832031 
 batch loss: 63.181060791015625 
 batch loss: 62.943458557128906 
 batch loss: 62.7125358581543 
 batch loss: 62.58357238769531 
 batch loss: 62.47087097167969 
 batch loss: 62.22898864746094 
 batch loss: 62.009586334228516 
 batch loss: 61.87458038330078 
 batch loss: 61.6683349609375 
 batch loss: 62.013790130615234 
 batch loss: 61.51766586303711 
 batch loss: 61.422142028808594 
 batch loss: 61.31859588623047 
 batch loss: 61.15433120727539 
 batch loss: 61.14137268066406 
 batch loss: 60.97181701660156 
 batch loss: 60.74314880371094 
 batch loss: 60.69007110595703 
 batch loss: 60.56884002685547 
 batch loss: 60.28146743774414 
 batch loss: 60.36189651489258 
 batch loss: 60.164405822753906 
 batch loss: 59.93347930908203 
 batch loss: 59.87614059448242 
 batch loss: 59.739288330078125 
 batch loss: 59.559085845947266 
 batch loss: 59.477622985839844 
 batch loss: 59.32792663574219 
 batch loss: 59.66687774658203 
 batch loss: 59.052268981933594 
 batch loss: 59.03644943237305 
 batch loss: 58.96531677246094 
 batch loss: 58.78584289550781 
 batch loss: 58.75208282470703 
 batch loss: 58.607120513916016 
 batch loss: 58.61679458618164 
 batch loss: 58.77960205078125 
 batch loss: 58.80105209350586 
 batch loss: 58.512939453125 
 batch loss: 58.77124786376953 
 batch loss: 58.62760543823242 
 batch loss: 58.64159393310547 
 batch loss: 58.55781936645508 
 batch loss: 58.823856353759766 
 batch loss: 58.77553176879883 
 batch loss: 58.559173583984375 
 batch loss: 58.51525115966797 
 batch loss: 58.56144714355469 
 batch loss: 58.61872482299805 
 batch loss: 58.57187271118164 
 batch loss: 58.62619400024414 
 batch loss: 58.58525848388672 
 batch loss: 58.56926727294922 
 batch loss: 58.54352569580078 
 Epoch 1 |Train loss: 63.844193649291995 |Validation loss: 58.63329391479492 
 batch loss: 58.537044525146484 
 batch loss: 58.67741394042969 
 batch loss: 58.43757247924805 
 batch loss: 58.318241119384766 
 batch loss: 58.26375961303711 
 batch loss: 58.18009567260742 
 batch loss: 58.29100799560547 
 batch loss: 57.898460388183594 
 batch loss: 57.7697639465332 
 batch loss: 57.79203414916992 
 batch loss: 57.584224700927734 
 batch loss: 57.43132781982422 
 batch loss: 57.303436279296875 
 batch loss: 57.30096435546875 
 batch loss: 57.07341766357422 
 batch loss: 57.091182708740234 
 batch loss: 57.013694763183594 
 batch loss: 56.84481430053711 
 batch loss: 56.70117950439453 
 batch loss: 56.72871780395508 
 batch loss: 56.81991195678711 
 batch loss: 56.400367736816406 
 batch loss: 56.397918701171875 
 batch loss: 56.3482780456543 
 batch loss: 56.29166793823242 
 batch loss: 55.9212760925293 
 batch loss: 55.81252670288086 
 batch loss: 55.78266143798828 
 batch loss: 55.558143615722656 
 batch loss: 55.299537658691406 
 batch loss: 55.13774108886719 
 batch loss: 55.055973052978516 
 batch loss: 54.86652755737305 
 batch loss: 54.69514083862305 
 batch loss: 54.50932693481445 
 batch loss: 54.394691467285156 
 batch loss: 55.14312744140625 
 batch loss: 54.24419021606445 
 batch loss: 54.08816909790039 
 batch loss: 54.093631744384766 
 batch loss: 53.76879119873047 
 batch loss: 53.66730880737305 
 batch loss: 53.470863342285156 
 batch loss: 53.40754318237305 
 batch loss: 53.531368255615234 
 batch loss: 53.26432800292969 
 batch loss: 53.5009880065918 
 batch loss: 53.155338287353516 
 batch loss: 53.082672119140625 
 batch loss: 52.9029655456543 
 batch loss: 52.863319396972656 
 batch loss: 52.7143440246582 
 batch loss: 52.79713821411133 
 batch loss: 52.51249694824219 
 batch loss: 52.36901092529297 
 batch loss: 52.39338684082031 
 batch loss: 52.2750358581543 
 batch loss: 52.15947341918945 
 batch loss: 51.95872497558594 
 batch loss: 51.97181701660156 
 batch loss: 51.82727813720703 
 batch loss: 51.74284744262695 
 batch loss: 51.54341506958008 
 batch loss: 51.583377838134766 
 batch loss: 51.65924072265625 
 batch loss: 51.283607482910156 
 batch loss: 52.120819091796875 
 batch loss: 51.34052276611328 
 batch loss: 51.847084045410156 
 batch loss: 51.21760559082031 
 batch loss: 51.167293548583984 
 batch loss: 51.17549133300781 
 batch loss: 51.038814544677734 
 batch loss: 50.88634490966797 
 batch loss: 50.95561981201172 
 batch loss: 50.62303924560547 
 batch loss: 50.649627685546875 
 batch loss: 50.4273796081543 
 batch loss: 50.603271484375 
 batch loss: 50.27467727661133 
 batch loss: 50.32685089111328 
 batch loss: 50.52943420410156 
 batch loss: 50.36528778076172 
 batch loss: 50.54765319824219 
 batch loss: 50.35266876220703 
 batch loss: 50.42047882080078 
 batch loss: 50.279029846191406 
 batch loss: 50.17367172241211 
 batch loss: 50.176517486572266 
 batch loss: 50.61703872680664 
 batch loss: 50.36216354370117 
 batch loss: 50.281436920166016 
 batch loss: 50.34285354614258 
 batch loss: 50.29243087768555 
 batch loss: 50.6705207824707 
 batch loss: 50.29106903076172 
 batch loss: 50.30221176147461 
 batch loss: 50.21699523925781 
 batch loss: 50.25278854370117 
 batch loss: 50.21747589111328 
 Epoch 2 |Train loss: 54.272917938232425 |Validation loss: 50.35092887878418 
 batch loss: 50.263797760009766 
 batch loss: 50.505611419677734 
 batch loss: 50.211891174316406 
 batch loss: 50.03062057495117 
 batch loss: 50.003021240234375 
 batch loss: 49.832401275634766 
 batch loss: 49.781124114990234 
 batch loss: 49.71711730957031 
 batch loss: 49.67376708984375 
 batch loss: 49.472408294677734 
 batch loss: 49.51273727416992 
 batch loss: 49.5543327331543 
 batch loss: 49.24500274658203 
 batch loss: 49.365474700927734 
 batch loss: 49.083580017089844 
 batch loss: 49.262020111083984 
 batch loss: 49.20649337768555 
 batch loss: 48.79994201660156 
 batch loss: 48.883323669433594 
 batch loss: 48.94355392456055 
 batch loss: 48.84040832519531 
 batch loss: 48.68769073486328 
 batch loss: 48.66266632080078 
 batch loss: 48.70117950439453 
 batch loss: 48.39346694946289 
 batch loss: 48.68177032470703 
 batch loss: 48.1070442199707 
 batch loss: 48.23949432373047 
 batch loss: 48.198490142822266 
 batch loss: 48.00714111328125 
 batch loss: 47.91079330444336 
 batch loss: 47.72397994995117 
 batch loss: 47.745201110839844 
 batch loss: 47.761966705322266 
 batch loss: 47.538631439208984 
 batch loss: 47.49175262451172 
 batch loss: 47.45273971557617 
 batch loss: 47.50425720214844 
 batch loss: 47.29443359375 
 batch loss: 47.36654281616211 
 batch loss: 47.043907165527344 
 batch loss: 46.94123077392578 
 batch loss: 46.84233856201172 
 batch loss: 47.172359466552734 
 batch loss: 46.76140594482422 
 batch loss: 46.6027717590332 
 batch loss: 46.473812103271484 
 batch loss: 46.68145751953125 
 batch loss: 46.289939880371094 
 batch loss: 46.38023376464844 
 batch loss: 46.310604095458984 
 batch loss: 46.19217300415039 
 batch loss: 45.950260162353516 
 batch loss: 45.89304733276367 
 batch loss: 45.74802780151367 
 batch loss: 45.64872741699219 
 batch loss: 45.654354095458984 
 batch loss: 45.52851867675781 
 batch loss: 45.3021240234375 
 batch loss: 45.21876525878906 
 batch loss: 45.419700622558594 
 batch loss: 44.9890251159668 
 batch loss: 44.92500686645508 
 batch loss: 45.09016799926758 
 batch loss: 44.774417877197266 
 batch loss: 44.56472396850586 
 batch loss: 44.475582122802734 
 batch loss: 44.45988464355469 
 batch loss: 44.440250396728516 
 batch loss: 44.248809814453125 
 batch loss: 44.15291213989258 
 batch loss: 44.12197494506836 
 batch loss: 43.9569091796875 
 batch loss: 44.053863525390625 
 batch loss: 44.0649299621582 
 batch loss: 44.15760803222656 
 batch loss: 43.91825485229492 
 batch loss: 43.657196044921875 
 batch loss: 43.572566986083984 
 batch loss: 43.51068115234375 
 batch loss: 43.69126892089844 
 batch loss: 43.32669448852539 
 batch loss: 43.63127136230469 
 batch loss: 43.48792266845703 
 batch loss: 43.38206481933594 
 batch loss: 43.8394660949707 
 batch loss: 43.55630874633789 
 batch loss: 43.38362121582031 
 batch loss: 43.352027893066406 
 batch loss: 43.68264389038086 
 batch loss: 43.48838806152344 
 batch loss: 43.33506393432617 
 batch loss: 43.25667953491211 
 batch loss: 43.51366424560547 
 batch loss: 43.45689392089844 
 batch loss: 43.36650848388672 
 batch loss: 43.577205657958984 
 batch loss: 43.34794616699219 
 batch loss: 43.40977478027344 
 batch loss: 43.40510559082031 
 Epoch 3 |Train loss: 47.03560495376587 |Validation loss: 43.47452602386475 
 batch loss: 43.4048957824707 
 batch loss: 43.49835205078125 
 batch loss: 43.146663665771484 
 batch loss: 43.026031494140625 
 batch loss: 42.8555793762207 
 batch loss: 42.63010787963867 
 batch loss: 42.94261169433594 
 batch loss: 42.865970611572266 
 batch loss: 42.935386657714844 
 batch loss: 42.743019104003906 
 batch loss: 42.53956985473633 
 batch loss: 42.57447814941406 
 batch loss: 42.22886276245117 
 batch loss: 42.06925964355469 
 batch loss: 41.90271759033203 
 batch loss: 41.89567565917969 
 batch loss: 41.629844665527344 
 batch loss: 41.43678665161133 
 batch loss: 41.31985092163086 
 batch loss: 41.34258270263672 
 batch loss: 41.61917495727539 
 batch loss: 41.61819839477539 
 batch loss: 40.95558547973633 
 batch loss: 41.318721771240234 
 batch loss: 41.09963607788086 
 batch loss: 40.93041229248047 
 batch loss: 40.64226531982422 
 batch loss: 40.44188690185547 
 batch loss: 40.555419921875 
 batch loss: 40.04738998413086 
 batch loss: 40.09748458862305 
 batch loss: 39.948246002197266 
 batch loss: 39.75495910644531 
 batch loss: 39.652950286865234 
 batch loss: 39.49428176879883 
 batch loss: 39.81060028076172 
 batch loss: 39.400054931640625 
 batch loss: 39.66819381713867 
 batch loss: 39.221290588378906 
 batch loss: 39.2161750793457 
 batch loss: 39.20925521850586 
 batch loss: 38.98651123046875 
 batch loss: 38.8494758605957 
 batch loss: 39.18827438354492 
 batch loss: 38.6854248046875 
 batch loss: 38.54469299316406 
 batch loss: 38.5716667175293 
 batch loss: 38.50967788696289 
 batch loss: 38.38249969482422 
 batch loss: 38.47678756713867 
 batch loss: 38.23743438720703 
 batch loss: 38.0882682800293 
 batch loss: 38.01885986328125 
 batch loss: 37.95197677612305 
 batch loss: 38.04209518432617 
 batch loss: 37.98232650756836 
 batch loss: 37.681549072265625 
 batch loss: 38.23418426513672 
 batch loss: 37.568973541259766 
 batch loss: 37.63582992553711 
 batch loss: 37.3848762512207 
 batch loss: 37.53339385986328 
 batch loss: 37.37548828125 
 batch loss: 37.291160583496094 
 batch loss: 37.340824127197266 
 batch loss: 37.26006317138672 
 batch loss: 37.1118278503418 
 batch loss: 37.37468338012695 
 batch loss: 37.20878982543945 
 batch loss: 36.80447006225586 
 batch loss: 37.079715728759766 
 batch loss: 36.7020149230957 
 batch loss: 36.72658920288086 
 batch loss: 36.81104278564453 
 batch loss: 36.51137161254883 
 batch loss: 36.74214553833008 
 batch loss: 36.50330352783203 
 batch loss: 36.519683837890625 
 batch loss: 36.301780700683594 
 batch loss: 36.28181076049805 
 batch loss: 36.3000373840332 
 batch loss: 36.53290557861328 
 batch loss: 36.356544494628906 
 batch loss: 36.32710647583008 
 batch loss: 36.306785583496094 
 batch loss: 36.160640716552734 
 batch loss: 36.29265213012695 
 batch loss: 36.29936599731445 
 batch loss: 36.45466232299805 
 batch loss: 36.33134078979492 
 batch loss: 36.3392333984375 
 batch loss: 36.45478439331055 
 batch loss: 36.207130432128906 
 batch loss: 36.1906623840332 
 batch loss: 36.22505569458008 
 batch loss: 36.183998107910156 
 batch loss: 36.214115142822266 
 batch loss: 36.27156448364258 
 batch loss: 36.413761138916016 
 batch loss: 36.29673385620117 
 Epoch 4 |Train loss: 39.50239968299866 |Validation loss: 36.30795402526856 
 batch loss: 36.314998626708984 
 batch loss: 36.187686920166016 
 batch loss: 36.105865478515625 
 batch loss: 36.015933990478516 
 batch loss: 36.009979248046875 
 batch loss: 35.89537048339844 
 batch loss: 35.92168045043945 
 batch loss: 35.75768280029297 
 batch loss: 35.92961502075195 
 batch loss: 35.66905975341797 
 batch loss: 35.51324462890625 
 batch loss: 35.57052993774414 
 batch loss: 35.6359748840332 
 batch loss: 35.62854766845703 
 batch loss: 35.53921127319336 
 batch loss: 35.4083251953125 
 batch loss: 35.27860641479492 
 batch loss: 35.27723693847656 
 batch loss: 35.222679138183594 
 batch loss: 35.115257263183594 
 batch loss: 35.00233459472656 
 batch loss: 35.30122375488281 
 batch loss: 34.96504211425781 
 batch loss: 35.073883056640625 
 batch loss: 34.937496185302734 
 batch loss: 34.74940872192383 
 batch loss: 34.7612190246582 
 batch loss: 34.65864944458008 
 batch loss: 34.6823616027832 
 batch loss: 34.51984786987305 
 batch loss: 34.184661865234375 
 batch loss: 34.152198791503906 
 batch loss: 33.840328216552734 
 batch loss: 33.864952087402344 
 batch loss: 33.624183654785156 
 batch loss: 33.43185806274414 
 batch loss: 33.44245147705078 
 batch loss: 33.39622116088867 
 batch loss: 33.07188034057617 
 batch loss: 32.88789367675781 
 batch loss: 32.81382751464844 
 batch loss: 32.53499984741211 
 batch loss: 32.53886413574219 
 batch loss: 32.28470993041992 
 batch loss: 32.251075744628906 
 batch loss: 32.094722747802734 
 batch loss: 32.10913848876953 
 batch loss: 32.02042770385742 
 batch loss: 31.91442108154297 
 batch loss: 31.926429748535156 
 batch loss: 31.650676727294922 
 batch loss: 31.729249954223633 
 batch loss: 31.668781280517578 
 batch loss: 31.528841018676758 
 batch loss: 31.744909286499023 
 batch loss: 31.464609146118164 
 batch loss: 31.505529403686523 
 batch loss: 31.271459579467773 
 batch loss: 31.299846649169922 
 batch loss: 31.06576919555664 
 batch loss: 30.978151321411133 
 batch loss: 31.04273223876953 
 batch loss: 30.945171356201172 
 batch loss: 30.794435501098633 
 batch loss: 30.774065017700195 
 batch loss: 30.6340389251709 
 batch loss: 30.56661033630371 
 batch loss: 30.89873504638672 
 batch loss: 30.616371154785156 
 batch loss: 30.588552474975586 
 batch loss: 30.567569732666016 
 batch loss: 30.200658798217773 
 batch loss: 30.191999435424805 
 batch loss: 30.19373893737793 
 batch loss: 30.11624526977539 
 batch loss: 30.04090118408203 
 batch loss: 29.911771774291992 
 batch loss: 29.939693450927734 
 batch loss: 30.107463836669922 
 batch loss: 29.949249267578125 
 batch loss: 29.76703453063965 
 batch loss: 29.72016143798828 
 batch loss: 29.69151496887207 
 batch loss: 29.650381088256836 
 batch loss: 30.00579833984375 
 batch loss: 29.748703002929688 
 batch loss: 29.711721420288086 
 batch loss: 29.6140079498291 
 batch loss: 29.7852840423584 
 batch loss: 30.07915496826172 
 batch loss: 29.791343688964844 
 batch loss: 29.681781768798828 
 batch loss: 29.804201126098633 
 batch loss: 29.601545333862305 
 batch loss: 29.79400634765625 
 batch loss: 29.61033821105957 
 batch loss: 29.72589874267578 
 batch loss: 29.635711669921875 
 batch loss: 29.695178985595703 
 batch loss: 29.626251220703125 
 Epoch 5 |Train loss: 33.06277532577515 |Validation loss: 29.737000942230225 
 batch loss: 29.801097869873047 
 batch loss: 29.658510208129883 
 batch loss: 29.589433670043945 
 batch loss: 29.4069766998291 
 batch loss: 29.40762710571289 
 batch loss: 29.399734497070312 
 batch loss: 29.427610397338867 
 batch loss: 29.51531219482422 
 batch loss: 29.00765037536621 
 batch loss: 29.116329193115234 
 batch loss: 29.001245498657227 
 batch loss: 28.939834594726562 
 batch loss: 28.894521713256836 
 batch loss: 28.8831729888916 
 batch loss: 28.964826583862305 
 batch loss: 28.882888793945312 
 batch loss: 28.66167449951172 
 batch loss: 28.653146743774414 
 batch loss: 28.700387954711914 
 batch loss: 28.76018714904785 
 batch loss: 28.565052032470703 
 batch loss: 28.50534439086914 
 batch loss: 28.47479820251465 
 batch loss: 28.513639450073242 
 batch loss: 28.804161071777344 
 batch loss: 28.3177490234375 
 batch loss: 28.65452766418457 
 batch loss: 28.244720458984375 
 batch loss: 28.76813507080078 
 batch loss: 28.304052352905273 
 batch loss: 28.21039390563965 
 batch loss: 28.463836669921875 
 batch loss: 28.348358154296875 
 batch loss: 27.88480567932129 
 batch loss: 27.818567276000977 
 batch loss: 27.93328285217285 
 batch loss: 27.977079391479492 
 batch loss: 27.73824119567871 
 batch loss: 27.688697814941406 
 batch loss: 27.9036865234375 
 batch loss: 27.73015785217285 
 batch loss: 27.715362548828125 
 batch loss: 28.289531707763672 
 batch loss: 27.621736526489258 
 batch loss: 27.477296829223633 
 batch loss: 27.590126037597656 
 batch loss: 27.684289932250977 
 batch loss: 27.967235565185547 
 batch loss: 27.227689743041992 
 batch loss: 28.306209564208984 
 batch loss: 29.088478088378906 
 batch loss: 27.488142013549805 
 batch loss: 27.373510360717773 
 batch loss: 27.4226016998291 
 batch loss: 27.11763572692871 
 batch loss: 26.97793197631836 
 batch loss: 26.904823303222656 
 batch loss: 26.777698516845703 
 batch loss: 26.80966567993164 
 batch loss: 26.834171295166016 
 batch loss: 26.733964920043945 
 batch loss: 26.603818893432617 
 batch loss: 27.356449127197266 
 batch loss: 26.574262619018555 
 batch loss: 26.777671813964844 
 batch loss: 26.970687866210938 
 batch loss: 26.51274871826172 
 batch loss: 26.662391662597656 
 batch loss: 26.719703674316406 
 batch loss: 26.345367431640625 
 batch loss: 26.082393646240234 
 batch loss: 26.09933090209961 
 batch loss: 26.420427322387695 
 batch loss: 26.412160873413086 
 batch loss: 25.916963577270508 
 batch loss: 25.88067054748535 
 batch loss: 25.963441848754883 
 batch loss: 25.777616500854492 
 batch loss: 25.98834800720215 
 batch loss: 25.704050064086914 
 batch loss: 26.02314567565918 
 batch loss: 25.765066146850586 
 batch loss: 26.38024139404297 
 batch loss: 25.784194946289062 
 batch loss: 26.249765396118164 
 batch loss: 26.40073585510254 
 batch loss: 25.864044189453125 
 batch loss: 25.760072708129883 
 batch loss: 26.009904861450195 
 batch loss: 25.999391555786133 
 batch loss: 26.248781204223633 
 batch loss: 25.634124755859375 
 batch loss: 25.959117889404297 
 batch loss: 25.718542098999023 
 batch loss: 25.95555305480957 
 batch loss: 25.957666397094727 
 batch loss: 26.573896408081055 
 batch loss: 25.864501953125 
 batch loss: 25.858957290649414 
 batch loss: 26.65732192993164 
 Epoch 6 |Train loss: 27.821225786209105 |Validation loss: 26.03325128555298 
 batch loss: 25.880186080932617 
 batch loss: 25.64835548400879 
 batch loss: 25.759037017822266 
 batch loss: 25.79212760925293 
 batch loss: 25.64672088623047 
 batch loss: 26.04207420349121 
 batch loss: 25.597482681274414 
 batch loss: 25.27987289428711 
 batch loss: 25.67583656311035 
 batch loss: 25.333789825439453 
 batch loss: 25.781747817993164 
 batch loss: 25.832988739013672 
 batch loss: 25.408706665039062 
 batch loss: 25.094512939453125 
 batch loss: 25.186817169189453 
 batch loss: 25.16880226135254 
 batch loss: 25.999897003173828 
 batch loss: 25.083293914794922 
 batch loss: 25.18508529663086 
 batch loss: 25.12622833251953 
 batch loss: 24.820343017578125 
 batch loss: 24.932294845581055 
 batch loss: 24.995201110839844 
 batch loss: 25.940330505371094 
 batch loss: 24.868141174316406 
 batch loss: 24.738510131835938 
 batch loss: 25.1782169342041 
 batch loss: 24.68804931640625 
 batch loss: 24.563390731811523 
 batch loss: 24.544965744018555 
 batch loss: 24.45351219177246 
 batch loss: 24.455244064331055 
 batch loss: 24.34654998779297 
 batch loss: 24.362510681152344 
 batch loss: 24.333627700805664 
 batch loss: 24.21331214904785 
 batch loss: 24.211050033569336 
 batch loss: 24.308469772338867 
 batch loss: 24.430831909179688 
 batch loss: 24.317712783813477 
 batch loss: 24.165260314941406 
 batch loss: 24.18541717529297 
 batch loss: 24.103961944580078 
 batch loss: 23.955387115478516 
 batch loss: 23.920808792114258 
 batch loss: 24.042320251464844 
 batch loss: 23.8939208984375 
 batch loss: 24.159589767456055 
 batch loss: 23.693307876586914 
 batch loss: 23.78791046142578 
 batch loss: 23.855043411254883 
 batch loss: 23.839374542236328 
 batch loss: 23.618240356445312 
 batch loss: 23.6705322265625 
 batch loss: 23.464956283569336 
 batch loss: 23.341577529907227 
 batch loss: 23.692289352416992 
 batch loss: 23.372461318969727 
 batch loss: 23.376739501953125 
 batch loss: 23.368698120117188 
 batch loss: 23.612260818481445 
 batch loss: 23.343610763549805 
 batch loss: 23.417776107788086 
 batch loss: 23.258800506591797 
 batch loss: 23.296659469604492 
 batch loss: 23.392820358276367 
 batch loss: 23.090822219848633 
 batch loss: 23.127553939819336 
 batch loss: 23.272571563720703 
 batch loss: 23.09221076965332 
 batch loss: 22.931354522705078 
 batch loss: 22.99639129638672 
 batch loss: 23.132854461669922 
 batch loss: 23.066282272338867 
 batch loss: 22.639719009399414 
 batch loss: 22.7413387298584 
 batch loss: 22.851341247558594 
 batch loss: 22.58130645751953 
 batch loss: 22.753019332885742 
 batch loss: 22.61141586303711 
 batch loss: 22.591760635375977 
 batch loss: 22.777124404907227 
 batch loss: 22.974451065063477 
 batch loss: 22.56891632080078 
 batch loss: 22.52042579650879 
 batch loss: 22.807872772216797 
 batch loss: 22.594533920288086 
 batch loss: 22.7650089263916 
 batch loss: 22.500986099243164 
 batch loss: 22.899564743041992 
 batch loss: 22.654287338256836 
 batch loss: 22.679704666137695 
 batch loss: 22.645137786865234 
 batch loss: 22.555158615112305 
 batch loss: 22.695720672607422 
 batch loss: 22.636796951293945 
 batch loss: 22.634532928466797 
 batch loss: 22.559429168701172 
 batch loss: 22.78197479248047 
 batch loss: 22.689279556274414 
 Epoch 7 |Train loss: 24.249296689033507 |Validation loss: 22.67663335800171 
 batch loss: 22.522964477539062 
 batch loss: 22.927793502807617 
 batch loss: 22.477949142456055 
 batch loss: 22.56923484802246 
 batch loss: 22.48779296875 
 batch loss: 22.55352210998535 
 batch loss: 22.48331069946289 
 batch loss: 22.51664924621582 
 batch loss: 22.37819480895996 
 batch loss: 22.357898712158203 
 batch loss: 22.520917892456055 
 batch loss: 22.340412139892578 
 batch loss: 22.385578155517578 
 batch loss: 22.248077392578125 
 batch loss: 22.268722534179688 
 batch loss: 22.176734924316406 
 batch loss: 22.198257446289062 
 batch loss: 22.153484344482422 
 batch loss: 22.127395629882812 
 batch loss: 22.17435646057129 
 batch loss: 22.11388397216797 
 batch loss: 22.187158584594727 
 batch loss: 22.360286712646484 
 batch loss: 21.939254760742188 
 batch loss: 22.183040618896484 
 batch loss: 22.06627082824707 
 batch loss: 21.994211196899414 
 batch loss: 21.794292449951172 
 batch loss: 22.510601043701172 
 batch loss: 21.946388244628906 
 batch loss: 21.907350540161133 
 batch loss: 22.218584060668945 
 batch loss: 21.796598434448242 
 batch loss: 22.019882202148438 
 batch loss: 21.755596160888672 
 batch loss: 21.84685516357422 
 batch loss: 21.656633377075195 
 batch loss: 22.2984676361084 
 batch loss: 21.633520126342773 
 batch loss: 21.63800048828125 
 batch loss: 21.806299209594727 
 batch loss: 21.49074935913086 
 batch loss: 21.610504150390625 
 batch loss: 21.501604080200195 
 batch loss: 21.657285690307617 
 batch loss: 21.430450439453125 
 batch loss: 21.901891708374023 
 batch loss: 21.51337242126465 
 batch loss: 21.491580963134766 
 batch loss: 21.40472984313965 
 batch loss: 21.811981201171875 
 batch loss: 21.317045211791992 
 batch loss: 21.48223114013672 
 batch loss: 21.443307876586914 
 batch loss: 21.3350887298584 
 batch loss: 21.24820899963379 
 batch loss: 21.286907196044922 
 batch loss: 21.225650787353516 
 batch loss: 22.063570022583008 
 batch loss: 22.11721420288086 
 batch loss: 21.7031192779541 
 batch loss: 21.71551513671875 
 batch loss: 21.35076141357422 
 batch loss: 21.60509490966797 
 batch loss: 21.470531463623047 
 batch loss: 21.177684783935547 
 batch loss: 21.125320434570312 
 batch loss: 21.114521026611328 
 batch loss: 21.070817947387695 
 batch loss: 21.092021942138672 
 batch loss: 21.153820037841797 
 batch loss: 20.85051727294922 
 batch loss: 20.924671173095703 
 batch loss: 20.863922119140625 
 batch loss: 20.77478790283203 
 batch loss: 20.7447452545166 
 batch loss: 20.98969841003418 
 batch loss: 20.699129104614258 
 batch loss: 20.7510929107666 
 batch loss: 20.626220703125 
 batch loss: 20.557140350341797 
 batch loss: 20.544841766357422 
 batch loss: 20.62920379638672 
 batch loss: 20.6467342376709 
 batch loss: 20.731964111328125 
 batch loss: 20.83294677734375 
 batch loss: 20.54511070251465 
 batch loss: 20.547916412353516 
 batch loss: 21.12238121032715 
 batch loss: 20.6551513671875 
 batch loss: 20.74665641784668 
 batch loss: 20.607715606689453 
 batch loss: 20.534095764160156 
 batch loss: 20.47637176513672 
 batch loss: 20.74535369873047 
 batch loss: 20.677490234375 
 batch loss: 20.669811248779297 
 batch loss: 20.838319778442383 
 batch loss: 20.41557502746582 
 batch loss: 21.016685485839844 
 Epoch 8 |Train loss: 21.758497381210326 |Validation loss: 20.677073287963868 
 batch loss: 20.63290023803711 
 batch loss: 20.402999877929688 
 batch loss: 20.53874969482422 
 batch loss: 20.573993682861328 
 batch loss: 20.351682662963867 
 batch loss: 20.390668869018555 
 batch loss: 20.53539276123047 
 batch loss: 20.476531982421875 
 batch loss: 20.375648498535156 
 batch loss: 20.303306579589844 
 batch loss: 20.655363082885742 
 batch loss: 19.99411964416504 
 batch loss: 20.055580139160156 
 batch loss: 19.86319923400879 
 batch loss: 19.881139755249023 
 batch loss: 19.767452239990234 
 batch loss: 19.73346519470215 
 batch loss: 19.597604751586914 
 batch loss: 19.553503036499023 
 batch loss: 19.548913955688477 
 batch loss: 19.491201400756836 
 batch loss: 19.48415184020996 
 batch loss: 19.640094757080078 
 batch loss: 19.379884719848633 
 batch loss: 19.28903579711914 
 batch loss: 19.526535034179688 
 batch loss: 19.504323959350586 
 batch loss: 19.290542602539062 
 batch loss: 19.497766494750977 
 batch loss: 19.310317993164062 
 batch loss: 19.370235443115234 
 batch loss: 19.314823150634766 
 batch loss: 19.18141746520996 
 batch loss: 19.026674270629883 
 batch loss: 18.98963737487793 
 batch loss: 19.07761573791504 
 batch loss: 18.851524353027344 
 batch loss: 19.60430335998535 
 batch loss: 18.981672286987305 
 batch loss: 18.843585968017578 
 batch loss: 19.11844253540039 
 batch loss: 18.889963150024414 
 batch loss: 18.757389068603516 
 batch loss: 18.78485107421875 
 batch loss: 18.728403091430664 
 batch loss: 18.757463455200195 
 batch loss: 18.752052307128906 
 batch loss: 18.650447845458984 
 batch loss: 18.55980110168457 
 batch loss: 19.506912231445312 
 batch loss: 18.70526695251465 
 batch loss: 18.635066986083984 
 batch loss: 18.624372482299805 
 batch loss: 18.716228485107422 
 batch loss: 18.58522605895996 
 batch loss: 18.43733787536621 
 batch loss: 18.459617614746094 
 batch loss: 18.489978790283203 
 batch loss: 18.502290725708008 
 batch loss: 18.611385345458984 
 batch loss: 18.259204864501953 
 batch loss: 18.262434005737305 
 batch loss: 18.282197952270508 
 batch loss: 18.338668823242188 
 batch loss: 18.31340980529785 
 batch loss: 18.344646453857422 
 batch loss: 18.437219619750977 
 batch loss: 18.1202449798584 
 batch loss: 18.400659561157227 
 batch loss: 18.10956382751465 
 batch loss: 18.035720825195312 
 batch loss: 18.365848541259766 
 batch loss: 18.06597137451172 
 batch loss: 18.295610427856445 
 batch loss: 18.208341598510742 
 batch loss: 18.428688049316406 
 batch loss: 18.15791130065918 
 batch loss: 18.079050064086914 
 batch loss: 17.92180061340332 
 batch loss: 18.13490104675293 
 batch loss: 18.049713134765625 
 batch loss: 18.052968978881836 
 batch loss: 18.06711196899414 
 batch loss: 18.054964065551758 
 batch loss: 18.073575973510742 
 batch loss: 18.143701553344727 
 batch loss: 18.184555053710938 
 batch loss: 18.03697395324707 
 batch loss: 18.196348190307617 
 batch loss: 18.06380271911621 
 batch loss: 17.912843704223633 
 batch loss: 17.84218406677246 
 batch loss: 17.867456436157227 
 batch loss: 17.822914123535156 
 batch loss: 17.933584213256836 
 batch loss: 17.943395614624023 
 batch loss: 18.02562713623047 
 batch loss: 17.93890953063965 
 batch loss: 18.055862426757812 
 batch loss: 17.84389877319336 
 Epoch 9 |Train loss: 19.096526885032652 |Validation loss: 18.005519580841064 
 batch loss: 17.893428802490234 
 batch loss: 17.963167190551758 
 batch loss: 17.91005516052246 
 batch loss: 18.083553314208984 
 batch loss: 17.884607315063477 
 batch loss: 17.84429168701172 
 batch loss: 17.778499603271484 
 batch loss: 18.05135154724121 
 batch loss: 17.77235984802246 
 batch loss: 17.725393295288086 
 batch loss: 17.75909996032715 
 batch loss: 17.69374656677246 
 batch loss: 17.80484390258789 
 batch loss: 17.656192779541016 
 batch loss: 17.71483039855957 
 batch loss: 17.59101104736328 
 batch loss: 17.812103271484375 
 batch loss: 17.498952865600586 
 batch loss: 17.502546310424805 
 batch loss: 17.606462478637695 
 batch loss: 17.707239151000977 
 batch loss: 17.595090866088867 
 batch loss: 17.734460830688477 
 batch loss: 17.502214431762695 
 batch loss: 17.379337310791016 
 batch loss: 17.417388916015625 
 batch loss: 17.40519142150879 
 batch loss: 17.595335006713867 
 batch loss: 17.435455322265625 
 batch loss: 17.607545852661133 
 batch loss: 17.490938186645508 
 batch loss: 17.40554428100586 
 batch loss: 17.3677921295166 
 batch loss: 17.40613555908203 
 batch loss: 17.368894577026367 
 batch loss: 17.321271896362305 
 batch loss: 17.188081741333008 
 batch loss: 17.358924865722656 
 batch loss: 17.144344329833984 
 batch loss: 17.499629974365234 
 batch loss: 17.24827003479004 
 batch loss: 17.327558517456055 
 batch loss: 17.28410530090332 
 batch loss: 17.616771697998047 
 batch loss: 17.212270736694336 
 batch loss: 17.04982566833496 
 batch loss: 17.051939010620117 
 batch loss: 17.255842208862305 
 batch loss: 17.052560806274414 
 batch loss: 17.098182678222656 
 batch loss: 17.04770851135254 
 batch loss: 17.016698837280273 
 batch loss: 16.968074798583984 
 batch loss: 17.017871856689453 
 batch loss: 16.950971603393555 
 batch loss: 16.970863342285156 
 batch loss: 16.858144760131836 
 batch loss: 17.013134002685547 
 batch loss: 16.853151321411133 
 batch loss: 16.77766227722168 
 batch loss: 16.8491268157959 
 batch loss: 16.946453094482422 
 batch loss: 16.869632720947266 
 batch loss: 16.795793533325195 
 batch loss: 16.714000701904297 
 batch loss: 16.789169311523438 
 batch loss: 16.701086044311523 
 batch loss: 16.782697677612305 
 batch loss: 16.66238021850586 
 batch loss: 16.754981994628906 
 batch loss: 16.645183563232422 
 batch loss: 16.71522331237793 
 batch loss: 16.831403732299805 
 batch loss: 16.701976776123047 
 batch loss: 16.562795639038086 
 batch loss: 16.606046676635742 
 batch loss: 16.698633193969727 
 batch loss: 16.708908081054688 
 batch loss: 16.78502655029297 
 batch loss: 16.749412536621094 
 batch loss: 16.70600128173828 
 batch loss: 16.813966751098633 
 batch loss: 16.851665496826172 
 batch loss: 16.7573299407959 
 batch loss: 16.666074752807617 
 batch loss: 16.963350296020508 
 batch loss: 16.77153205871582 
 batch loss: 16.731861114501953 
 batch loss: 16.671171188354492 
 batch loss: 16.650392532348633 
 batch loss: 16.806842803955078 
 batch loss: 16.703075408935547 
 batch loss: 16.57266616821289 
 batch loss: 16.664804458618164 
 batch loss: 16.89719009399414 
 batch loss: 16.641340255737305 
 batch loss: 16.526582717895508 
 batch loss: 16.95491600036621 
 batch loss: 16.71736717224121 
 batch loss: 16.735279083251953 
 Epoch 10 |Train loss: 17.26273567676544 |Validation loss: 16.7401704788208 
 batch loss: 16.661027908325195 
 batch loss: 16.611587524414062 
 batch loss: 16.555784225463867 
 batch loss: 16.7088680267334 
 batch loss: 16.577651977539062 
 batch loss: 16.971046447753906 
 batch loss: 16.537681579589844 
 batch loss: 16.624399185180664 
 batch loss: 16.422380447387695 
 batch loss: 16.508832931518555 
 batch loss: 16.49982452392578 
 batch loss: 16.52671241760254 
 batch loss: 16.342411041259766 
 batch loss: 16.55974006652832 
 batch loss: 16.511920928955078 
 batch loss: 16.366138458251953 
 batch loss: 16.30350112915039 
 batch loss: 16.43134307861328 
 batch loss: 16.497705459594727 
 batch loss: 16.36040687561035 
 batch loss: 16.2587833404541 
 batch loss: 16.348976135253906 
 batch loss: 16.29014015197754 
 batch loss: 16.273244857788086 
 batch loss: 16.210033416748047 
 batch loss: 16.23215103149414 
 batch loss: 16.238452911376953 
 batch loss: 16.18338966369629 
 batch loss: 16.222042083740234 
 batch loss: 16.165599822998047 
 batch loss: 16.18143653869629 
 batch loss: 16.16524314880371 
 batch loss: 16.2059383392334 
 batch loss: 16.077009201049805 
 batch loss: 16.187463760375977 
 batch loss: 16.15963363647461 
 batch loss: 16.111560821533203 
 batch loss: 16.041332244873047 
 batch loss: 15.955928802490234 
 batch loss: 16.101282119750977 
 batch loss: 15.978525161743164 
 batch loss: 15.964212417602539 
 batch loss: 16.0272274017334 
 batch loss: 15.91992473602295 
 batch loss: 16.04193687438965 
 batch loss: 16.045574188232422 
 batch loss: 15.906354904174805 
 batch loss: 15.918877601623535 
 batch loss: 15.909783363342285 
 batch loss: 15.845271110534668 
 batch loss: 15.886591911315918 
 batch loss: 15.877815246582031 
 batch loss: 15.771622657775879 
 batch loss: 15.794605255126953 
 batch loss: 15.765454292297363 
 batch loss: 15.707555770874023 
 batch loss: 15.633810997009277 
 batch loss: 15.796370506286621 
 batch loss: 15.729690551757812 
 batch loss: 15.688087463378906 
 batch loss: 15.645655632019043 
 batch loss: 15.559290885925293 
 batch loss: 15.631189346313477 
 batch loss: 15.876100540161133 
 batch loss: 15.520846366882324 
 batch loss: 15.636310577392578 
 batch loss: 15.632013320922852 
 batch loss: 15.660757064819336 
 batch loss: 15.51824951171875 
 batch loss: 15.870994567871094 
 batch loss: 15.556548118591309 
 batch loss: 15.455859184265137 
 batch loss: 15.39634895324707 
 batch loss: 15.482508659362793 
 batch loss: 15.390148162841797 
 batch loss: 15.4966402053833 
 batch loss: 15.358076095581055 
 batch loss: 15.782995223999023 
 batch loss: 15.306501388549805 
 batch loss: 15.301945686340332 
 batch loss: 15.297876358032227 
 batch loss: 15.389713287353516 
 batch loss: 15.352249145507812 
 batch loss: 15.20327377319336 
 batch loss: 15.397505760192871 
 batch loss: 15.356571197509766 
 batch loss: 15.263799667358398 
 batch loss: 15.268943786621094 
 batch loss: 15.368310928344727 
 batch loss: 15.379341125488281 
 batch loss: 15.300095558166504 
 batch loss: 15.405561447143555 
 batch loss: 15.320080757141113 
 batch loss: 15.325623512268066 
 batch loss: 15.289398193359375 
 batch loss: 15.2842378616333 
 batch loss: 15.274866104125977 
 batch loss: 15.258757591247559 
 batch loss: 15.23420238494873 
 batch loss: 15.228607177734375 
 Epoch 11 |Train loss: 16.030960977077484 |Validation loss: 15.30995078086853 
 batch loss: 15.289628028869629 
 batch loss: 15.27536678314209 
 batch loss: 15.23086166381836 
 batch loss: 15.163102149963379 
 batch loss: 15.304057121276855 
 batch loss: 15.149028778076172 
 batch loss: 15.139336585998535 
 batch loss: 15.14293098449707 
 batch loss: 15.113200187683105 
 batch loss: 15.16092300415039 
 batch loss: 15.240874290466309 
 batch loss: 15.232460021972656 
 batch loss: 15.106019020080566 
 batch loss: 15.176496505737305 
 batch loss: 15.021017074584961 
 batch loss: 15.018851280212402 
 batch loss: 15.048129081726074 
 batch loss: 15.076992988586426 
 batch loss: 14.976031303405762 
 batch loss: 14.91463851928711 
 batch loss: 14.902671813964844 
 batch loss: 15.042045593261719 
 batch loss: 14.914543151855469 
 batch loss: 14.878608703613281 
 batch loss: 14.902235984802246 
 batch loss: 14.844114303588867 
 batch loss: 14.901909828186035 
 batch loss: 14.765432357788086 
 batch loss: 14.818964004516602 
 batch loss: 14.695141792297363 
 batch loss: 14.752023696899414 
 batch loss: 14.67930793762207 
 batch loss: 14.727438926696777 
 batch loss: 14.64504337310791 
 batch loss: 14.640552520751953 
 batch loss: 14.635482788085938 
 batch loss: 14.851086616516113 
 batch loss: 14.6159029006958 
 batch loss: 14.675461769104004 
 batch loss: 14.7718505859375 
 batch loss: 14.648913383483887 
 batch loss: 14.622512817382812 
 batch loss: 14.59699821472168 
 batch loss: 14.560201644897461 
 batch loss: 14.53079605102539 
 batch loss: 14.598855018615723 
 batch loss: 14.469897270202637 
 batch loss: 14.463205337524414 
 batch loss: 14.408102989196777 
 batch loss: 14.537528991699219 
 batch loss: 14.353850364685059 
 batch loss: 14.411416053771973 
 batch loss: 14.405263900756836 
 batch loss: 14.470198631286621 
 batch loss: 14.336021423339844 
 batch loss: 14.39348030090332 
 batch loss: 14.463266372680664 
 batch loss: 14.26266098022461 
 batch loss: 14.290836334228516 
 batch loss: 14.277448654174805 
 batch loss: 14.391661643981934 
 batch loss: 14.1709623336792 
 batch loss: 14.24513053894043 
 batch loss: 14.22653865814209 
 batch loss: 14.26927661895752 
 batch loss: 14.188919067382812 
 batch loss: 14.186055183410645 
 batch loss: 14.217789649963379 
 batch loss: 14.049173355102539 
 batch loss: 14.13707447052002 
 batch loss: 14.102009773254395 
 batch loss: 14.229941368103027 
 batch loss: 13.96328067779541 
 batch loss: 13.976460456848145 
 batch loss: 14.0440673828125 
 batch loss: 14.042217254638672 
 batch loss: 13.869118690490723 
 batch loss: 13.881980895996094 
 batch loss: 13.865188598632812 
 batch loss: 13.975067138671875 
 batch loss: 13.851324081420898 
 batch loss: 13.790342330932617 
 batch loss: 13.842144966125488 
 batch loss: 13.771706581115723 
 batch loss: 13.826665878295898 
 batch loss: 13.809389114379883 
 batch loss: 13.95329761505127 
 batch loss: 13.811366081237793 
 batch loss: 13.95496654510498 
 batch loss: 13.8629789352417 
 batch loss: 13.87556266784668 
 batch loss: 13.955976486206055 
 batch loss: 13.87265682220459 
 batch loss: 13.791011810302734 
 batch loss: 13.907981872558594 
 batch loss: 13.817329406738281 
 batch loss: 13.869614601135254 
 batch loss: 13.92995548248291 
 batch loss: 13.88636589050293 
 batch loss: 13.810356140136719 
 Epoch 12 |Train loss: 14.61966415643692 |Validation loss: 13.85954966545105 
 batch loss: 13.798579216003418 
 batch loss: 13.85299301147461 
 batch loss: 13.822516441345215 
 batch loss: 13.738009452819824 
 batch loss: 13.863737106323242 
 batch loss: 13.7578706741333 
 batch loss: 13.663369178771973 
 batch loss: 13.658461570739746 
 batch loss: 13.669878005981445 
 batch loss: 13.690597534179688 
 batch loss: 13.638972282409668 
 batch loss: 13.651312828063965 
 batch loss: 13.629005432128906 
 batch loss: 13.557109832763672 
 batch loss: 13.616518020629883 
 batch loss: 13.643814086914062 
 batch loss: 13.552833557128906 
 batch loss: 13.52710247039795 
 batch loss: 13.49620532989502 
 batch loss: 13.479135513305664 
 batch loss: 13.502626419067383 
 batch loss: 13.545431137084961 
 batch loss: 13.40506649017334 
 batch loss: 13.562604904174805 
 batch loss: 13.513772964477539 
 batch loss: 13.435401916503906 
 batch loss: 13.478854179382324 
 batch loss: 13.451598167419434 
 batch loss: 13.368468284606934 
 batch loss: 13.350875854492188 
 batch loss: 13.362921714782715 
 batch loss: 13.35341739654541 
 batch loss: 13.331336975097656 
 batch loss: 13.368728637695312 
 batch loss: 13.321706771850586 
 batch loss: 13.285425186157227 
 batch loss: 13.43675422668457 
 batch loss: 13.28960132598877 
 batch loss: 13.275640487670898 
 batch loss: 13.33543586730957 
 batch loss: 13.332634925842285 
 batch loss: 13.267266273498535 
 batch loss: 13.220004081726074 
 batch loss: 13.241630554199219 
 batch loss: 13.224388122558594 
 batch loss: 13.221991539001465 
 batch loss: 13.16140079498291 
 batch loss: 13.208937644958496 
 batch loss: 13.18435001373291 
 batch loss: 13.163658142089844 
 batch loss: 13.167110443115234 
 batch loss: 13.112907409667969 
 batch loss: 13.192098617553711 
 batch loss: 13.106083869934082 
 batch loss: 13.124360084533691 
 batch loss: 13.090675354003906 
 batch loss: 13.09946346282959 
 batch loss: 13.081771850585938 
 batch loss: 13.035115242004395 
 batch loss: 13.077971458435059 
 batch loss: 13.055272102355957 
 batch loss: 13.06735897064209 
 batch loss: 13.004176139831543 
 batch loss: 13.092183113098145 
 batch loss: 13.009166717529297 
 batch loss: 12.99633502960205 
 batch loss: 13.080079078674316 
 batch loss: 13.173612594604492 
 batch loss: 13.167035102844238 
 batch loss: 13.067517280578613 
 batch loss: 13.001739501953125 
 batch loss: 12.995229721069336 
 batch loss: 12.953316688537598 
 batch loss: 12.890207290649414 
 batch loss: 12.957196235656738 
 batch loss: 12.89782428741455 
 batch loss: 12.913528442382812 
 batch loss: 12.851434707641602 
 batch loss: 12.85441780090332 
 batch loss: 13.0724515914917 
 batch loss: 12.905682563781738 
 batch loss: 12.798260688781738 
 batch loss: 12.906987190246582 
 batch loss: 12.964396476745605 
 batch loss: 12.91411018371582 
 batch loss: 12.949164390563965 
 batch loss: 12.94745922088623 
 batch loss: 12.871426582336426 
 batch loss: 12.918044090270996 
 batch loss: 12.949172019958496 
 batch loss: 12.888001441955566 
 batch loss: 12.905271530151367 
 batch loss: 12.86388874053955 
 batch loss: 12.817658424377441 
 batch loss: 13.086226463317871 
 batch loss: 12.926261901855469 
 batch loss: 12.840119361877441 
 batch loss: 13.065695762634277 
 batch loss: 12.951539993286133 
 batch loss: 12.84521770477295 
 Epoch 13 |Train loss: 13.308719909191131 |Validation loss: 12.915729236602782 
 batch loss: 12.816076278686523 
 batch loss: 12.85930347442627 
 batch loss: 12.817319869995117 
 batch loss: 12.88911247253418 
 batch loss: 12.881342887878418 
 batch loss: 12.826081275939941 
 batch loss: 12.761191368103027 
 batch loss: 12.742107391357422 
 batch loss: 12.86711311340332 
 batch loss: 12.78288745880127 
 batch loss: 12.904486656188965 
 batch loss: 12.771965980529785 
 batch loss: 12.755598068237305 
 batch loss: 12.731266021728516 
 batch loss: 12.748139381408691 
 batch loss: 12.731752395629883 
 batch loss: 12.699121475219727 
 batch loss: 12.695633888244629 
 batch loss: 12.745368957519531 
 batch loss: 12.682637214660645 
 batch loss: 12.64477252960205 
 batch loss: 12.623422622680664 
 batch loss: 12.701597213745117 
 batch loss: 12.595232963562012 
 batch loss: 12.604170799255371 
 batch loss: 12.532437324523926 
 batch loss: 12.545793533325195 
 batch loss: 12.530121803283691 
 batch loss: 12.533395767211914 
 batch loss: 12.49453067779541 
 batch loss: 12.471526145935059 
 batch loss: 12.524201393127441 
 batch loss: 12.43660831451416 
 batch loss: 12.456795692443848 
 batch loss: 12.46474552154541 
 batch loss: 12.496844291687012 
 batch loss: 12.44995403289795 
 batch loss: 12.523433685302734 
 batch loss: 12.360984802246094 
 batch loss: 12.390310287475586 
 batch loss: 12.482656478881836 
 batch loss: 12.38292407989502 
 batch loss: 12.339014053344727 
 batch loss: 12.334060668945312 
 batch loss: 12.317977905273438 
 batch loss: 12.25788402557373 
 batch loss: 12.356882095336914 
 batch loss: 12.3541898727417 
 batch loss: 12.280868530273438 
 batch loss: 12.26447868347168 
 batch loss: 12.306391716003418 
 batch loss: 12.250864028930664 
 batch loss: 12.19291877746582 
 batch loss: 12.19029712677002 
 batch loss: 12.265791893005371 
 batch loss: 12.194658279418945 
 batch loss: 12.790629386901855 
 batch loss: 12.172136306762695 
 batch loss: 12.231148719787598 
 batch loss: 12.211044311523438 
 batch loss: 12.229092597961426 
 batch loss: 12.221893310546875 
 batch loss: 12.198282241821289 
 batch loss: 12.272316932678223 
 batch loss: 12.111932754516602 
 batch loss: 12.171391487121582 
 batch loss: 12.107562065124512 
 batch loss: 12.169251441955566 
 batch loss: 12.127490997314453 
 batch loss: 12.101821899414062 
 batch loss: 12.147048950195312 
 batch loss: 12.100543975830078 
 batch loss: 12.060762405395508 
 batch loss: 12.096474647521973 
 batch loss: 12.021718978881836 
 batch loss: 12.07138442993164 
 batch loss: 11.986601829528809 
 batch loss: 11.970329284667969 
 batch loss: 12.040156364440918 
 batch loss: 12.051243782043457 
 batch loss: 12.019524574279785 
 batch loss: 11.993573188781738 
 batch loss: 12.041206359863281 
 batch loss: 12.01672649383545 
 batch loss: 12.011530876159668 
 batch loss: 12.034323692321777 
 batch loss: 11.975006103515625 
 batch loss: 11.991101264953613 
 batch loss: 11.998289108276367 
 batch loss: 12.087911605834961 
 batch loss: 11.966465950012207 
 batch loss: 11.948941230773926 
 batch loss: 11.955540657043457 
 batch loss: 11.982646942138672 
 batch loss: 11.996614456176758 
 batch loss: 12.025031089782715 
 batch loss: 11.98931884765625 
 batch loss: 11.944890975952148 
 batch loss: 12.022394180297852 
 batch loss: 11.996536254882812 
 Epoch 14 |Train loss: 12.431543779373168 |Validation loss: 11.999878692626954 
 batch loss: 12.006332397460938 
 batch loss: 11.994377136230469 
 batch loss: 11.995363235473633 
 batch loss: 11.928911209106445 
 batch loss: 11.968790054321289 
 batch loss: 12.009254455566406 
 batch loss: 11.908010482788086 
 batch loss: 11.888161659240723 
 batch loss: 11.902113914489746 
 batch loss: 11.88576889038086 
 batch loss: 11.849505424499512 
 batch loss: 11.917513847351074 
 batch loss: 11.892450332641602 
 batch loss: 11.814779281616211 
 batch loss: 11.874162673950195 
 batch loss: 11.823474884033203 
 batch loss: 11.829873085021973 
 batch loss: 11.815348625183105 
 batch loss: 11.859277725219727 
 batch loss: 11.834590911865234 
 batch loss: 11.820828437805176 
 batch loss: 11.829604148864746 
 batch loss: 11.75770378112793 
 batch loss: 11.738496780395508 
 batch loss: 11.774794578552246 
 batch loss: 11.698055267333984 
 batch loss: 11.714309692382812 
 batch loss: 11.667140007019043 
 batch loss: 11.686279296875 
 batch loss: 11.734296798706055 
 batch loss: 11.632522583007812 
 batch loss: 11.679994583129883 
 batch loss: 11.650038719177246 
 batch loss: 11.59693717956543 
 batch loss: 11.615652084350586 
 batch loss: 11.590951919555664 
 batch loss: 11.631412506103516 
 batch loss: 11.543041229248047 
 batch loss: 11.579371452331543 
 batch loss: 11.588675498962402 
 batch loss: 11.543440818786621 
 batch loss: 11.503074645996094 
 batch loss: 11.543511390686035 
 batch loss: 11.519000053405762 
 batch loss: 11.486196517944336 
 batch loss: 11.517253875732422 
 batch loss: 11.480147361755371 
 batch loss: 11.511561393737793 
 batch loss: 11.480527877807617 
 batch loss: 11.415435791015625 
 batch loss: 11.464255332946777 
 batch loss: 11.440523147583008 
 batch loss: 11.515761375427246 
 batch loss: 11.44366455078125 
 batch loss: 11.463748931884766 
 batch loss: 11.432149887084961 
 batch loss: 11.38615894317627 
 batch loss: 11.406825065612793 
 batch loss: 11.512578010559082 
 batch loss: 11.380144119262695 
 batch loss: 11.3934965133667 
 batch loss: 11.358548164367676 
 batch loss: 11.331456184387207 
 batch loss: 11.365670204162598 
 batch loss: 11.290302276611328 
 batch loss: 11.329056739807129 
 batch loss: 11.331647872924805 
 batch loss: 11.313032150268555 
 batch loss: 11.272855758666992 
 batch loss: 11.256020545959473 
 batch loss: 11.308798789978027 
 batch loss: 11.256074905395508 
 batch loss: 11.239407539367676 
 batch loss: 11.253629684448242 
 batch loss: 11.241161346435547 
 batch loss: 11.212368965148926 
 batch loss: 11.253105163574219 
 batch loss: 11.276293754577637 
 batch loss: 11.191963195800781 
 batch loss: 11.265908241271973 
 batch loss: 11.227898597717285 
 batch loss: 11.169197082519531 
 batch loss: 11.206929206848145 
 batch loss: 11.151860237121582 
 batch loss: 11.250592231750488 
 batch loss: 11.270885467529297 
 batch loss: 11.181057929992676 
 batch loss: 11.269476890563965 
 batch loss: 11.204924583435059 
 batch loss: 11.24251937866211 
 batch loss: 11.203513145446777 
 batch loss: 11.175617218017578 
 batch loss: 11.173360824584961 
 batch loss: 11.206084251403809 
 batch loss: 11.188701629638672 
 batch loss: 11.248441696166992 
 batch loss: 11.210821151733398 
 batch loss: 11.248090744018555 
 batch loss: 11.173834800720215 
 batch loss: 11.411951065063477 
 Epoch 15 |Train loss: 11.583936548233032 |Validation loss: 11.220787906646729 
 batch loss: 11.202189445495605 
 batch loss: 11.159217834472656 
 batch loss: 11.147984504699707 
 batch loss: 11.171784400939941 
 batch loss: 11.225236892700195 
 batch loss: 11.15688419342041 
 batch loss: 11.105110168457031 
 batch loss: 11.109457969665527 
 batch loss: 11.12468147277832 
 batch loss: 11.16407585144043 
 batch loss: 11.101327896118164 
 batch loss: 11.090880393981934 
 batch loss: 11.076406478881836 
 batch loss: 11.065140724182129 
 batch loss: 11.074507713317871 
 batch loss: 11.041797637939453 
 batch loss: 11.04245376586914 
 batch loss: 11.033577919006348 
 batch loss: 11.037545204162598 
 batch loss: 11.066628456115723 
 batch loss: 11.014769554138184 
 batch loss: 10.991622924804688 
 batch loss: 10.98636245727539 
 batch loss: 10.96993637084961 
 batch loss: 10.986770629882812 
 batch loss: 10.981542587280273 
 batch loss: 10.962313652038574 
 batch loss: 11.018250465393066 
 batch loss: 10.948853492736816 
 batch loss: 10.932812690734863 
 batch loss: 10.95530891418457 
 batch loss: 10.890213012695312 
 batch loss: 10.90395736694336 
 batch loss: 10.948573112487793 
 batch loss: 11.158158302307129 
 batch loss: 10.880287170410156 
 batch loss: 10.860553741455078 
 batch loss: 10.985589981079102 
 batch loss: 10.830734252929688 
 batch loss: 10.85376262664795 
 batch loss: 10.851346969604492 
 batch loss: 10.83566665649414 
 batch loss: 10.871528625488281 
 batch loss: 10.79731559753418 
 batch loss: 10.791861534118652 
 batch loss: 10.804301261901855 
 batch loss: 10.785723686218262 
 batch loss: 10.873964309692383 
 batch loss: 10.784782409667969 
 batch loss: 10.829262733459473 
 batch loss: 10.758138656616211 
 batch loss: 10.751925468444824 
 batch loss: 10.78189468383789 
 batch loss: 10.97518539428711 
 batch loss: 10.703752517700195 
 batch loss: 10.789078712463379 
 batch loss: 10.731276512145996 
 batch loss: 10.71423625946045 
 batch loss: 10.68691349029541 
 batch loss: 10.686797142028809 
 batch loss: 10.636616706848145 
 batch loss: 10.716323852539062 
 batch loss: 10.733161926269531 
 batch loss: 10.626031875610352 
 batch loss: 10.622223854064941 
 batch loss: 10.652081489562988 
 batch loss: 10.626779556274414 
 batch loss: 10.636942863464355 
 batch loss: 10.631518363952637 
 batch loss: 10.631677627563477 
 batch loss: 10.567147254943848 
 batch loss: 10.587970733642578 
 batch loss: 10.556937217712402 
 batch loss: 10.515655517578125 
 batch loss: 10.54562759399414 
 batch loss: 10.496071815490723 
 batch loss: 10.59115219116211 
 batch loss: 10.488740921020508 
 batch loss: 10.703529357910156 
 batch loss: 10.488554954528809 
 batch loss: 10.474725723266602 
 batch loss: 10.418951988220215 
 batch loss: 10.467824935913086 
 batch loss: 10.4660005569458 
 batch loss: 10.493230819702148 
 batch loss: 10.486750602722168 
 batch loss: 10.478216171264648 
 batch loss: 10.472633361816406 
 batch loss: 10.527237892150879 
 batch loss: 10.51098918914795 
 batch loss: 10.52725887298584 
 batch loss: 10.453072547912598 
 batch loss: 10.524178504943848 
 batch loss: 10.471470832824707 
 batch loss: 10.490225791931152 
 batch loss: 10.459283828735352 
 batch loss: 10.540339469909668 
 batch loss: 10.478215217590332 
 batch loss: 10.505824089050293 
 batch loss: 10.498076438903809 
 Epoch 16 |Train loss: 10.863962006568908 |Validation loss: 10.487225341796876 
 batch loss: 10.467844009399414 
 batch loss: 10.46683120727539 
 batch loss: 10.44843578338623 
 batch loss: 10.469670295715332 
 batch loss: 10.489526748657227 
 batch loss: 10.414036750793457 
 batch loss: 10.410057067871094 
 batch loss: 10.39233684539795 
 batch loss: 10.353267669677734 
 batch loss: 10.323740005493164 
 batch loss: 10.3250732421875 
 batch loss: 10.332361221313477 
 batch loss: 10.297467231750488 
 batch loss: 10.304678916931152 
 batch loss: 10.369669914245605 
 batch loss: 10.31411075592041 
 batch loss: 10.30226993560791 
 batch loss: 10.23951244354248 
 batch loss: 10.252143859863281 
 batch loss: 10.194293022155762 
 batch loss: 10.241877555847168 
 batch loss: 10.305876731872559 
 batch loss: 10.224639892578125 
 batch loss: 10.281292915344238 
 batch loss: 10.209404945373535 
 batch loss: 10.239832878112793 
 batch loss: 10.221805572509766 
 batch loss: 10.163165092468262 
 batch loss: 10.156085014343262 
 batch loss: 10.142885208129883 
 batch loss: 10.23273754119873 
 batch loss: 10.122332572937012 
 batch loss: 10.053642272949219 
 batch loss: 10.065282821655273 
 batch loss: 10.04064655303955 
 batch loss: 10.019571304321289 
 batch loss: 10.038803100585938 
 batch loss: 9.99907112121582 
 batch loss: 10.010462760925293 
 batch loss: 9.995919227600098 
 batch loss: 9.969813346862793 
 batch loss: 9.981950759887695 
 batch loss: 9.899236679077148 
 batch loss: 9.90333366394043 
 batch loss: 9.916560173034668 
 batch loss: 9.969520568847656 
 batch loss: 9.8573637008667 
 batch loss: 9.83382511138916 
 batch loss: 9.875387191772461 
 batch loss: 9.845549583435059 
 batch loss: 9.839173316955566 
 batch loss: 9.826695442199707 
 batch loss: 9.825072288513184 
 batch loss: 9.749076843261719 
 batch loss: 9.777843475341797 
 batch loss: 9.71508502960205 
 batch loss: 9.897245407104492 
 batch loss: 9.792266845703125 
 batch loss: 9.722373962402344 
 batch loss: 9.756824493408203 
 batch loss: 9.781272888183594 
 batch loss: 9.663862228393555 
 batch loss: 9.684712409973145 
 batch loss: 9.657288551330566 
 batch loss: 9.64547348022461 
 batch loss: 9.608247756958008 
 batch loss: 9.584734916687012 
 batch loss: 9.605256080627441 
 batch loss: 9.579814910888672 
 batch loss: 9.597912788391113 
 batch loss: 9.572396278381348 
 batch loss: 9.574383735656738 
 batch loss: 9.56497859954834 
 batch loss: 9.506625175476074 
 batch loss: 9.514371871948242 
 batch loss: 9.624920845031738 
 batch loss: 9.548676490783691 
 batch loss: 9.58530044555664 
 batch loss: 9.524415016174316 
 batch loss: 9.472800254821777 
 batch loss: 9.465971946716309 
 batch loss: 9.477094650268555 
 batch loss: 9.466981887817383 
 batch loss: 9.508922576904297 
 batch loss: 9.479357719421387 
 batch loss: 9.525494575500488 
 batch loss: 9.43740177154541 
 batch loss: 9.447101593017578 
 batch loss: 9.465991973876953 
 batch loss: 9.489461898803711 
 batch loss: 9.466992378234863 
 batch loss: 9.4353609085083 
 batch loss: 9.497817993164062 
 batch loss: 9.454642295837402 
 batch loss: 9.431137084960938 
 batch loss: 9.474166870117188 
 batch loss: 9.48007869720459 
 batch loss: 9.493121147155762 
 batch loss: 9.482544898986816 
 batch loss: 9.420783042907715 
 Epoch 17 |Train loss: 9.984803807735442 |Validation loss: 9.470021295547486 
 batch loss: 9.499295234680176 
 batch loss: 9.519553184509277 
 batch loss: 9.437949180603027 
 batch loss: 9.489568710327148 
 batch loss: 9.433887481689453 
 batch loss: 9.440099716186523 
 batch loss: 9.433382034301758 
 batch loss: 9.380115509033203 
 batch loss: 9.379459381103516 
 batch loss: 9.346485137939453 
 batch loss: 9.3389892578125 
 batch loss: 9.322240829467773 
 batch loss: 9.299424171447754 
 batch loss: 9.295255661010742 
 batch loss: 9.289527893066406 
 batch loss: 9.243772506713867 
 batch loss: 9.202445983886719 
 batch loss: 9.277786254882812 
 batch loss: 9.184062957763672 
 batch loss: 9.216226577758789 
 batch loss: 9.179364204406738 
 batch loss: 9.151532173156738 
 batch loss: 9.188586235046387 
 batch loss: 9.209184646606445 
 batch loss: 9.123706817626953 
 batch loss: 9.143529891967773 
 batch loss: 9.080228805541992 
 batch loss: 9.10878849029541 
 batch loss: 9.067965507507324 
 batch loss: 9.115514755249023 
 batch loss: 9.125946998596191 
 batch loss: 9.018571853637695 
 batch loss: 9.01668643951416 
 batch loss: 9.028670310974121 
 batch loss: 9.01033878326416 
 batch loss: 8.997756004333496 
 batch loss: 8.96676254272461 
 batch loss: 9.057154655456543 
 batch loss: 9.013053894042969 
 batch loss: 8.96198558807373 
 batch loss: 8.919642448425293 
 batch loss: 8.927834510803223 
 batch loss: 8.913674354553223 
 batch loss: 8.88214111328125 
 batch loss: 8.900110244750977 
 batch loss: 8.840503692626953 
 batch loss: 8.886091232299805 
 batch loss: 8.817447662353516 
 batch loss: 8.830059051513672 
 batch loss: 8.774141311645508 
 batch loss: 8.78065299987793 
 batch loss: 8.775543212890625 
 batch loss: 8.7857084274292 
 batch loss: 8.793299674987793 
 batch loss: 8.761727333068848 
 batch loss: 8.770376205444336 
 batch loss: 8.73143196105957 
 batch loss: 8.70132064819336 
 batch loss: 8.704659461975098 
 batch loss: 8.712678909301758 
 batch loss: 8.711563110351562 
 batch loss: 8.69736099243164 
 batch loss: 8.644795417785645 
 batch loss: 8.654118537902832 
 batch loss: 8.663650512695312 
 batch loss: 8.638144493103027 
 batch loss: 8.677617073059082 
 batch loss: 8.626197814941406 
 batch loss: 8.64096736907959 
 batch loss: 8.606024742126465 
 batch loss: 8.606298446655273 
 batch loss: 8.543243408203125 
 batch loss: 8.653191566467285 
 batch loss: 8.546467781066895 
 batch loss: 8.612345695495605 
 batch loss: 8.53479290008545 
 batch loss: 8.557419776916504 
 batch loss: 8.521472930908203 
 batch loss: 8.50235366821289 
 batch loss: 8.491528511047363 
 batch loss: 8.496603012084961 
 batch loss: 8.488375663757324 
 batch loss: 8.525490760803223 
 batch loss: 8.562810897827148 
 batch loss: 8.513860702514648 
 batch loss: 8.525036811828613 
 batch loss: 8.513331413269043 
 batch loss: 8.487380981445312 
 batch loss: 8.538387298583984 
 batch loss: 8.544116973876953 
 batch loss: 8.497332572937012 
 batch loss: 8.4821195602417 
 batch loss: 8.453660011291504 
 batch loss: 8.462343215942383 
 batch loss: 8.4733247756958 
 batch loss: 8.501818656921387 
 batch loss: 8.48423957824707 
 batch loss: 8.488170623779297 
 batch loss: 8.522591590881348 
 batch loss: 8.492570877075195 
 Epoch 18 |Train loss: 8.961668193340302 |Validation loss: 8.502678298950196 
 batch loss: 8.482946395874023 
 batch loss: 8.547210693359375 
 batch loss: 8.482290267944336 
 batch loss: 8.487403869628906 
 batch loss: 8.449579238891602 
 batch loss: 8.481379508972168 
 batch loss: 8.442495346069336 
 batch loss: 8.440876960754395 
 batch loss: 8.417523384094238 
 batch loss: 8.386711120605469 
 batch loss: 8.429733276367188 
 batch loss: 8.428862571716309 
 batch loss: 8.41312313079834 
 batch loss: 8.374041557312012 
 batch loss: 8.360298156738281 
 batch loss: 8.38248062133789 
 batch loss: 8.336965560913086 
 batch loss: 8.333182334899902 
 batch loss: 8.307110786437988 
 batch loss: 8.291154861450195 
 batch loss: 8.362924575805664 
 batch loss: 8.361510276794434 
 batch loss: 8.321298599243164 
 batch loss: 8.27064037322998 
 batch loss: 8.276493072509766 
 batch loss: 8.29358196258545 
 batch loss: 8.28530502319336 
 batch loss: 8.229269981384277 
 batch loss: 8.200723648071289 
 batch loss: 8.22566032409668 
 batch loss: 8.16767692565918 
 batch loss: 8.156025886535645 
 batch loss: 8.117302894592285 
 batch loss: 8.142439842224121 
 batch loss: 8.126598358154297 
 batch loss: 8.12324047088623 
 batch loss: 8.12478256225586 
 batch loss: 8.118282318115234 
 batch loss: 8.114137649536133 
 batch loss: 8.084531784057617 
 batch loss: 8.106111526489258 
 batch loss: 8.0785551071167 
 batch loss: 8.071593284606934 
 batch loss: 8.059497833251953 
 batch loss: 8.060113906860352 
 batch loss: 8.061931610107422 
 batch loss: 8.045843124389648 
 batch loss: 8.02237606048584 
 batch loss: 8.029953002929688 
 batch loss: 8.079599380493164 
 batch loss: 8.056853294372559 
 batch loss: 8.106001853942871 
 batch loss: 7.996478080749512 
 batch loss: 7.983488082885742 
 batch loss: 8.000497817993164 
 batch loss: 8.01575756072998 
 batch loss: 7.970189094543457 
 batch loss: 7.991921901702881 
 batch loss: 7.932321071624756 
 batch loss: 7.93751859664917 
 batch loss: 7.914872169494629 
 batch loss: 7.914907932281494 
 batch loss: 7.905968189239502 
 batch loss: 7.913204193115234 
 batch loss: 7.873845100402832 
 batch loss: 7.881709575653076 
 batch loss: 7.88929557800293 
 batch loss: 7.8638916015625 
 batch loss: 7.840290546417236 
 batch loss: 7.856799125671387 
 batch loss: 7.817897319793701 
 batch loss: 7.843692302703857 
 batch loss: 7.824950218200684 
 batch loss: 7.815860271453857 
 batch loss: 7.821887969970703 
 batch loss: 7.8536176681518555 
 batch loss: 7.805295467376709 
 batch loss: 7.810929775238037 
 batch loss: 7.74634313583374 
 batch loss: 7.790521144866943 
 batch loss: 7.762679576873779 
 batch loss: 7.789085388183594 
 batch loss: 7.776215076446533 
 batch loss: 7.743863582611084 
 batch loss: 7.764305591583252 
 batch loss: 7.768680572509766 
 batch loss: 7.766238212585449 
 batch loss: 7.80336856842041 
 batch loss: 7.787265777587891 
 batch loss: 7.819680690765381 
 batch loss: 7.750711441040039 
 batch loss: 7.754735469818115 
 batch loss: 7.777596473693848 
 batch loss: 7.7852373123168945 
 batch loss: 7.804024696350098 
 batch loss: 7.769451141357422 
 batch loss: 7.80039644241333 
 batch loss: 7.789767742156982 
 batch loss: 7.756005764007568 
 batch loss: 7.764902114868164 
 Epoch 19 |Train loss: 8.124627220630646 |Validation loss: 7.77671058177948 
