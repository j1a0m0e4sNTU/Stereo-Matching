      
 ID: Net_bf_v1 
 parameter number: 8237658 
 infomation: Brute Force -- fit left image to its disparity 
 Epoch number: 20 
 Batch size: 2 
 =======================

Net_bf(
  (feature_extract): FeatureNet(
    (net): Sequential(
      (0): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (1): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (2): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (3): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (4): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (5): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(256, 350, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
    )
  )
  (disp_extract): DispNet(
    (net): Sequential(
      (0): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(350, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (1): ConvTranspose2dBlock(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (2): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (3): ConvTranspose2dBlock(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (4): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (5): ConvTranspose2dBlock(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (6): Conv2dBlock(
        (net): Sequential(
          (0): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): Softmax()
    )
  )
  (regression): DisparityRegression()
)
 batch loss: 94.24833679199219 
 batch loss: 92.31184387207031 
 batch loss: 90.49281311035156 
 batch loss: 89.49845886230469 
 batch loss: 86.26387023925781 
 batch loss: 84.35014343261719 
 batch loss: 86.31543731689453 
 batch loss: 84.3739242553711 
 batch loss: 81.7917251586914 
 batch loss: 81.06260681152344 
 batch loss: 80.3591079711914 
 batch loss: 79.93550872802734 
 batch loss: 79.28251647949219 
 batch loss: 78.95585632324219 
 batch loss: 78.5438003540039 
 batch loss: 78.63277435302734 
 batch loss: 78.27682495117188 
 batch loss: 78.44725036621094 
 batch loss: 77.69329071044922 
 batch loss: 77.39725494384766 
 batch loss: 77.12782287597656 
 batch loss: 77.44075775146484 
 batch loss: 77.49627685546875 
 batch loss: 77.00050354003906 
 batch loss: 76.60910034179688 
 batch loss: 76.56214904785156 
 batch loss: 76.403564453125 
 batch loss: 76.38851928710938 
 batch loss: 76.28217315673828 
 batch loss: 75.85712432861328 
 batch loss: 76.11953735351562 
 batch loss: 75.52912139892578 
 batch loss: 75.39862823486328 
 batch loss: 75.39942169189453 
 batch loss: 75.284423828125 
 batch loss: 75.10763549804688 
 batch loss: 75.135986328125 
 batch loss: 74.92020416259766 
 batch loss: 74.76260375976562 
 batch loss: 74.61328887939453 
 batch loss: 74.43115997314453 
 batch loss: 74.31719207763672 
 batch loss: 74.0567855834961 
 batch loss: 74.032470703125 
 batch loss: 73.8819580078125 
 batch loss: 73.73665618896484 
 batch loss: 73.66119384765625 
 batch loss: 73.5183334350586 
 batch loss: 73.32876586914062 
 batch loss: 73.33222961425781 
 batch loss: 73.20247650146484 
 batch loss: 73.07706451416016 
 batch loss: 72.98370361328125 
 batch loss: 72.92494201660156 
 batch loss: 72.74813842773438 
 batch loss: 72.71379852294922 
 batch loss: 72.63910675048828 
 batch loss: 72.4692611694336 
 batch loss: 72.41038513183594 
 batch loss: 72.24736785888672 
 batch loss: 72.08580780029297 
 batch loss: 71.99239349365234 
 batch loss: 71.82913208007812 
 batch loss: 71.81692504882812 
 batch loss: 71.65863800048828 
 batch loss: 71.56292724609375 
 batch loss: 71.47490692138672 
 batch loss: 71.32661437988281 
 batch loss: 71.32119750976562 
 batch loss: 71.15840148925781 
 batch loss: 71.05033111572266 
 batch loss: 70.94221496582031 
 batch loss: 70.84396362304688 
 batch loss: 70.83079528808594 
 batch loss: 70.70108032226562 
 batch loss: 70.546142578125 
 batch loss: 70.48274230957031 
 batch loss: 70.39628601074219 
 batch loss: 70.27421569824219 
 batch loss: 70.18301391601562 
 batch loss: 70.13056182861328 
 batch loss: 70.07791137695312 
 batch loss: 70.1318359375 
 batch loss: 70.1105728149414 
 batch loss: 70.11918640136719 
 batch loss: 70.10330963134766 
 batch loss: 70.1207275390625 
 batch loss: 70.09153747558594 
 batch loss: 70.1279296875 
 batch loss: 70.11100006103516 
 batch loss: 70.12144470214844 
 batch loss: 70.02688598632812 
 batch loss: 70.1063003540039 
 batch loss: 70.0509262084961 
 batch loss: 70.00408172607422 
 batch loss: 70.0532455444336 
 batch loss: 70.13105010986328 
 batch loss: 70.11808013916016 
 batch loss: 70.09635925292969 
 batch loss: 70.08692169189453 
 Epoch 0 |Train loss: 75.82328634262085 |Validation loss: 70.09599342346192 
 batch loss: 70.1124267578125 
 batch loss: 69.95223236083984 
 batch loss: 69.88908386230469 
 batch loss: 69.8048324584961 
 batch loss: 69.64616394042969 
 batch loss: 69.61135864257812 
 batch loss: 69.50220489501953 
 batch loss: 69.4214096069336 
 batch loss: 69.30630493164062 
 batch loss: 69.28446960449219 
 batch loss: 69.13358306884766 
 batch loss: 68.97410583496094 
 batch loss: 68.9030532836914 
 batch loss: 68.83929443359375 
 batch loss: 68.68523406982422 
 batch loss: 68.56474304199219 
 batch loss: 68.54271697998047 
 batch loss: 68.37909698486328 
 batch loss: 68.38717651367188 
 batch loss: 68.26744842529297 
 batch loss: 68.13566589355469 
 batch loss: 68.04344177246094 
 batch loss: 67.95503234863281 
 batch loss: 67.84955596923828 
 batch loss: 67.71914672851562 
 batch loss: 67.69279479980469 
 batch loss: 67.51424407958984 
 batch loss: 67.43788146972656 
 batch loss: 67.32594299316406 
 batch loss: 67.25457000732422 
 batch loss: 67.15715789794922 
 batch loss: 67.0161361694336 
 batch loss: 66.94065856933594 
 batch loss: 66.87220001220703 
 batch loss: 66.7424545288086 
 batch loss: 66.67387390136719 
 batch loss: 66.54547119140625 
 batch loss: 66.46736907958984 
 batch loss: 66.34159088134766 
 batch loss: 66.26480865478516 
 batch loss: 66.18054962158203 
 batch loss: 66.06944274902344 
 batch loss: 65.96307373046875 
 batch loss: 65.85708618164062 
 batch loss: 65.78524017333984 
 batch loss: 65.68724822998047 
 batch loss: 65.57719421386719 
 batch loss: 65.5019302368164 
 batch loss: 65.41401672363281 
 batch loss: 65.2923812866211 
 batch loss: 65.18832397460938 
 batch loss: 65.10321044921875 
 batch loss: 65.09123229980469 
 batch loss: 64.99532318115234 
 batch loss: 64.88581085205078 
 batch loss: 64.730712890625 
 batch loss: 64.66546630859375 
 batch loss: 64.54794311523438 
 batch loss: 64.47411346435547 
 batch loss: 64.3434829711914 
 batch loss: 64.23542022705078 
 batch loss: 64.17181396484375 
 batch loss: 64.06752014160156 
 batch loss: 63.95989227294922 
 batch loss: 63.83736801147461 
 batch loss: 63.74912643432617 
 batch loss: 63.63433074951172 
 batch loss: 63.5057258605957 
 batch loss: 63.40791320800781 
 batch loss: 63.300926208496094 
 batch loss: 63.18935012817383 
 batch loss: 63.11219787597656 
 batch loss: 63.04076385498047 
 batch loss: 62.87042236328125 
 batch loss: 62.759544372558594 
 batch loss: 62.587303161621094 
 batch loss: 62.470096588134766 
 batch loss: 62.30777359008789 
 batch loss: 62.11226272583008 
 batch loss: 62.136619567871094 
 batch loss: 61.84008026123047 
 batch loss: 61.81288528442383 
 batch loss: 61.84340286254883 
 batch loss: 61.85054016113281 
 batch loss: 61.807899475097656 
 batch loss: 61.834354400634766 
 batch loss: 61.83945846557617 
 batch loss: 61.87794876098633 
 batch loss: 61.86503219604492 
 batch loss: 61.810665130615234 
 batch loss: 61.8408088684082 
 batch loss: 61.84178924560547 
 batch loss: 61.87453842163086 
 batch loss: 61.91386795043945 
 batch loss: 61.83462905883789 
 batch loss: 61.845970153808594 
 batch loss: 61.87309265136719 
 batch loss: 61.84272003173828 
 batch loss: 61.834049224853516 
 batch loss: 61.82736587524414 
 Epoch 1 |Train loss: 66.21208863258362 |Validation loss: 61.84555492401123 
 batch loss: 61.946266174316406 
 batch loss: 61.742305755615234 
 batch loss: 61.53730773925781 
 batch loss: 61.430397033691406 
 batch loss: 61.18067169189453 
 batch loss: 61.1351203918457 
 batch loss: 60.90084457397461 
 batch loss: 61.13576126098633 
 batch loss: 60.811607360839844 
 batch loss: 60.95035171508789 
 batch loss: 60.55215835571289 
 batch loss: 60.40462112426758 
 batch loss: 60.22734832763672 
 batch loss: 59.98735427856445 
 batch loss: 59.81292724609375 
 batch loss: 59.813323974609375 
 batch loss: 59.52360534667969 
 batch loss: 59.37134552001953 
 batch loss: 59.18135070800781 
 batch loss: 59.04142761230469 
 batch loss: 58.9675178527832 
 batch loss: 58.73815155029297 
 batch loss: 58.71450424194336 
 batch loss: 58.63926315307617 
 batch loss: 58.41566467285156 
 batch loss: 58.488521575927734 
 batch loss: 58.14374923706055 
 batch loss: 58.04712677001953 
 batch loss: 57.80198669433594 
 batch loss: 57.91389465332031 
 batch loss: 57.555660247802734 
 batch loss: 57.53782272338867 
 batch loss: 57.372337341308594 
 batch loss: 57.15705108642578 
 batch loss: 56.9697265625 
 batch loss: 57.017242431640625 
 batch loss: 56.77155303955078 
 batch loss: 56.740074157714844 
 batch loss: 56.59667205810547 
 batch loss: 56.3633918762207 
 batch loss: 56.22319412231445 
 batch loss: 56.10087966918945 
 batch loss: 56.10188293457031 
 batch loss: 55.70021057128906 
 batch loss: 55.55734634399414 
 batch loss: 55.49066162109375 
 batch loss: 55.2169189453125 
 batch loss: 55.11832046508789 
 batch loss: 54.918006896972656 
 batch loss: 54.661624908447266 
 batch loss: 54.460018157958984 
 batch loss: 54.304847717285156 
 batch loss: 54.26395797729492 
 batch loss: 53.84581756591797 
 batch loss: 53.480228424072266 
 batch loss: 53.36836242675781 
 batch loss: 53.157840728759766 
 batch loss: 52.8942756652832 
 batch loss: 53.48833084106445 
 batch loss: 53.020111083984375 
 batch loss: 52.90239334106445 
 batch loss: 52.5871696472168 
 batch loss: 52.405174255371094 
 batch loss: 52.16342544555664 
 batch loss: 51.88526916503906 
 batch loss: 51.75576400756836 
 batch loss: 51.54407501220703 
 batch loss: 51.41629409790039 
 batch loss: 51.038021087646484 
 batch loss: 51.138423919677734 
 batch loss: 50.98727798461914 
 batch loss: 51.11767578125 
 batch loss: 50.62883377075195 
 batch loss: 50.48600769042969 
 batch loss: 50.14358901977539 
 batch loss: 49.9063720703125 
 batch loss: 50.01728820800781 
 batch loss: 50.296142578125 
 batch loss: 49.73646545410156 
 batch loss: 49.7611083984375 
 batch loss: 50.008544921875 
 batch loss: 49.82573318481445 
 batch loss: 49.988407135009766 
 batch loss: 49.64737319946289 
 batch loss: 50.326698303222656 
 batch loss: 49.72270202636719 
 batch loss: 49.81404495239258 
 batch loss: 49.617610931396484 
 batch loss: 49.94636535644531 
 batch loss: 49.862674713134766 
 batch loss: 49.640708923339844 
 batch loss: 49.65098190307617 
 batch loss: 49.515525817871094 
 batch loss: 50.28163528442383 
 batch loss: 49.63693618774414 
 batch loss: 49.797550201416016 
 batch loss: 49.625797271728516 
 batch loss: 50.505252838134766 
 batch loss: 49.99634552001953 
 batch loss: 49.9184455871582 
 Epoch 2 |Train loss: 55.97409520149231 |Validation loss: 49.86646671295166 
 batch loss: 49.93148422241211 
 batch loss: 49.56504821777344 
 batch loss: 49.319618225097656 
 batch loss: 49.60372543334961 
 batch loss: 49.278316497802734 
 batch loss: 49.44139862060547 
 batch loss: 49.43960189819336 
 batch loss: 48.93318176269531 
 batch loss: 48.73207092285156 
 batch loss: 49.2259521484375 
 batch loss: 48.3548469543457 
 batch loss: 48.26682662963867 
 batch loss: 48.147857666015625 
 batch loss: 48.15050506591797 
 batch loss: 47.8044548034668 
 batch loss: 47.90301513671875 
 batch loss: 48.03093719482422 
 batch loss: 47.667728424072266 
 batch loss: 47.40501022338867 
 batch loss: 47.319583892822266 
 batch loss: 47.12546157836914 
 batch loss: 46.91557693481445 
 batch loss: 46.92939758300781 
 batch loss: 46.981807708740234 
 batch loss: 46.84770202636719 
 batch loss: 46.54772186279297 
 batch loss: 46.38707733154297 
 batch loss: 46.2131462097168 
 batch loss: 46.690887451171875 
 batch loss: 46.16960525512695 
 batch loss: 45.90214538574219 
 batch loss: 45.903865814208984 
 batch loss: 45.804290771484375 
 batch loss: 45.75743103027344 
 batch loss: 45.59331130981445 
 batch loss: 45.379425048828125 
 batch loss: 45.36919021606445 
 batch loss: 45.47538375854492 
 batch loss: 44.982948303222656 
 batch loss: 44.946842193603516 
 batch loss: 44.96076583862305 
 batch loss: 44.90553665161133 
 batch loss: 44.78318786621094 
 batch loss: 44.629478454589844 
 batch loss: 44.52781295776367 
 batch loss: 44.51987838745117 
 batch loss: 44.30606460571289 
 batch loss: 44.29592514038086 
 batch loss: 44.05632019042969 
 batch loss: 44.22359085083008 
 batch loss: 44.018653869628906 
 batch loss: 43.781105041503906 
 batch loss: 43.59980010986328 
 batch loss: 43.78281784057617 
 batch loss: 43.29975891113281 
 batch loss: 43.35044860839844 
 batch loss: 43.49090576171875 
 batch loss: 43.14246368408203 
 batch loss: 43.293392181396484 
 batch loss: 43.065040588378906 
 batch loss: 42.98134231567383 
 batch loss: 43.33373260498047 
 batch loss: 42.75421905517578 
 batch loss: 43.45537567138672 
 batch loss: 43.01008605957031 
 batch loss: 42.78263854980469 
 batch loss: 42.47602081298828 
 batch loss: 42.71267318725586 
 batch loss: 42.333152770996094 
 batch loss: 42.20122528076172 
 batch loss: 42.066497802734375 
 batch loss: 41.978515625 
 batch loss: 41.911468505859375 
 batch loss: 41.830047607421875 
 batch loss: 41.79462814331055 
 batch loss: 41.58283996582031 
 batch loss: 41.741329193115234 
 batch loss: 41.4993896484375 
 batch loss: 41.426612854003906 
 batch loss: 41.26271057128906 
 batch loss: 41.233028411865234 
 batch loss: 41.817466735839844 
 batch loss: 41.5098762512207 
 batch loss: 41.59660720825195 
 batch loss: 41.60539245605469 
 batch loss: 41.53502655029297 
 batch loss: 41.3056640625 
 batch loss: 41.335914611816406 
 batch loss: 41.444725036621094 
 batch loss: 41.42485427856445 
 batch loss: 41.473838806152344 
 batch loss: 41.394859313964844 
 batch loss: 41.22743225097656 
 batch loss: 41.626285552978516 
 batch loss: 41.49481201171875 
 batch loss: 41.67316818237305 
 batch loss: 41.59511947631836 
 batch loss: 41.80104446411133 
 batch loss: 41.199684143066406 
 batch loss: 41.54632568359375 
 Epoch 3 |Train loss: 45.245147943496704 |Validation loss: 41.49205627441406 
 batch loss: 41.34931945800781 
 batch loss: 41.214149475097656 
 batch loss: 41.33008575439453 
 batch loss: 41.06741714477539 
 batch loss: 40.9873046875 
 batch loss: 41.02126693725586 
 batch loss: 40.855472564697266 
 batch loss: 40.90899658203125 
 batch loss: 40.69664001464844 
 batch loss: 40.71818923950195 
 batch loss: 40.84283447265625 
 batch loss: 40.694580078125 
 batch loss: 40.48040771484375 
 batch loss: 40.458641052246094 
 batch loss: 40.71741485595703 
 batch loss: 40.4224967956543 
 batch loss: 40.1285514831543 
 batch loss: 40.488624572753906 
 batch loss: 40.22358703613281 
 batch loss: 40.03025817871094 
 batch loss: 40.69407272338867 
 batch loss: 40.113624572753906 
 batch loss: 40.1645622253418 
 batch loss: 39.80551528930664 
 batch loss: 39.589839935302734 
 batch loss: 39.46269226074219 
 batch loss: 40.096248626708984 
 batch loss: 39.64939880371094 
 batch loss: 39.60040283203125 
 batch loss: 39.27329635620117 
 batch loss: 39.651084899902344 
 batch loss: 39.32732009887695 
 batch loss: 39.16243362426758 
 batch loss: 39.39805603027344 
 batch loss: 38.88617706298828 
 batch loss: 39.15951919555664 
 batch loss: 38.84125900268555 
 batch loss: 38.8104362487793 
 batch loss: 39.175559997558594 
 batch loss: 38.578731536865234 
 batch loss: 38.67158126831055 
 batch loss: 38.64293670654297 
 batch loss: 38.39331817626953 
 batch loss: 38.312408447265625 
 batch loss: 38.375640869140625 
 batch loss: 38.437774658203125 
 batch loss: 38.408973693847656 
 batch loss: 38.1906623840332 
 batch loss: 38.01530456542969 
 batch loss: 37.88011932373047 
 batch loss: 38.149436950683594 
 batch loss: 38.01378631591797 
 batch loss: 38.28608322143555 
 batch loss: 38.03653335571289 
 batch loss: 37.80951690673828 
 batch loss: 37.840579986572266 
 batch loss: 37.68513870239258 
 batch loss: 37.80907440185547 
 batch loss: 37.488800048828125 
 batch loss: 37.74142837524414 
 batch loss: 37.39192581176758 
 batch loss: 37.2255859375 
 batch loss: 37.34437942504883 
 batch loss: 37.099884033203125 
 batch loss: 37.27814483642578 
 batch loss: 36.981510162353516 
 batch loss: 36.87989807128906 
 batch loss: 36.769264221191406 
 batch loss: 36.68404769897461 
 batch loss: 36.66318130493164 
 batch loss: 36.87018585205078 
 batch loss: 36.48249053955078 
 batch loss: 36.444461822509766 
 batch loss: 36.420833587646484 
 batch loss: 36.55707931518555 
 batch loss: 36.70650863647461 
 batch loss: 37.21440124511719 
 batch loss: 36.449317932128906 
 batch loss: 37.38113784790039 
 batch loss: 36.5760498046875 
 batch loss: 36.42402267456055 
 batch loss: 36.35852813720703 
 batch loss: 36.15850830078125 
 batch loss: 36.049922943115234 
 batch loss: 36.41802978515625 
 batch loss: 36.195892333984375 
 batch loss: 36.15781784057617 
 batch loss: 36.306396484375 
 batch loss: 36.23583221435547 
 batch loss: 36.17852783203125 
 batch loss: 36.11294174194336 
 batch loss: 36.34311294555664 
 batch loss: 36.31031799316406 
 batch loss: 36.29207992553711 
 batch loss: 36.96122360229492 
 batch loss: 36.10913848876953 
 batch loss: 36.168914794921875 
 batch loss: 36.542945861816406 
 batch loss: 36.11878204345703 
 batch loss: 36.42596435546875 
 Epoch 4 |Train loss: 38.79607319831848 |Validation loss: 36.29344501495361 
 batch loss: 36.28014373779297 
 batch loss: 36.076507568359375 
 batch loss: 35.979522705078125 
 batch loss: 36.14914321899414 
 batch loss: 35.84845733642578 
 batch loss: 35.92950439453125 
 batch loss: 35.551239013671875 
 batch loss: 35.8072624206543 
 batch loss: 35.54887390136719 
 batch loss: 36.26741409301758 
 batch loss: 35.31425476074219 
 batch loss: 35.590362548828125 
 batch loss: 35.36442184448242 
 batch loss: 35.177947998046875 
 batch loss: 35.0375862121582 
 batch loss: 34.8841438293457 
 batch loss: 35.047828674316406 
 batch loss: 35.36830520629883 
 batch loss: 35.068458557128906 
 batch loss: 34.95225143432617 
 batch loss: 34.99012756347656 
 batch loss: 34.68226623535156 
 batch loss: 34.79169464111328 
 batch loss: 34.558860778808594 
 batch loss: 34.517578125 
 batch loss: 34.40525436401367 
 batch loss: 34.4007453918457 
 batch loss: 34.37868881225586 
 batch loss: 34.362449645996094 
 batch loss: 37.703285217285156 
 batch loss: 34.976287841796875 
 batch loss: 35.20314025878906 
 batch loss: 34.67913818359375 
 batch loss: 34.640403747558594 
 batch loss: 34.44526290893555 
 batch loss: 34.409324645996094 
 batch loss: 34.59571075439453 
 batch loss: 34.3669319152832 
 batch loss: 34.458316802978516 
 batch loss: 34.50606918334961 
 batch loss: 34.02096939086914 
 batch loss: 34.128936767578125 
 batch loss: 33.816890716552734 
 batch loss: 33.8746452331543 
 batch loss: 33.76054763793945 
 batch loss: 33.53956604003906 
 batch loss: 33.482826232910156 
 batch loss: 33.49639892578125 
 batch loss: 33.44927215576172 
 batch loss: 33.474769592285156 
 batch loss: 33.3188362121582 
 batch loss: 33.455421447753906 
 batch loss: 33.579776763916016 
 batch loss: 33.018035888671875 
 batch loss: 33.30663299560547 
 batch loss: 32.998329162597656 
 batch loss: 33.503013610839844 
 batch loss: 33.05693435668945 
 batch loss: 32.849281311035156 
 batch loss: 32.84569549560547 
 batch loss: 32.78038787841797 
 batch loss: 32.68020248413086 
 batch loss: 32.89072036743164 
 batch loss: 32.56071472167969 
 batch loss: 32.61443328857422 
 batch loss: 32.49026870727539 
 batch loss: 32.482994079589844 
 batch loss: 32.68292999267578 
 batch loss: 32.514156341552734 
 batch loss: 32.522159576416016 
 batch loss: 32.35747146606445 
 batch loss: 32.245994567871094 
 batch loss: 32.331932067871094 
 batch loss: 32.20136642456055 
 batch loss: 32.0921745300293 
 batch loss: 32.170166015625 
 batch loss: 31.965831756591797 
 batch loss: 32.512847900390625 
 batch loss: 31.855348587036133 
 batch loss: 31.993915557861328 
 batch loss: 32.00554656982422 
 batch loss: 31.85784339904785 
 batch loss: 31.96088218688965 
 batch loss: 32.11006546020508 
 batch loss: 32.08094024658203 
 batch loss: 31.94792366027832 
 batch loss: 32.066017150878906 
 batch loss: 32.358123779296875 
 batch loss: 32.12949752807617 
 batch loss: 32.061466217041016 
 batch loss: 32.00987243652344 
 batch loss: 31.882713317871094 
 batch loss: 32.137081146240234 
 batch loss: 31.878036499023438 
 batch loss: 32.442691802978516 
 batch loss: 32.165889739990234 
 batch loss: 31.875221252441406 
 batch loss: 32.04411315917969 
 batch loss: 31.920522689819336 
 batch loss: 32.12030792236328 
 Epoch 5 |Train loss: 34.04047453403473 |Validation loss: 32.05273780822754 
 batch loss: 31.837120056152344 
 batch loss: 31.804128646850586 
 batch loss: 31.807016372680664 
 batch loss: 31.987232208251953 
 batch loss: 31.83802604675293 
 batch loss: 31.66635513305664 
 batch loss: 31.63804054260254 
 batch loss: 31.579126358032227 
 batch loss: 31.617841720581055 
 batch loss: 31.481781005859375 
 batch loss: 31.380207061767578 
 batch loss: 31.438121795654297 
 batch loss: 31.368837356567383 
 batch loss: 31.273956298828125 
 batch loss: 31.218772888183594 
 batch loss: 31.18239402770996 
 batch loss: 31.319246292114258 
 batch loss: 31.08417320251465 
 batch loss: 31.078125 
 batch loss: 31.18963050842285 
 batch loss: 31.023916244506836 
 batch loss: 30.86910057067871 
 batch loss: 30.833032608032227 
 batch loss: 30.9510555267334 
 batch loss: 30.76506233215332 
 batch loss: 30.761512756347656 
 batch loss: 30.46710968017578 
 batch loss: 31.414688110351562 
 batch loss: 30.380077362060547 
 batch loss: 30.318084716796875 
 batch loss: 30.233274459838867 
 batch loss: 30.119762420654297 
 batch loss: 30.043500900268555 
 batch loss: 30.039302825927734 
 batch loss: 29.722959518432617 
 batch loss: 29.682994842529297 
 batch loss: 29.529239654541016 
 batch loss: 29.480113983154297 
 batch loss: 29.53778076171875 
 batch loss: 29.21207618713379 
 batch loss: 29.290987014770508 
 batch loss: 29.13298797607422 
 batch loss: 29.17412757873535 
 batch loss: 28.956308364868164 
 batch loss: 28.905508041381836 
 batch loss: 29.025081634521484 
 batch loss: 28.920150756835938 
 batch loss: 28.793930053710938 
 batch loss: 28.68375587463379 
 batch loss: 28.62374496459961 
 batch loss: 28.582088470458984 
 batch loss: 28.78438377380371 
 batch loss: 28.571332931518555 
 batch loss: 28.54623031616211 
 batch loss: 28.879627227783203 
 batch loss: 28.510793685913086 
 batch loss: 28.50654411315918 
 batch loss: 28.38064956665039 
 batch loss: 28.86374855041504 
 batch loss: 28.906723022460938 
 batch loss: 28.225872039794922 
 batch loss: 28.431079864501953 
 batch loss: 28.365154266357422 
 batch loss: 28.390153884887695 
 batch loss: 28.272085189819336 
 batch loss: 28.100303649902344 
 batch loss: 27.900104522705078 
 batch loss: 27.962806701660156 
 batch loss: 28.03099250793457 
 batch loss: 28.080652236938477 
 batch loss: 27.94732666015625 
 batch loss: 28.098772048950195 
 batch loss: 27.88149642944336 
 batch loss: 28.044511795043945 
 batch loss: 27.873241424560547 
 batch loss: 27.85273551940918 
 batch loss: 27.68841552734375 
 batch loss: 27.92938995361328 
 batch loss: 27.882511138916016 
 batch loss: 27.659494400024414 
 batch loss: 27.531177520751953 
 batch loss: 27.675546646118164 
 batch loss: 27.718650817871094 
 batch loss: 27.74614906311035 
 batch loss: 27.654857635498047 
 batch loss: 27.780160903930664 
 batch loss: 27.643421173095703 
 batch loss: 27.632057189941406 
 batch loss: 27.637773513793945 
 batch loss: 27.658891677856445 
 batch loss: 27.731525421142578 
 batch loss: 27.753833770751953 
 batch loss: 27.58666229248047 
 batch loss: 27.63457489013672 
 batch loss: 27.66777992248535 
 batch loss: 27.592487335205078 
 batch loss: 28.04974937438965 
 batch loss: 27.62723731994629 
 batch loss: 27.728511810302734 
 batch loss: 27.534303665161133 
 Epoch 6 |Train loss: 29.64788227081299 |Validation loss: 27.679267597198486 
 batch loss: 27.532978057861328 
 batch loss: 27.70708656311035 
 batch loss: 27.474605560302734 
 batch loss: 27.66146469116211 
 batch loss: 27.42852783203125 
 batch loss: 27.332773208618164 
 batch loss: 27.471027374267578 
 batch loss: 27.950183868408203 
 batch loss: 27.424476623535156 
 batch loss: 27.625402450561523 
 batch loss: 27.48702049255371 
 batch loss: 27.53569793701172 
 batch loss: 27.509042739868164 
 batch loss: 27.3791446685791 
 batch loss: 27.698984146118164 
 batch loss: 27.218320846557617 
 batch loss: 27.25362777709961 
 batch loss: 27.25678253173828 
 batch loss: 27.140972137451172 
 batch loss: 27.2127742767334 
 batch loss: 27.22661018371582 
 batch loss: 27.152055740356445 
 batch loss: 27.000232696533203 
 batch loss: 27.21287727355957 
 batch loss: 26.88825798034668 
 batch loss: 26.839794158935547 
 batch loss: 26.912694931030273 
 batch loss: 26.980981826782227 
 batch loss: 26.934329986572266 
 batch loss: 26.930370330810547 
 batch loss: 26.769763946533203 
 batch loss: 26.68332290649414 
 batch loss: 26.69553565979004 
 batch loss: 26.589038848876953 
 batch loss: 26.68845558166504 
 batch loss: 26.691396713256836 
 batch loss: 26.528141021728516 
 batch loss: 26.572650909423828 
 batch loss: 27.065982818603516 
 batch loss: 26.483617782592773 
 batch loss: 26.67766761779785 
 batch loss: 26.50345802307129 
 batch loss: 26.49106788635254 
 batch loss: 26.507644653320312 
 batch loss: 26.989513397216797 
 batch loss: 26.47104835510254 
 batch loss: 26.359283447265625 
 batch loss: 26.337514877319336 
 batch loss: 26.514636993408203 
 batch loss: 26.39518928527832 
 batch loss: 26.289127349853516 
 batch loss: 26.422962188720703 
 batch loss: 26.24236488342285 
 batch loss: 26.2684326171875 
 batch loss: 26.296829223632812 
 batch loss: 26.15778923034668 
 batch loss: 26.13059425354004 
 batch loss: 26.072988510131836 
 batch loss: 26.33359718322754 
 batch loss: 26.134279251098633 
 batch loss: 25.916866302490234 
 batch loss: 26.19700050354004 
 batch loss: 26.238908767700195 
 batch loss: 25.830045700073242 
 batch loss: 25.77635955810547 
 batch loss: 26.566862106323242 
 batch loss: 26.173282623291016 
 batch loss: 26.365190505981445 
 batch loss: 26.05608558654785 
 batch loss: 25.817888259887695 
 batch loss: 25.8427734375 
 batch loss: 25.892536163330078 
 batch loss: 25.728290557861328 
 batch loss: 25.686506271362305 
 batch loss: 25.635412216186523 
 batch loss: 25.479150772094727 
 batch loss: 25.612279891967773 
 batch loss: 25.697431564331055 
 batch loss: 25.52707290649414 
 batch loss: 25.5076847076416 
 batch loss: 25.399585723876953 
 batch loss: 25.56838607788086 
 batch loss: 25.554689407348633 
 batch loss: 25.332740783691406 
 batch loss: 25.70429229736328 
 batch loss: 25.545930862426758 
 batch loss: 25.509662628173828 
 batch loss: 25.479555130004883 
 batch loss: 25.340789794921875 
 batch loss: 25.67439079284668 
 batch loss: 25.44265365600586 
 batch loss: 25.53182029724121 
 batch loss: 25.671579360961914 
 batch loss: 25.304271697998047 
 batch loss: 25.704652786254883 
 batch loss: 25.528728485107422 
 batch loss: 25.469316482543945 
 batch loss: 25.44449806213379 
 batch loss: 25.70845603942871 
 batch loss: 26.101961135864258 
 Epoch 7 |Train loss: 26.64113278388977 |Validation loss: 25.55089807510376 
 batch loss: 25.680715560913086 
 batch loss: 25.283538818359375 
 batch loss: 25.64706802368164 
 batch loss: 25.32965660095215 
 batch loss: 25.15913200378418 
 batch loss: 25.11994171142578 
 batch loss: 25.274438858032227 
 batch loss: 25.1445255279541 
 batch loss: 25.31104278564453 
 batch loss: 24.946491241455078 
 batch loss: 25.13888168334961 
 batch loss: 24.994646072387695 
 batch loss: 25.08690643310547 
 batch loss: 24.926910400390625 
 batch loss: 24.837326049804688 
 batch loss: 24.751121520996094 
 batch loss: 24.669315338134766 
 batch loss: 24.90932846069336 
 batch loss: 24.583791732788086 
 batch loss: 24.61671257019043 
 batch loss: 24.502578735351562 
 batch loss: 24.392696380615234 
 batch loss: 24.382951736450195 
 batch loss: 24.391782760620117 
 batch loss: 24.33722496032715 
 batch loss: 24.25993537902832 
 batch loss: 24.27651023864746 
 batch loss: 24.233612060546875 
 batch loss: 24.29069709777832 
 batch loss: 24.138153076171875 
 batch loss: 24.10089874267578 
 batch loss: 24.263458251953125 
 batch loss: 24.165027618408203 
 batch loss: 24.695871353149414 
 batch loss: 24.065311431884766 
 batch loss: 24.01032829284668 
 batch loss: 23.962881088256836 
 batch loss: 23.98146629333496 
 batch loss: 23.99053382873535 
 batch loss: 23.923437118530273 
 batch loss: 23.956575393676758 
 batch loss: 23.889019012451172 
 batch loss: 23.884313583374023 
 batch loss: 23.77234649658203 
 batch loss: 23.69148063659668 
 batch loss: 23.88587188720703 
 batch loss: 23.742490768432617 
 batch loss: 23.809415817260742 
 batch loss: 23.637210845947266 
 batch loss: 23.644840240478516 
 batch loss: 23.74135398864746 
 batch loss: 23.577146530151367 
 batch loss: 23.483274459838867 
 batch loss: 23.547893524169922 
 batch loss: 23.451519012451172 
 batch loss: 23.508808135986328 
 batch loss: 23.604324340820312 
 batch loss: 23.42104721069336 
 batch loss: 23.448856353759766 
 batch loss: 23.320056915283203 
 batch loss: 23.26596450805664 
 batch loss: 23.366165161132812 
 batch loss: 23.3321533203125 
 batch loss: 23.348512649536133 
 batch loss: 23.324424743652344 
 batch loss: 23.109285354614258 
 batch loss: 23.10490608215332 
 batch loss: 23.301231384277344 
 batch loss: 23.305696487426758 
 batch loss: 23.025575637817383 
 batch loss: 22.974130630493164 
 batch loss: 22.98110580444336 
 batch loss: 23.069589614868164 
 batch loss: 23.11195945739746 
 batch loss: 22.93104362487793 
 batch loss: 23.25531768798828 
 batch loss: 22.869436264038086 
 batch loss: 22.940959930419922 
 batch loss: 22.91855812072754 
 batch loss: 22.896251678466797 
 batch loss: 22.88951873779297 
 batch loss: 23.03481674194336 
 batch loss: 23.144914627075195 
 batch loss: 22.925355911254883 
 batch loss: 22.996204376220703 
 batch loss: 23.03287696838379 
 batch loss: 23.00950813293457 
 batch loss: 22.871671676635742 
 batch loss: 23.022058486938477 
 batch loss: 22.80217742919922 
 batch loss: 22.893754959106445 
 batch loss: 22.83932113647461 
 batch loss: 22.859472274780273 
 batch loss: 22.727643966674805 
 batch loss: 22.855119705200195 
 batch loss: 22.765846252441406 
 batch loss: 22.76192283630371 
 batch loss: 23.214981079101562 
 batch loss: 22.850017547607422 
 batch loss: 23.22859001159668 
 Epoch 8 |Train loss: 24.015337014198302 |Validation loss: 22.9362886428833 
 batch loss: 22.801864624023438 
 batch loss: 22.81049156188965 
 batch loss: 22.95024299621582 
 batch loss: 22.689294815063477 
 batch loss: 22.857585906982422 
 batch loss: 22.58000373840332 
 batch loss: 22.559160232543945 
 batch loss: 22.515390396118164 
 batch loss: 22.477624893188477 
 batch loss: 22.338153839111328 
 batch loss: 22.57306671142578 
 batch loss: 22.237096786499023 
 batch loss: 22.4774169921875 
 batch loss: 22.41270637512207 
 batch loss: 22.50713348388672 
 batch loss: 22.468290328979492 
 batch loss: 22.839210510253906 
 batch loss: 22.338756561279297 
 batch loss: 22.259244918823242 
 batch loss: 22.133302688598633 
 batch loss: 22.076862335205078 
 batch loss: 22.024675369262695 
 batch loss: 22.078948974609375 
 batch loss: 21.94111442565918 
 batch loss: 21.98465347290039 
 batch loss: 22.257343292236328 
 batch loss: 21.9962100982666 
 batch loss: 21.841068267822266 
 batch loss: 21.80014991760254 
 batch loss: 22.28824234008789 
 batch loss: 21.691972732543945 
 batch loss: 21.76066017150879 
 batch loss: 21.78500747680664 
 batch loss: 21.797258377075195 
 batch loss: 21.658674240112305 
 batch loss: 21.869338989257812 
 batch loss: 21.44874382019043 
 batch loss: 21.540889739990234 
 batch loss: 21.668123245239258 
 batch loss: 21.568180084228516 
 batch loss: 21.5117244720459 
 batch loss: 21.453346252441406 
 batch loss: 21.36313247680664 
 batch loss: 21.375783920288086 
 batch loss: 21.484601974487305 
 batch loss: 21.388586044311523 
 batch loss: 21.325029373168945 
 batch loss: 21.306575775146484 
 batch loss: 21.316299438476562 
 batch loss: 21.429073333740234 
 batch loss: 21.225635528564453 
 batch loss: 21.661596298217773 
 batch loss: 21.292591094970703 
 batch loss: 21.297138214111328 
 batch loss: 21.252622604370117 
 batch loss: 21.49887466430664 
 batch loss: 21.155088424682617 
 batch loss: 21.307170867919922 
 batch loss: 20.848783493041992 
 batch loss: 20.807397842407227 
 batch loss: 20.711578369140625 
 batch loss: 20.84843635559082 
 batch loss: 20.605789184570312 
 batch loss: 20.665407180786133 
 batch loss: 20.67142677307129 
 batch loss: 20.52839469909668 
 batch loss: 20.544185638427734 
 batch loss: 20.5267333984375 
 batch loss: 20.7745304107666 
 batch loss: 20.546537399291992 
 batch loss: 20.35791015625 
 batch loss: 20.601781845092773 
 batch loss: 20.413543701171875 
 batch loss: 20.258609771728516 
 batch loss: 20.33319664001465 
 batch loss: 20.361948013305664 
 batch loss: 20.35626220703125 
 batch loss: 20.26763343811035 
 batch loss: 20.50636100769043 
 batch loss: 20.406471252441406 
 batch loss: 20.29079246520996 
 batch loss: 20.117172241210938 
 batch loss: 20.27306365966797 
 batch loss: 20.214155197143555 
 batch loss: 20.478771209716797 
 batch loss: 20.406949996948242 
 batch loss: 20.14919090270996 
 batch loss: 20.1297607421875 
 batch loss: 20.281936645507812 
 batch loss: 20.157550811767578 
 batch loss: 20.158611297607422 
 batch loss: 20.22957992553711 
 batch loss: 20.137052536010742 
 batch loss: 20.116762161254883 
 batch loss: 20.103172302246094 
 batch loss: 20.12775993347168 
 batch loss: 20.070514678955078 
 batch loss: 20.117713928222656 
 batch loss: 20.206134796142578 
 batch loss: 20.172138214111328 
 Epoch 9 |Train loss: 21.556149315834045 |Validation loss: 20.196939182281493 
 batch loss: 20.529600143432617 
 batch loss: 20.12046241760254 
 batch loss: 20.387357711791992 
 batch loss: 20.16340446472168 
 batch loss: 19.970685958862305 
 batch loss: 19.760440826416016 
 batch loss: 19.85457420349121 
 batch loss: 19.909461975097656 
 batch loss: 19.883621215820312 
 batch loss: 19.68336296081543 
 batch loss: 19.86357307434082 
 batch loss: 19.64327049255371 
 batch loss: 19.586688995361328 
 batch loss: 19.583106994628906 
 batch loss: 19.640554428100586 
 batch loss: 19.562673568725586 
 batch loss: 19.507831573486328 
 batch loss: 19.402708053588867 
 batch loss: 19.57319450378418 
 batch loss: 19.638765335083008 
 batch loss: 19.389793395996094 
 batch loss: 19.666048049926758 
 batch loss: 19.446754455566406 
 batch loss: 19.47939682006836 
 batch loss: 19.37281608581543 
 batch loss: 19.343536376953125 
 batch loss: 19.333919525146484 
 batch loss: 19.242246627807617 
 batch loss: 19.27876853942871 
 batch loss: 19.179824829101562 
 batch loss: 19.198801040649414 
 batch loss: 19.256454467773438 
 batch loss: 19.084686279296875 
 batch loss: 19.23250389099121 
 batch loss: 19.161426544189453 
 batch loss: 19.269559860229492 
 batch loss: 19.057096481323242 
 batch loss: 19.18410301208496 
 batch loss: 19.27956771850586 
 batch loss: 19.075109481811523 
 batch loss: 19.17293357849121 
 batch loss: 19.40790557861328 
 batch loss: 19.236175537109375 
 batch loss: 19.14929962158203 
 batch loss: 19.01812171936035 
 batch loss: 18.977008819580078 
 batch loss: 18.929018020629883 
 batch loss: 19.005075454711914 
 batch loss: 18.810035705566406 
 batch loss: 18.80176544189453 
 batch loss: 19.27045440673828 
 batch loss: 18.994787216186523 
 batch loss: 19.014141082763672 
 batch loss: 18.845626831054688 
 batch loss: 18.905506134033203 
 batch loss: 18.88663101196289 
 batch loss: 18.801856994628906 
 batch loss: 18.798009872436523 
 batch loss: 18.624717712402344 
 batch loss: 18.712831497192383 
 batch loss: 18.729476928710938 
 batch loss: 18.6705265045166 
 batch loss: 18.70053482055664 
 batch loss: 18.566699981689453 
 batch loss: 18.672128677368164 
 batch loss: 18.661211013793945 
 batch loss: 18.476224899291992 
 batch loss: 18.568307876586914 
 batch loss: 18.532426834106445 
 batch loss: 18.585222244262695 
 batch loss: 18.407730102539062 
 batch loss: 18.45725440979004 
 batch loss: 18.44485092163086 
 batch loss: 18.452144622802734 
 batch loss: 18.346160888671875 
 batch loss: 18.366405487060547 
 batch loss: 18.340557098388672 
 batch loss: 18.323001861572266 
 batch loss: 18.274578094482422 
 batch loss: 18.314598083496094 
 batch loss: 18.17827796936035 
 batch loss: 18.376998901367188 
 batch loss: 18.234821319580078 
 batch loss: 18.249038696289062 
 batch loss: 18.33477020263672 
 batch loss: 18.296661376953125 
 batch loss: 18.245441436767578 
 batch loss: 18.233619689941406 
 batch loss: 18.38029670715332 
 batch loss: 18.25701904296875 
 batch loss: 18.146276473999023 
 batch loss: 18.22686195373535 
 batch loss: 18.247621536254883 
 batch loss: 18.233129501342773 
 batch loss: 18.46601676940918 
 batch loss: 18.22243309020996 
 batch loss: 18.23888397216797 
 batch loss: 18.271909713745117 
 batch loss: 19.393049240112305 
 batch loss: 18.227867126464844 
 Epoch 10 |Train loss: 19.13812119960785 |Validation loss: 18.32304973602295 
 batch loss: 18.29277801513672 
 batch loss: 18.30082130432129 
 batch loss: 18.27488899230957 
 batch loss: 18.311071395874023 
 batch loss: 18.18452262878418 
 batch loss: 18.260297775268555 
 batch loss: 18.219905853271484 
 batch loss: 18.23453140258789 
 batch loss: 18.217117309570312 
 batch loss: 18.067035675048828 
 batch loss: 18.01306915283203 
 batch loss: 18.166255950927734 
 batch loss: 18.08907699584961 
 batch loss: 17.98857307434082 
 batch loss: 17.936521530151367 
 batch loss: 17.93575668334961 
 batch loss: 18.022769927978516 
 batch loss: 18.04509925842285 
 batch loss: 17.96356773376465 
 batch loss: 18.009389877319336 
 batch loss: 17.891145706176758 
 batch loss: 18.004314422607422 
 batch loss: 17.866823196411133 
 batch loss: 17.835182189941406 
 batch loss: 17.888809204101562 
 batch loss: 17.84170150756836 
 batch loss: 17.884775161743164 
 batch loss: 17.833942413330078 
 batch loss: 17.762409210205078 
 batch loss: 17.779422760009766 
 batch loss: 17.819673538208008 
 batch loss: 17.682079315185547 
 batch loss: 17.783761978149414 
 batch loss: 17.84241485595703 
 batch loss: 17.66655158996582 
 batch loss: 17.766199111938477 
 batch loss: 17.862548828125 
 batch loss: 18.107284545898438 
 batch loss: 17.637849807739258 
 batch loss: 17.718263626098633 
 batch loss: 17.78620147705078 
 batch loss: 17.81656265258789 
 batch loss: 17.73488426208496 
 batch loss: 17.635562896728516 
 batch loss: 17.66297149658203 
 batch loss: 17.813505172729492 
 batch loss: 17.690338134765625 
 batch loss: 17.58127212524414 
 batch loss: 17.547121047973633 
 batch loss: 17.473691940307617 
 batch loss: 17.4890079498291 
 batch loss: 17.610626220703125 
 batch loss: 17.5864200592041 
 batch loss: 17.475919723510742 
 batch loss: 17.461984634399414 
 batch loss: 17.467880249023438 
 batch loss: 17.402116775512695 
 batch loss: 17.540910720825195 
 batch loss: 17.35637664794922 
 batch loss: 17.380210876464844 
 batch loss: 17.2598876953125 
 batch loss: 17.428354263305664 
 batch loss: 17.313825607299805 
 batch loss: 17.18604850769043 
 batch loss: 17.211177825927734 
 batch loss: 17.398273468017578 
 batch loss: 17.099397659301758 
 batch loss: 17.13446807861328 
 batch loss: 17.405973434448242 
 batch loss: 17.10805320739746 
 batch loss: 17.026830673217773 
 batch loss: 17.12347984313965 
 batch loss: 17.064668655395508 
 batch loss: 16.979406356811523 
 batch loss: 16.966665267944336 
 batch loss: 16.855192184448242 
 batch loss: 16.881296157836914 
 batch loss: 16.877065658569336 
 batch loss: 16.70645523071289 
 batch loss: 16.76548194885254 
 batch loss: 16.721403121948242 
 batch loss: 16.799638748168945 
 batch loss: 16.95014762878418 
 batch loss: 16.849430084228516 
 batch loss: 16.70121955871582 
 batch loss: 16.969810485839844 
 batch loss: 16.705890655517578 
 batch loss: 16.85056495666504 
 batch loss: 16.766456604003906 
 batch loss: 16.990793228149414 
 batch loss: 16.7554931640625 
 batch loss: 16.81719207763672 
 batch loss: 17.013671875 
 batch loss: 16.845623016357422 
 batch loss: 16.90026092529297 
 batch loss: 16.873777389526367 
 batch loss: 17.036853790283203 
 batch loss: 16.876707077026367 
 batch loss: 16.83098793029785 
 batch loss: 16.77766990661621 
 Epoch 11 |Train loss: 17.65392212867737 |Validation loss: 16.851679611206055 
 batch loss: 16.885358810424805 
 batch loss: 16.809284210205078 
 batch loss: 16.706995010375977 
 batch loss: 16.689863204956055 
 batch loss: 16.7717342376709 
 batch loss: 16.751171112060547 
 batch loss: 16.819473266601562 
 batch loss: 16.58263397216797 
 batch loss: 16.543081283569336 
 batch loss: 16.538543701171875 
 batch loss: 16.74647331237793 
 batch loss: 16.501529693603516 
 batch loss: 16.679582595825195 
 batch loss: 16.324655532836914 
 batch loss: 16.323625564575195 
 batch loss: 16.291006088256836 
 batch loss: 16.393577575683594 
 batch loss: 16.020034790039062 
 batch loss: 15.962486267089844 
 batch loss: 16.011127471923828 
 batch loss: 15.873994827270508 
 batch loss: 15.9033203125 
 batch loss: 15.875041961669922 
 batch loss: 15.816286087036133 
 batch loss: 15.756186485290527 
 batch loss: 15.860271453857422 
 batch loss: 15.713692665100098 
 batch loss: 15.742979049682617 
 batch loss: 15.685892105102539 
 batch loss: 15.77505111694336 
 batch loss: 15.670514106750488 
 batch loss: 15.475576400756836 
 batch loss: 15.508818626403809 
 batch loss: 15.456335067749023 
 batch loss: 15.428824424743652 
 batch loss: 15.41474437713623 
 batch loss: 15.427425384521484 
 batch loss: 15.436464309692383 
 batch loss: 15.64111614227295 
 batch loss: 15.370429992675781 
 batch loss: 15.4433012008667 
 batch loss: 15.391213417053223 
 batch loss: 15.32487964630127 
 batch loss: 15.322232246398926 
 batch loss: 15.333874702453613 
 batch loss: 15.264073371887207 
 batch loss: 15.222217559814453 
 batch loss: 15.160054206848145 
 batch loss: 15.161986351013184 
 batch loss: 15.175236701965332 
 batch loss: 15.097296714782715 
 batch loss: 15.208245277404785 
 batch loss: 15.137653350830078 
 batch loss: 15.106497764587402 
 batch loss: 15.28762435913086 
 batch loss: 15.143689155578613 
 batch loss: 15.186015129089355 
 batch loss: 15.433059692382812 
 batch loss: 15.327154159545898 
 batch loss: 15.119170188903809 
 batch loss: 15.111835479736328 
 batch loss: 15.0770845413208 
 batch loss: 15.041632652282715 
 batch loss: 15.080073356628418 
 batch loss: 15.02874755859375 
 batch loss: 15.066681861877441 
 batch loss: 15.173052787780762 
 batch loss: 15.09327507019043 
 batch loss: 14.934032440185547 
 batch loss: 14.969823837280273 
 batch loss: 14.914782524108887 
 batch loss: 14.949853897094727 
 batch loss: 15.007184028625488 
 batch loss: 14.908215522766113 
 batch loss: 14.810545921325684 
 batch loss: 14.921697616577148 
 batch loss: 14.821450233459473 
 batch loss: 14.83549690246582 
 batch loss: 14.877510070800781 
 batch loss: 14.828608512878418 
 batch loss: 14.779497146606445 
 batch loss: 14.789041519165039 
 batch loss: 14.903362274169922 
 batch loss: 14.858563423156738 
 batch loss: 14.908398628234863 
 batch loss: 14.76455020904541 
 batch loss: 14.812579154968262 
 batch loss: 14.793623924255371 
 batch loss: 14.798819541931152 
 batch loss: 14.976402282714844 
 batch loss: 15.037675857543945 
 batch loss: 14.88393783569336 
 batch loss: 14.843421936035156 
 batch loss: 14.755330085754395 
 batch loss: 14.784071922302246 
 batch loss: 14.814695358276367 
 batch loss: 14.808670043945312 
 batch loss: 14.728734016418457 
 batch loss: 14.715912818908691 
 batch loss: 14.760677337646484 
 Epoch 12 |Train loss: 15.593528282642364 |Validation loss: 14.825898265838623 
 batch loss: 14.756254196166992 
 batch loss: 14.69164752960205 
 batch loss: 14.66151237487793 
 batch loss: 14.743128776550293 
 batch loss: 14.743406295776367 
 batch loss: 14.598348617553711 
 batch loss: 14.65356159210205 
 batch loss: 14.623525619506836 
 batch loss: 14.594548225402832 
 batch loss: 14.621532440185547 
 batch loss: 14.62950611114502 
 batch loss: 14.515158653259277 
 batch loss: 14.507515907287598 
 batch loss: 14.616950035095215 
 batch loss: 14.561047554016113 
 batch loss: 14.46928882598877 
 batch loss: 14.434020042419434 
 batch loss: 14.4618558883667 
 batch loss: 14.451743125915527 
 batch loss: 14.502115249633789 
 batch loss: 14.482086181640625 
 batch loss: 14.428802490234375 
 batch loss: 14.391874313354492 
 batch loss: 14.356038093566895 
 batch loss: 14.394753456115723 
 batch loss: 14.33865737915039 
 batch loss: 14.384119033813477 
 batch loss: 14.36662769317627 
 batch loss: 14.283402442932129 
 batch loss: 14.348738670349121 
 batch loss: 14.30963134765625 
 batch loss: 14.450091361999512 
 batch loss: 14.296185493469238 
 batch loss: 14.306734085083008 
 batch loss: 14.281211853027344 
 batch loss: 14.294702529907227 
 batch loss: 14.280149459838867 
 batch loss: 14.326692581176758 
 batch loss: 14.301360130310059 
 batch loss: 14.333502769470215 
 batch loss: 14.275390625 
 batch loss: 14.279887199401855 
 batch loss: 14.228320121765137 
 batch loss: 14.225801467895508 
 batch loss: 14.264129638671875 
 batch loss: 14.146336555480957 
 batch loss: 14.260618209838867 
 batch loss: 14.172501564025879 
 batch loss: 14.307608604431152 
 batch loss: 14.143418312072754 
 batch loss: 14.12024211883545 
 batch loss: 14.104911804199219 
 batch loss: 14.10882568359375 
 batch loss: 14.039347648620605 
 batch loss: 14.114449501037598 
 batch loss: 14.1175537109375 
 batch loss: 14.07286548614502 
 batch loss: 14.025813102722168 
 batch loss: 14.065591812133789 
 batch loss: 14.013504028320312 
 batch loss: 13.976655006408691 
 batch loss: 13.982771873474121 
 batch loss: 13.993011474609375 
 batch loss: 14.12650203704834 
 batch loss: 14.03879165649414 
 batch loss: 13.971466064453125 
 batch loss: 13.984183311462402 
 batch loss: 13.953310012817383 
 batch loss: 13.95675277709961 
 batch loss: 14.012826919555664 
 batch loss: 13.941972732543945 
 batch loss: 13.887864112854004 
 batch loss: 13.87630844116211 
 batch loss: 13.91427230834961 
 batch loss: 13.896737098693848 
 batch loss: 13.831374168395996 
 batch loss: 13.881646156311035 
 batch loss: 13.91100788116455 
 batch loss: 13.808030128479004 
 batch loss: 13.771844863891602 
 batch loss: 13.87972354888916 
 batch loss: 13.83176040649414 
 batch loss: 13.894976615905762 
 batch loss: 13.829863548278809 
 batch loss: 13.836777687072754 
 batch loss: 13.803976058959961 
 batch loss: 13.81480884552002 
 batch loss: 13.852110862731934 
 batch loss: 13.821374893188477 
 batch loss: 13.897984504699707 
 batch loss: 13.782635688781738 
 batch loss: 13.794002532958984 
 batch loss: 13.865635871887207 
 batch loss: 13.82669734954834 
 batch loss: 13.795180320739746 
 batch loss: 13.854722023010254 
 batch loss: 13.816496849060059 
 batch loss: 13.825345993041992 
 batch loss: 13.793314933776855 
 batch loss: 13.832425117492676 
 Epoch 13 |Train loss: 14.257455933094025 |Validation loss: 13.832490682601929 
 batch loss: 13.945242881774902 
 batch loss: 13.868400573730469 
 batch loss: 13.825711250305176 
 batch loss: 13.850556373596191 
 batch loss: 13.85396957397461 
 batch loss: 13.787577629089355 
 batch loss: 13.833943367004395 
 batch loss: 13.760695457458496 
 batch loss: 13.753246307373047 
 batch loss: 13.760004997253418 
 batch loss: 13.765483856201172 
 batch loss: 13.972861289978027 
 batch loss: 13.724386215209961 
 batch loss: 13.713656425476074 
 batch loss: 13.735647201538086 
 batch loss: 13.706681251525879 
 batch loss: 13.678759574890137 
 batch loss: 13.627090454101562 
 batch loss: 13.66558837890625 
 batch loss: 13.590392112731934 
 batch loss: 13.732742309570312 
 batch loss: 13.859811782836914 
 batch loss: 13.613662719726562 
 batch loss: 13.70831298828125 
 batch loss: 13.601117134094238 
 batch loss: 13.729581832885742 
 batch loss: 13.624359130859375 
 batch loss: 13.721884727478027 
 batch loss: 13.632139205932617 
 batch loss: 13.551884651184082 
 batch loss: 13.580489158630371 
 batch loss: 13.62328052520752 
 batch loss: 13.55675220489502 
 batch loss: 13.665117263793945 
 batch loss: 13.518345832824707 
 batch loss: 13.717118263244629 
 batch loss: 13.5722017288208 
 batch loss: 13.62371826171875 
 batch loss: 13.545575141906738 
 batch loss: 13.551122665405273 
 batch loss: 13.467767715454102 
 batch loss: 13.461532592773438 
 batch loss: 13.419608116149902 
 batch loss: 13.454427719116211 
 batch loss: 13.51833724975586 
 batch loss: 13.416501998901367 
 batch loss: 13.45551872253418 
 batch loss: 13.651496887207031 
 batch loss: 13.46006965637207 
 batch loss: 13.343559265136719 
 batch loss: 13.389202117919922 
 batch loss: 13.580753326416016 
 batch loss: 13.39768123626709 
 batch loss: 13.43714714050293 
 batch loss: 13.491961479187012 
 batch loss: 13.34811019897461 
 batch loss: 13.411300659179688 
 batch loss: 13.350116729736328 
 batch loss: 13.429427146911621 
 batch loss: 13.30288028717041 
 batch loss: 13.39046859741211 
 batch loss: 13.347020149230957 
 batch loss: 13.460290908813477 
 batch loss: 13.283685684204102 
 batch loss: 13.276036262512207 
 batch loss: 13.356356620788574 
 batch loss: 13.290948867797852 
 batch loss: 13.267839431762695 
 batch loss: 13.233758926391602 
 batch loss: 13.242742538452148 
 batch loss: 13.253229141235352 
 batch loss: 13.272584915161133 
 batch loss: 13.222673416137695 
 batch loss: 13.21222972869873 
 batch loss: 13.16804027557373 
 batch loss: 13.152996063232422 
 batch loss: 13.202278137207031 
 batch loss: 13.152547836303711 
 batch loss: 13.145304679870605 
 batch loss: 13.180741310119629 
 batch loss: 13.110749244689941 
 batch loss: 13.184721946716309 
 batch loss: 13.155187606811523 
 batch loss: 13.191356658935547 
 batch loss: 13.188285827636719 
 batch loss: 13.157609939575195 
 batch loss: 13.178220748901367 
 batch loss: 13.19129467010498 
 batch loss: 13.189396858215332 
 batch loss: 13.135289192199707 
 batch loss: 13.123503684997559 
 batch loss: 13.179413795471191 
 batch loss: 13.130855560302734 
 batch loss: 13.147038459777832 
 batch loss: 13.134087562561035 
 batch loss: 13.167943954467773 
 batch loss: 13.138996124267578 
 batch loss: 13.196255683898926 
 batch loss: 13.161308288574219 
 batch loss: 13.131780624389648 
 Epoch 14 |Train loss: 13.525603580474854 |Validation loss: 13.159664821624755 
 batch loss: 13.169129371643066 
 batch loss: 13.1161527633667 
 batch loss: 13.187459945678711 
 batch loss: 13.072680473327637 
 batch loss: 13.127357482910156 
 batch loss: 13.094316482543945 
 batch loss: 13.129167556762695 
 batch loss: 13.07641887664795 
 batch loss: 13.087467193603516 
 batch loss: 13.10891342163086 
 batch loss: 13.039710998535156 
 batch loss: 13.062304496765137 
 batch loss: 13.0619478225708 
 batch loss: 13.08134651184082 
 batch loss: 13.2418851852417 
 batch loss: 13.034537315368652 
 batch loss: 13.068543434143066 
 batch loss: 13.03036880493164 
 batch loss: 13.031647682189941 
 batch loss: 13.035669326782227 
 batch loss: 13.012734413146973 
 batch loss: 13.029833793640137 
 batch loss: 12.977500915527344 
 batch loss: 13.049307823181152 
 batch loss: 12.990559577941895 
 batch loss: 12.992487907409668 
 batch loss: 12.93051528930664 
 batch loss: 12.982396125793457 
 batch loss: 12.928509712219238 
 batch loss: 12.927328109741211 
 batch loss: 12.926587104797363 
 batch loss: 12.939321517944336 
 batch loss: 12.884602546691895 
 batch loss: 12.914020538330078 
 batch loss: 12.873562812805176 
 batch loss: 12.923898696899414 
 batch loss: 12.915069580078125 
 batch loss: 12.884934425354004 
 batch loss: 12.870909690856934 
 batch loss: 12.845935821533203 
 batch loss: 12.860515594482422 
 batch loss: 12.840435028076172 
 batch loss: 12.897884368896484 
 batch loss: 12.815019607543945 
 batch loss: 12.823029518127441 
 batch loss: 12.826306343078613 
 batch loss: 12.790283203125 
 batch loss: 12.799479484558105 
 batch loss: 12.771645545959473 
 batch loss: 12.768177032470703 
 batch loss: 12.759533882141113 
 batch loss: 12.761262893676758 
 batch loss: 12.75312328338623 
 batch loss: 12.747957229614258 
 batch loss: 12.711605072021484 
 batch loss: 12.717973709106445 
 batch loss: 12.722269058227539 
 batch loss: 12.765923500061035 
 batch loss: 12.717586517333984 
 batch loss: 12.69845962524414 
 batch loss: 12.729639053344727 
 batch loss: 12.671941757202148 
 batch loss: 12.701261520385742 
 batch loss: 12.631667137145996 
 batch loss: 12.65048885345459 
 batch loss: 12.653531074523926 
 batch loss: 12.620719909667969 
 batch loss: 12.600080490112305 
 batch loss: 12.63413143157959 
 batch loss: 12.762383460998535 
 batch loss: 12.609566688537598 
 batch loss: 12.662175178527832 
 batch loss: 12.580448150634766 
 batch loss: 12.611209869384766 
 batch loss: 12.635049819946289 
 batch loss: 12.612278938293457 
 batch loss: 12.555883407592773 
 batch loss: 12.581894874572754 
 batch loss: 12.548447608947754 
 batch loss: 12.583659172058105 
 batch loss: 12.550235748291016 
 batch loss: 12.548510551452637 
 batch loss: 12.599393844604492 
 batch loss: 12.582322120666504 
 batch loss: 12.583639144897461 
 batch loss: 12.603005409240723 
 batch loss: 12.576729774475098 
 batch loss: 12.577714920043945 
 batch loss: 12.545683860778809 
 batch loss: 12.557639122009277 
 batch loss: 12.576964378356934 
 batch loss: 12.538553237915039 
 batch loss: 12.573051452636719 
 batch loss: 12.555643081665039 
 batch loss: 12.565105438232422 
 batch loss: 12.539517402648926 
 batch loss: 12.80299186706543 
 batch loss: 12.59750747680664 
 batch loss: 12.56942081451416 
 batch loss: 12.619706153869629 
 Epoch 15 |Train loss: 12.86052463054657 |Validation loss: 12.583166790008544 
 batch loss: 12.55433177947998 
 batch loss: 12.57082462310791 
 batch loss: 12.514053344726562 
 batch loss: 12.507824897766113 
 batch loss: 12.53000259399414 
 batch loss: 12.532184600830078 
 batch loss: 12.483765602111816 
 batch loss: 12.523191452026367 
 batch loss: 12.496800422668457 
 batch loss: 12.495990753173828 
 batch loss: 12.461241722106934 
 batch loss: 12.463085174560547 
 batch loss: 12.449389457702637 
 batch loss: 12.460286140441895 
 batch loss: 12.496879577636719 
 batch loss: 12.464486122131348 
 batch loss: 12.40847110748291 
 batch loss: 12.475242614746094 
 batch loss: 12.399807929992676 
 batch loss: 12.403658866882324 
 batch loss: 12.384222984313965 
 batch loss: 12.431032180786133 
 batch loss: 12.3666353225708 
 batch loss: 12.344242095947266 
 batch loss: 12.40304946899414 
 batch loss: 12.38702392578125 
 batch loss: 12.353086471557617 
 batch loss: 12.365525245666504 
 batch loss: 12.345656394958496 
 batch loss: 12.34703540802002 
 batch loss: 12.36928653717041 
 batch loss: 12.335225105285645 
 batch loss: 12.290889739990234 
 batch loss: 12.300337791442871 
 batch loss: 12.316839218139648 
 batch loss: 12.341851234436035 
 batch loss: 12.303053855895996 
 batch loss: 12.267462730407715 
 batch loss: 12.266855239868164 
 batch loss: 12.286680221557617 
 batch loss: 12.224382400512695 
 batch loss: 12.279255867004395 
 batch loss: 12.273067474365234 
 batch loss: 12.273260116577148 
 batch loss: 12.269591331481934 
 batch loss: 12.232376098632812 
 batch loss: 12.222090721130371 
 batch loss: 12.22479248046875 
 batch loss: 12.204785346984863 
 batch loss: 12.182696342468262 
 batch loss: 12.20873737335205 
 batch loss: 12.19217586517334 
 batch loss: 12.210977554321289 
 batch loss: 12.178947448730469 
 batch loss: 12.1520414352417 
 batch loss: 12.14395523071289 
 batch loss: 12.171951293945312 
 batch loss: 12.180778503417969 
 batch loss: 12.133188247680664 
 batch loss: 12.121492385864258 
 batch loss: 12.1427001953125 
 batch loss: 12.17746639251709 
 batch loss: 12.136425018310547 
 batch loss: 12.133151054382324 
 batch loss: 12.086797714233398 
 batch loss: 12.114157676696777 
 batch loss: 12.064300537109375 
 batch loss: 12.066743850708008 
 batch loss: 12.065423965454102 
 batch loss: 12.099940299987793 
 batch loss: 12.04537296295166 
 batch loss: 12.056848526000977 
 batch loss: 12.028881072998047 
 batch loss: 12.056126594543457 
 batch loss: 11.994102478027344 
 batch loss: 12.004691123962402 
 batch loss: 12.000520706176758 
 batch loss: 12.01114273071289 
 batch loss: 11.976768493652344 
 batch loss: 11.989656448364258 
 batch loss: 11.955410957336426 
 batch loss: 11.968106269836426 
 batch loss: 11.968049049377441 
 batch loss: 11.97410774230957 
 batch loss: 11.962615966796875 
 batch loss: 11.978283882141113 
 batch loss: 11.998059272766113 
 batch loss: 11.982276916503906 
 batch loss: 11.965810775756836 
 batch loss: 11.984404563903809 
 batch loss: 11.956798553466797 
 batch loss: 11.959684371948242 
 batch loss: 11.967934608459473 
 batch loss: 11.948712348937988 
 batch loss: 12.002246856689453 
 batch loss: 11.91997241973877 
 batch loss: 11.956305503845215 
 batch loss: 12.03271484375 
 batch loss: 11.957873344421387 
 batch loss: 12.028409004211426 
 Epoch 16 |Train loss: 12.272865891456604 |Validation loss: 11.973388862609863 
 batch loss: 11.965642929077148 
 batch loss: 11.925558090209961 
 batch loss: 11.935612678527832 
 batch loss: 11.920788764953613 
 batch loss: 11.929359436035156 
 batch loss: 11.902238845825195 
 batch loss: 11.910964965820312 
 batch loss: 11.908109664916992 
 batch loss: 11.89334487915039 
 batch loss: 11.8685302734375 
 batch loss: 11.894464492797852 
 batch loss: 11.886208534240723 
 batch loss: 11.921321868896484 
 batch loss: 11.899251937866211 
 batch loss: 11.858292579650879 
 batch loss: 11.877120971679688 
 batch loss: 11.891655921936035 
 batch loss: 11.914101600646973 
 batch loss: 11.834088325500488 
 batch loss: 11.83364486694336 
 batch loss: 11.85532283782959 
 batch loss: 11.810059547424316 
 batch loss: 11.849699020385742 
 batch loss: 11.851168632507324 
 batch loss: 11.826581954956055 
 batch loss: 11.845990180969238 
 batch loss: 11.83912467956543 
 batch loss: 11.80396556854248 
 batch loss: 11.84751033782959 
 batch loss: 11.79784107208252 
 batch loss: 11.783366203308105 
 batch loss: 11.789929389953613 
 batch loss: 11.843772888183594 
 batch loss: 11.794224739074707 
 batch loss: 11.826693534851074 
 batch loss: 11.84317398071289 
 batch loss: 11.808740615844727 
 batch loss: 11.753305435180664 
 batch loss: 11.807515144348145 
 batch loss: 11.802444458007812 
 batch loss: 11.74560546875 
 batch loss: 11.736563682556152 
 batch loss: 11.759637832641602 
 batch loss: 11.73331356048584 
 batch loss: 11.75536060333252 
 batch loss: 11.72648811340332 
 batch loss: 11.721948623657227 
 batch loss: 11.722597122192383 
 batch loss: 11.708176612854004 
 batch loss: 11.699222564697266 
 batch loss: 11.698988914489746 
 batch loss: 11.679302215576172 
 batch loss: 11.678613662719727 
 batch loss: 11.7002592086792 
 batch loss: 11.6694974899292 
 batch loss: 11.686266899108887 
 batch loss: 11.657699584960938 
 batch loss: 11.673543930053711 
 batch loss: 11.659671783447266 
 batch loss: 11.673249244689941 
 batch loss: 11.631478309631348 
 batch loss: 11.623074531555176 
 batch loss: 11.654891014099121 
 batch loss: 11.648161888122559 
 batch loss: 11.760701179504395 
 batch loss: 11.612361907958984 
 batch loss: 11.576970100402832 
 batch loss: 11.819352149963379 
 batch loss: 11.703882217407227 
 batch loss: 11.635250091552734 
 batch loss: 11.607389450073242 
 batch loss: 11.645336151123047 
 batch loss: 11.592528343200684 
 batch loss: 11.607644081115723 
 batch loss: 11.589629173278809 
 batch loss: 11.653192520141602 
 batch loss: 11.604150772094727 
 batch loss: 11.600252151489258 
 batch loss: 11.62540340423584 
 batch loss: 11.571327209472656 
 batch loss: 11.583073616027832 
 batch loss: 11.636861801147461 
 batch loss: 11.60680103302002 
 batch loss: 11.606821060180664 
 batch loss: 11.628995895385742 
 batch loss: 11.599146842956543 
 batch loss: 11.574319839477539 
 batch loss: 11.5950345993042 
 batch loss: 11.614897727966309 
 batch loss: 11.61976146697998 
 batch loss: 11.627636909484863 
 batch loss: 11.572629928588867 
 batch loss: 11.595704078674316 
 batch loss: 11.557555198669434 
 batch loss: 11.636028289794922 
 batch loss: 11.553565979003906 
 batch loss: 11.605310440063477 
 batch loss: 11.592801094055176 
 batch loss: 11.58346939086914 
 batch loss: 11.607015609741211 
 Epoch 17 |Train loss: 11.764996445178985 |Validation loss: 11.59987154006958 
 batch loss: 11.603224754333496 
 batch loss: 11.56263256072998 
 batch loss: 11.585131645202637 
 batch loss: 11.566036224365234 
 batch loss: 11.579693794250488 
 batch loss: 11.58606243133545 
 batch loss: 11.563177108764648 
 batch loss: 11.560210227966309 
 batch loss: 11.54732608795166 
 batch loss: 11.524481773376465 
 batch loss: 11.556112289428711 
 batch loss: 11.522563934326172 
 batch loss: 11.46300220489502 
 batch loss: 11.513904571533203 
 batch loss: 11.469252586364746 
 batch loss: 11.486371040344238 
 batch loss: 11.44619369506836 
 batch loss: 11.458454132080078 
 batch loss: 11.541169166564941 
 batch loss: 11.5121431350708 
 batch loss: 11.45545482635498 
 batch loss: 11.445438385009766 
 batch loss: 11.449810981750488 
 batch loss: 11.488100051879883 
 batch loss: 11.428571701049805 
 batch loss: 11.433512687683105 
 batch loss: 11.465312004089355 
 batch loss: 11.415006637573242 
 batch loss: 11.44221305847168 
 batch loss: 11.459632873535156 
 batch loss: 11.416749000549316 
 batch loss: 11.400458335876465 
 batch loss: 11.358782768249512 
 batch loss: 11.375466346740723 
 batch loss: 11.382240295410156 
 batch loss: 11.36664867401123 
 batch loss: 11.416187286376953 
 batch loss: 11.359210014343262 
 batch loss: 11.329090118408203 
 batch loss: 11.343486785888672 
 batch loss: 11.379024505615234 
 batch loss: 11.316780090332031 
 batch loss: 11.293482780456543 
 batch loss: 11.336694717407227 
 batch loss: 11.288208961486816 
 batch loss: 11.288189888000488 
 batch loss: 11.286413192749023 
 batch loss: 11.284371376037598 
 batch loss: 11.25769329071045 
 batch loss: 11.302385330200195 
 batch loss: 11.231226921081543 
 batch loss: 11.245139122009277 
 batch loss: 11.241615295410156 
 batch loss: 11.258506774902344 
 batch loss: 11.234637260437012 
 batch loss: 11.204490661621094 
 batch loss: 11.217367172241211 
 batch loss: 11.232632637023926 
 batch loss: 11.229708671569824 
 batch loss: 11.190442085266113 
 batch loss: 11.1809663772583 
 batch loss: 11.229364395141602 
 batch loss: 11.185619354248047 
 batch loss: 11.224565505981445 
 batch loss: 11.184087753295898 
 batch loss: 11.193341255187988 
 batch loss: 11.178362846374512 
 batch loss: 11.179625511169434 
 batch loss: 11.171486854553223 
 batch loss: 11.16601848602295 
 batch loss: 11.18811321258545 
 batch loss: 11.157902717590332 
 batch loss: 11.128342628479004 
 batch loss: 11.126704216003418 
 batch loss: 11.131425857543945 
 batch loss: 11.16697883605957 
 batch loss: 11.11833667755127 
 batch loss: 11.118265151977539 
 batch loss: 11.120994567871094 
 batch loss: 11.114912986755371 
 batch loss: 11.121400833129883 
 batch loss: 11.112545013427734 
 batch loss: 11.112146377563477 
 batch loss: 11.114556312561035 
 batch loss: 11.117266654968262 
 batch loss: 11.114620208740234 
 batch loss: 11.133027076721191 
 batch loss: 11.107803344726562 
 batch loss: 11.105987548828125 
 batch loss: 11.114990234375 
 batch loss: 11.061823844909668 
 batch loss: 11.07624340057373 
 batch loss: 11.107991218566895 
 batch loss: 11.096044540405273 
 batch loss: 11.128687858581543 
 batch loss: 11.077101707458496 
 batch loss: 11.124763488769531 
 batch loss: 11.09416675567627 
 batch loss: 11.138921737670898 
 batch loss: 11.136402130126953 
 Epoch 18 |Train loss: 11.343286776542664 |Validation loss: 11.109824514389038 
 batch loss: 11.066462516784668 
 batch loss: 11.107938766479492 
 batch loss: 11.086848258972168 
 batch loss: 11.067924499511719 
 batch loss: 11.11365032196045 
 batch loss: 11.06824779510498 
 batch loss: 11.058934211730957 
 batch loss: 11.085411071777344 
 batch loss: 11.03823184967041 
 batch loss: 11.060834884643555 
 batch loss: 11.065735816955566 
 batch loss: 11.063373565673828 
 batch loss: 11.050049781799316 
 batch loss: 11.041165351867676 
 batch loss: 11.025078773498535 
 batch loss: 11.066245079040527 
 batch loss: 11.01412296295166 
 batch loss: 11.00857925415039 
 batch loss: 11.038220405578613 
 batch loss: 10.99145221710205 
 batch loss: 11.002676010131836 
 batch loss: 11.014642715454102 
 batch loss: 10.994354248046875 
 batch loss: 10.972416877746582 
 batch loss: 10.993227005004883 
 batch loss: 10.979146003723145 
 batch loss: 10.979948043823242 
 batch loss: 10.9788179397583 
 batch loss: 10.979280471801758 
 batch loss: 10.934244155883789 
 batch loss: 10.974991798400879 
 batch loss: 10.958839416503906 
 batch loss: 10.986297607421875 
 batch loss: 10.983598709106445 
 batch loss: 10.946964263916016 
 batch loss: 10.949872970581055 
 batch loss: 10.961698532104492 
 batch loss: 10.932808876037598 
 batch loss: 10.94707202911377 
 batch loss: 10.97115421295166 
 batch loss: 10.954034805297852 
 batch loss: 10.9175386428833 
 batch loss: 10.929152488708496 
 batch loss: 10.927203178405762 
 batch loss: 10.909185409545898 
 batch loss: 10.945899963378906 
 batch loss: 10.935196876525879 
 batch loss: 10.914581298828125 
 batch loss: 10.931904792785645 
 batch loss: 10.94196891784668 
 batch loss: 10.91980266571045 
 batch loss: 10.96496868133545 
 batch loss: 10.906081199645996 
 batch loss: 10.901007652282715 
 batch loss: 10.884618759155273 
 batch loss: 10.878901481628418 
 batch loss: 10.884400367736816 
 batch loss: 10.875199317932129 
 batch loss: 10.86660099029541 
 batch loss: 10.874009132385254 
 batch loss: 10.866371154785156 
 batch loss: 10.88997745513916 
 batch loss: 10.839427947998047 
 batch loss: 10.891712188720703 
 batch loss: 10.804545402526855 
 batch loss: 10.827104568481445 
 batch loss: 10.823918342590332 
 batch loss: 10.846285820007324 
 batch loss: 10.830068588256836 
 batch loss: 10.808585166931152 
 batch loss: 10.834364891052246 
 batch loss: 10.812865257263184 
 batch loss: 10.81550121307373 
 batch loss: 10.788455963134766 
 batch loss: 10.823433876037598 
 batch loss: 10.8345308303833 
 batch loss: 10.820322036743164 
 batch loss: 10.830768585205078 
 batch loss: 10.802180290222168 
 batch loss: 10.786153793334961 
 batch loss: 10.813860893249512 
 batch loss: 10.799492835998535 
 batch loss: 10.80685043334961 
 batch loss: 10.832876205444336 
 batch loss: 10.795870780944824 
 batch loss: 10.825160026550293 
 batch loss: 10.815112113952637 
 batch loss: 10.830265998840332 
 batch loss: 10.859107971191406 
 batch loss: 10.84352970123291 
 batch loss: 10.78451156616211 
 batch loss: 10.774384498596191 
 batch loss: 10.814629554748535 
 batch loss: 10.834080696105957 
 batch loss: 10.786824226379395 
 batch loss: 10.797707557678223 
 batch loss: 10.774835586547852 
 batch loss: 10.810420989990234 
 batch loss: 10.790663719177246 
 batch loss: 10.807510375976562 
 Epoch 19 |Train loss: 10.942492365837097 |Validation loss: 10.809884786605835 
