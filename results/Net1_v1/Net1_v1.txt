     
 ID: Net1_v1 
 parameter number: 8225716
 infomation: baseline -- stack then extract feature 
 Epoch number: 20 
 Batch size: 2 
 =======================

Net_1(
  (feature_extract): FeatureNet(
    (net): Sequential(
      (0): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (1): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (2): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (3): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (4): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (5): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 348, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(348, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
    )
  )
  (disp_extracct): DispNet(
    (net): Sequential(
      (0): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(348, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (1): ConvTranspose2d_block(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (2): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (3): ConvTranspose2d_block(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (4): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (5): ConvTranspose2d_block(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (6): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): Softmax()
    )
  )
  (regression): DisparityRegression()
) batch loss: 94.10490417480469 
 batch loss: 92.56280517578125 
 batch loss: 89.92859649658203 
 batch loss: 88.4651107788086 
 batch loss: 86.045166015625 
 batch loss: 83.59407043457031 
 batch loss: 81.73604583740234 
 batch loss: 80.23965454101562 
 batch loss: 79.51441192626953 
 batch loss: 79.53052520751953 
 batch loss: 78.5036849975586 
 batch loss: 78.68057250976562 
 batch loss: 77.71868133544922 
 batch loss: 78.34276580810547 
 batch loss: 77.08287811279297 
 batch loss: 76.72745513916016 
 batch loss: 76.57298278808594 
 batch loss: 76.56592559814453 
 batch loss: 76.4201431274414 
 batch loss: 76.1708755493164 
 batch loss: 76.04912567138672 
 batch loss: 76.01051330566406 
 batch loss: 75.58011627197266 
 batch loss: 75.43338012695312 
 batch loss: 75.36382293701172 
 batch loss: 75.26685333251953 
 batch loss: 74.96969604492188 
 batch loss: 74.83901977539062 
 batch loss: 74.821533203125 
 batch loss: 74.64327239990234 
 batch loss: 74.46678924560547 
 batch loss: 74.33518981933594 
 batch loss: 74.16962432861328 
 batch loss: 74.06578826904297 
 batch loss: 73.92430114746094 
 batch loss: 73.8161392211914 
 batch loss: 73.6474609375 
 batch loss: 73.60160064697266 
 batch loss: 73.55008697509766 
 batch loss: 73.28673553466797 
 batch loss: 73.13279724121094 
 batch loss: 73.01648712158203 
 batch loss: 73.00408935546875 
 batch loss: 73.01531219482422 
 batch loss: 72.69580841064453 
 batch loss: 72.77680969238281 
 batch loss: 72.71560668945312 
 batch loss: 72.48783111572266 
 batch loss: 72.25636291503906 
 batch loss: 72.2168197631836 
 batch loss: 72.40615844726562 
 batch loss: 72.03605651855469 
 batch loss: 71.85966491699219 
 batch loss: 71.7522201538086 
 batch loss: 71.69586181640625 
 batch loss: 71.57229614257812 
 batch loss: 71.52545928955078 
 batch loss: 71.40528106689453 
 batch loss: 71.4646987915039 
 batch loss: 71.1690902709961 
 batch loss: 71.0372085571289 
 batch loss: 71.02532196044922 
 batch loss: 70.96128845214844 
 batch loss: 70.74040222167969 
 batch loss: 70.62201690673828 
 batch loss: 70.72561645507812 
 batch loss: 70.40811157226562 
 batch loss: 70.36951446533203 
 batch loss: 70.40044403076172 
 batch loss: 70.19947814941406 
 batch loss: 70.05136108398438 
 batch loss: 69.93577575683594 
 batch loss: 69.87627410888672 
 batch loss: 69.72238159179688 
 batch loss: 69.71334075927734 
 batch loss: 69.51447296142578 
 batch loss: 69.46395111083984 
 batch loss: 69.32616424560547 
 batch loss: 69.22985076904297 
 batch loss: 69.18362426757812 
 batch loss: 69.0266342163086 
 batch loss: 69.03724670410156 
 batch loss: 69.05644226074219 
 batch loss: 69.05133819580078 
 batch loss: 69.0414047241211 
 batch loss: 69.07257080078125 
 batch loss: 69.03367614746094 
 batch loss: 68.99464416503906 
 batch loss: 69.04230499267578 
 batch loss: 69.03763580322266 
 batch loss: 68.99810028076172 
 batch loss: 69.00199890136719 
 batch loss: 69.01670837402344 
 batch loss: 69.02587890625 
 batch loss: 69.07230377197266 
 batch loss: 69.0130615234375 
 batch loss: 69.05200958251953 
 batch loss: 69.02215576171875 
 batch loss: 68.99851989746094 
 batch loss: 69.01826477050781 
 Epoch 0 |Train loss: 74.58824520111084 |Validation loss: 69.03064498901367 
 batch loss: 69.0136489868164 
 batch loss: 68.87611389160156 
 batch loss: 68.80130004882812 
 batch loss: 68.73507690429688 
 batch loss: 68.62306213378906 
 batch loss: 68.47061920166016 
 batch loss: 68.3795166015625 
 batch loss: 68.30335998535156 
 batch loss: 68.1635971069336 
 batch loss: 68.1910400390625 
 batch loss: 67.79261779785156 
 batch loss: 67.71607971191406 
 batch loss: 67.6230697631836 
 batch loss: 67.40164947509766 
 batch loss: 67.2042236328125 
 batch loss: 67.17726135253906 
 batch loss: 67.0252914428711 
 batch loss: 66.87797546386719 
 batch loss: 66.78009033203125 
 batch loss: 66.80736541748047 
 batch loss: 66.95771026611328 
 batch loss: 66.34394073486328 
 batch loss: 66.20609283447266 
 batch loss: 65.98783874511719 
 batch loss: 65.58750915527344 
 batch loss: 65.20565032958984 
 batch loss: 64.90975952148438 
 batch loss: 64.71089172363281 
 batch loss: 64.46290588378906 
 batch loss: 64.40080261230469 
 batch loss: 64.30980682373047 
 batch loss: 64.1396484375 
 batch loss: 64.03514099121094 
 batch loss: 64.1216049194336 
 batch loss: 63.84016036987305 
 batch loss: 63.6703987121582 
 batch loss: 63.634830474853516 
 batch loss: 63.41220474243164 
 batch loss: 63.32246017456055 
 batch loss: 63.182647705078125 
 batch loss: 63.12298583984375 
 batch loss: 63.034751892089844 
 batch loss: 62.799503326416016 
 batch loss: 62.65488052368164 
 batch loss: 62.50770568847656 
 batch loss: 62.4012565612793 
 batch loss: 62.50050735473633 
 batch loss: 62.346038818359375 
 batch loss: 62.28341293334961 
 batch loss: 62.17774200439453 
 batch loss: 61.9219856262207 
 batch loss: 61.815677642822266 
 batch loss: 61.76509475708008 
 batch loss: 61.55109786987305 
 batch loss: 61.448768615722656 
 batch loss: 61.270912170410156 
 batch loss: 61.18050765991211 
 batch loss: 61.185306549072266 
 batch loss: 60.91347885131836 
 batch loss: 60.78477478027344 
 batch loss: 60.68033981323242 
 batch loss: 60.504737854003906 
 batch loss: 60.5634651184082 
 batch loss: 60.35990524291992 
 batch loss: 60.3155632019043 
 batch loss: 60.073726654052734 
 batch loss: 59.9610710144043 
 batch loss: 59.80373764038086 
 batch loss: 59.636810302734375 
 batch loss: 59.38232421875 
 batch loss: 59.3402214050293 
 batch loss: 58.96039581298828 
 batch loss: 58.93599319458008 
 batch loss: 58.572811126708984 
 batch loss: 58.40786361694336 
 batch loss: 58.28242492675781 
 batch loss: 58.06912612915039 
 batch loss: 57.872676849365234 
 batch loss: 58.06180191040039 
 batch loss: 57.507083892822266 
 batch loss: 57.6787109375 
 batch loss: 57.91453170776367 
 batch loss: 57.694175720214844 
 batch loss: 57.4383430480957 
 batch loss: 57.645225524902344 
 batch loss: 57.670196533203125 
 batch loss: 57.49333190917969 
 batch loss: 57.70927429199219 
 batch loss: 57.605037689208984 
 batch loss: 57.50980758666992 
 batch loss: 57.589908599853516 
 batch loss: 57.441619873046875 
 batch loss: 57.58259963989258 
 batch loss: 57.47861099243164 
 batch loss: 57.419456481933594 
 batch loss: 57.50804138183594 
 batch loss: 57.3180046081543 
 batch loss: 57.65525817871094 
 batch loss: 57.8612060546875 
 batch loss: 57.88963317871094 
 Epoch 1 |Train loss: 63.442042922973634 |Validation loss: 57.60514869689941 
 batch loss: 57.65182876586914 
 batch loss: 57.31240463256836 
 batch loss: 57.94892501831055 
 batch loss: 57.11143112182617 
 batch loss: 56.8658447265625 
 batch loss: 56.82620620727539 
 batch loss: 56.69797897338867 
 batch loss: 56.38471984863281 
 batch loss: 56.49519729614258 
 batch loss: 56.53519821166992 
 batch loss: 55.957210540771484 
 batch loss: 56.78291320800781 
 batch loss: 55.77361297607422 
 batch loss: 56.74144744873047 
 batch loss: 55.70335388183594 
 batch loss: 55.46058654785156 
 batch loss: 55.90035629272461 
 batch loss: 56.051795959472656 
 batch loss: 56.03473663330078 
 batch loss: 55.70610809326172 
 batch loss: 55.44097900390625 
 batch loss: 55.093013763427734 
 batch loss: 55.411006927490234 
 batch loss: 54.61894607543945 
 batch loss: 54.46384811401367 
 batch loss: 54.50490951538086 
 batch loss: 54.446990966796875 
 batch loss: 54.158912658691406 
 batch loss: 54.01884078979492 
 batch loss: 53.85346603393555 
 batch loss: 53.857948303222656 
 batch loss: 53.59779739379883 
 batch loss: 53.4525032043457 
 batch loss: 53.24932861328125 
 batch loss: 53.34162521362305 
 batch loss: 53.200653076171875 
 batch loss: 53.07820510864258 
 batch loss: 52.97290802001953 
 batch loss: 52.59189224243164 
 batch loss: 52.609527587890625 
 batch loss: 52.38346862792969 
 batch loss: 52.54611587524414 
 batch loss: 52.372840881347656 
 batch loss: 52.221282958984375 
 batch loss: 52.088809967041016 
 batch loss: 51.88359069824219 
 batch loss: 51.93943405151367 
 batch loss: 51.66162872314453 
 batch loss: 51.732120513916016 
 batch loss: 51.405906677246094 
 batch loss: 51.44843673706055 
 batch loss: 51.16581344604492 
 batch loss: 50.99034118652344 
 batch loss: 51.158721923828125 
 batch loss: 50.936126708984375 
 batch loss: 51.125431060791016 
 batch loss: 50.577423095703125 
 batch loss: 50.657474517822266 
 batch loss: 50.83158874511719 
 batch loss: 50.67695236206055 
 batch loss: 50.077301025390625 
 batch loss: 50.24492645263672 
 batch loss: 50.072792053222656 
 batch loss: 49.902587890625 
 batch loss: 49.763973236083984 
 batch loss: 49.521827697753906 
 batch loss: 49.43613815307617 
 batch loss: 49.30012130737305 
 batch loss: 49.27779006958008 
 batch loss: 48.91902542114258 
 batch loss: 48.70575714111328 
 batch loss: 48.41218185424805 
 batch loss: 48.59564208984375 
 batch loss: 48.44211196899414 
 batch loss: 48.13486862182617 
 batch loss: 48.24089050292969 
 batch loss: 48.28816223144531 
 batch loss: 48.31541061401367 
 batch loss: 47.91826629638672 
 batch loss: 47.563114166259766 
 batch loss: 47.3994255065918 
 batch loss: 47.45457077026367 
 batch loss: 47.651615142822266 
 batch loss: 47.39003372192383 
 batch loss: 47.4822883605957 
 batch loss: 47.6511344909668 
 batch loss: 47.53825378417969 
 batch loss: 47.51207733154297 
 batch loss: 47.735572814941406 
 batch loss: 47.657222747802734 
 batch loss: 47.40055847167969 
 batch loss: 47.37154006958008 
 batch loss: 47.5444450378418 
 batch loss: 47.491825103759766 
 batch loss: 47.3661994934082 
 batch loss: 47.654136657714844 
 batch loss: 47.47119903564453 
 batch loss: 47.42048645019531 
 batch loss: 47.434669494628906 
 batch loss: 47.43973922729492 
 Epoch 2 |Train loss: 52.7105194568634 |Validation loss: 47.503349685668944 
 batch loss: 47.49955749511719 
 batch loss: 47.303070068359375 
 batch loss: 47.24015426635742 
 batch loss: 47.18498611450195 
 batch loss: 46.81427764892578 
 batch loss: 47.09954833984375 
 batch loss: 47.397274017333984 
 batch loss: 46.60142517089844 
 batch loss: 46.619258880615234 
 batch loss: 46.38679885864258 
 batch loss: 46.10424041748047 
 batch loss: 46.85065460205078 
 batch loss: 46.18678665161133 
 batch loss: 46.27109146118164 
 batch loss: 46.427001953125 
 batch loss: 45.67638397216797 
 batch loss: 45.77125549316406 
 batch loss: 45.82947540283203 
 batch loss: 45.46495056152344 
 batch loss: 45.32370376586914 
 batch loss: 45.39891052246094 
 batch loss: 45.26237869262695 
 batch loss: 45.085914611816406 
 batch loss: 45.17267608642578 
 batch loss: 44.89762496948242 
 batch loss: 44.846588134765625 
 batch loss: 44.58342361450195 
 batch loss: 44.6429328918457 
 batch loss: 44.48660659790039 
 batch loss: 44.26063537597656 
 batch loss: 44.53971481323242 
 batch loss: 43.99781036376953 
 batch loss: 44.58274841308594 
 batch loss: 43.97377014160156 
 batch loss: 43.79440689086914 
 batch loss: 43.77119445800781 
 batch loss: 44.63788986206055 
 batch loss: 44.03654479980469 
 batch loss: 43.847137451171875 
 batch loss: 43.5363883972168 
 batch loss: 43.247047424316406 
 batch loss: 43.139198303222656 
 batch loss: 43.28439712524414 
 batch loss: 43.06912612915039 
 batch loss: 42.88956069946289 
 batch loss: 42.8738899230957 
 batch loss: 42.708709716796875 
 batch loss: 42.590606689453125 
 batch loss: 42.41572570800781 
 batch loss: 42.2364501953125 
 batch loss: 42.2266960144043 
 batch loss: 42.36602020263672 
 batch loss: 42.03895568847656 
 batch loss: 42.111183166503906 
 batch loss: 41.98859405517578 
 batch loss: 41.77158737182617 
 batch loss: 42.17599105834961 
 batch loss: 42.02387237548828 
 batch loss: 41.536033630371094 
 batch loss: 41.32088851928711 
 batch loss: 41.61652374267578 
 batch loss: 41.265987396240234 
 batch loss: 41.20655059814453 
 batch loss: 41.51123046875 
 batch loss: 41.15898895263672 
 batch loss: 41.027313232421875 
 batch loss: 40.8779296875 
 batch loss: 40.66101837158203 
 batch loss: 40.698421478271484 
 batch loss: 40.5008430480957 
 batch loss: 40.448238372802734 
 batch loss: 40.38241958618164 
 batch loss: 40.360816955566406 
 batch loss: 40.105751037597656 
 batch loss: 40.0217170715332 
 batch loss: 39.95558166503906 
 batch loss: 40.06122589111328 
 batch loss: 39.873477935791016 
 batch loss: 39.97664260864258 
 batch loss: 39.686126708984375 
 batch loss: 39.62836456298828 
 batch loss: 39.90225601196289 
 batch loss: 39.793983459472656 
 batch loss: 39.6249885559082 
 batch loss: 39.55886459350586 
 batch loss: 39.631744384765625 
 batch loss: 39.54452133178711 
 batch loss: 39.56360626220703 
 batch loss: 39.68809509277344 
 batch loss: 39.70078659057617 
 batch loss: 39.63383483886719 
 batch loss: 39.5682373046875 
 batch loss: 39.992835998535156 
 batch loss: 39.615577697753906 
 batch loss: 39.94175338745117 
 batch loss: 39.69493103027344 
 batch loss: 40.112709045410156 
 batch loss: 39.86677551269531 
 batch loss: 40.41404342651367 
 batch loss: 40.0316162109375 
 Epoch 3 |Train loss: 43.48523163795471 |Validation loss: 39.77547626495361 
 batch loss: 39.69327163696289 
 batch loss: 44.777862548828125 
 batch loss: 40.628910064697266 
 batch loss: 41.667667388916016 
 batch loss: 41.03425598144531 
 batch loss: 41.312042236328125 
 batch loss: 40.984867095947266 
 batch loss: 40.92009353637695 
 batch loss: 40.191959381103516 
 batch loss: 39.94819641113281 
 batch loss: 39.73335647583008 
 batch loss: 40.24925231933594 
 batch loss: 39.41899871826172 
 batch loss: 39.39981460571289 
 batch loss: 39.228328704833984 
 batch loss: 38.859622955322266 
 batch loss: 38.72045135498047 
 batch loss: 38.806304931640625 
 batch loss: 38.644535064697266 
 batch loss: 38.570648193359375 
 batch loss: 38.45509719848633 
 batch loss: 38.224849700927734 
 batch loss: 38.44146728515625 
 batch loss: 38.15703582763672 
 batch loss: 38.02437973022461 
 batch loss: 37.93415069580078 
 batch loss: 37.91305923461914 
 batch loss: 37.72561264038086 
 batch loss: 38.00105285644531 
 batch loss: 37.76851272583008 
 batch loss: 37.453392028808594 
 batch loss: 37.940975189208984 
 batch loss: 37.163169860839844 
 batch loss: 37.17373275756836 
 batch loss: 37.152042388916016 
 batch loss: 37.11652374267578 
 batch loss: 36.96841049194336 
 batch loss: 36.97246170043945 
 batch loss: 36.805599212646484 
 batch loss: 37.051639556884766 
 batch loss: 36.71984100341797 
 batch loss: 36.75965118408203 
 batch loss: 36.913875579833984 
 batch loss: 36.42838668823242 
 batch loss: 36.43031692504883 
 batch loss: 36.505889892578125 
 batch loss: 36.276729583740234 
 batch loss: 36.59770584106445 
 batch loss: 36.19734573364258 
 batch loss: 35.94306564331055 
 batch loss: 35.852108001708984 
 batch loss: 36.08502197265625 
 batch loss: 35.91836166381836 
 batch loss: 35.67647933959961 
 batch loss: 35.81761932373047 
 batch loss: 36.096988677978516 
 batch loss: 35.405174255371094 
 batch loss: 35.59254455566406 
 batch loss: 35.87187957763672 
 batch loss: 35.251365661621094 
 batch loss: 35.1489372253418 
 batch loss: 35.197391510009766 
 batch loss: 35.26167678833008 
 batch loss: 35.19008255004883 
 batch loss: 35.255943298339844 
 batch loss: 34.92776870727539 
 batch loss: 34.89451217651367 
 batch loss: 34.78740692138672 
 batch loss: 34.75117874145508 
 batch loss: 34.56575393676758 
 batch loss: 34.45771408081055 
 batch loss: 34.4593620300293 
 batch loss: 34.5843391418457 
 batch loss: 34.30854797363281 
 batch loss: 34.44347381591797 
 batch loss: 34.136077880859375 
 batch loss: 34.27492141723633 
 batch loss: 34.04848098754883 
 batch loss: 33.94533920288086 
 batch loss: 34.054969787597656 
 batch loss: 34.21842575073242 
 batch loss: 33.9239387512207 
 batch loss: 34.328800201416016 
 batch loss: 34.053958892822266 
 batch loss: 34.188560485839844 
 batch loss: 34.3223991394043 
 batch loss: 34.04560089111328 
 batch loss: 33.957130432128906 
 batch loss: 34.22674560546875 
 batch loss: 34.071571350097656 
 batch loss: 34.0949592590332 
 batch loss: 34.121944427490234 
 batch loss: 33.960365295410156 
 batch loss: 34.28329086303711 
 batch loss: 34.19173812866211 
 batch loss: 33.94185256958008 
 batch loss: 33.99412536621094 
 batch loss: 34.096675872802734 
 batch loss: 33.93902587890625 
 batch loss: 33.955299377441406 
 Epoch 4 |Train loss: 37.12834792137146 |Validation loss: 34.09582042694092 
 batch loss: 33.86964797973633 
 batch loss: 33.93546676635742 
 batch loss: 33.7459602355957 
 batch loss: 33.754600524902344 
 batch loss: 33.561614990234375 
 batch loss: 33.74641036987305 
 batch loss: 33.50672149658203 
 batch loss: 33.68253707885742 
 batch loss: 33.468666076660156 
 batch loss: 33.32678985595703 
 batch loss: 33.45759963989258 
 batch loss: 33.52499771118164 
 batch loss: 33.24359893798828 
 batch loss: 33.35475158691406 
 batch loss: 33.6060676574707 
 batch loss: 33.083675384521484 
 batch loss: 32.933799743652344 
 batch loss: 33.14753341674805 
 batch loss: 33.14636993408203 
 batch loss: 32.93717956542969 
 batch loss: 32.88298034667969 
 batch loss: 32.98265075683594 
 batch loss: 33.04944610595703 
 batch loss: 32.67181396484375 
 batch loss: 32.772422790527344 
 batch loss: 32.574039459228516 
 batch loss: 32.59841537475586 
 batch loss: 32.828636169433594 
 batch loss: 32.46050262451172 
 batch loss: 32.59755325317383 
 batch loss: 32.87482833862305 
 batch loss: 32.1869010925293 
 batch loss: 32.17531967163086 
 batch loss: 32.56189727783203 
 batch loss: 32.67422866821289 
 batch loss: 31.972705841064453 
 batch loss: 31.871767044067383 
 batch loss: 32.146705627441406 
 batch loss: 32.17267608642578 
 batch loss: 31.880905151367188 
 batch loss: 31.803789138793945 
 batch loss: 31.54242515563965 
 batch loss: 31.770620346069336 
 batch loss: 31.648643493652344 
 batch loss: 31.3897647857666 
 batch loss: 31.87158966064453 
 batch loss: 31.359193801879883 
 batch loss: 31.324125289916992 
 batch loss: 31.083101272583008 
 batch loss: 31.34343910217285 
 batch loss: 30.96832847595215 
 batch loss: 31.043527603149414 
 batch loss: 31.27962303161621 
 batch loss: 30.97780990600586 
 batch loss: 30.998096466064453 
 batch loss: 31.143272399902344 
 batch loss: 30.69725799560547 
 batch loss: 30.915725708007812 
 batch loss: 30.505708694458008 
 batch loss: 30.739444732666016 
 batch loss: 30.2895450592041 
 batch loss: 30.09453773498535 
 batch loss: 29.870376586914062 
 batch loss: 29.731605529785156 
 batch loss: 29.825714111328125 
 batch loss: 29.626056671142578 
 batch loss: 29.358684539794922 
 batch loss: 29.2993106842041 
 batch loss: 29.045440673828125 
 batch loss: 28.949953079223633 
 batch loss: 29.064191818237305 
 batch loss: 28.769807815551758 
 batch loss: 28.59946632385254 
 batch loss: 28.387170791625977 
 batch loss: 28.289470672607422 
 batch loss: 28.42672348022461 
 batch loss: 28.300559997558594 
 batch loss: 28.10338592529297 
 batch loss: 28.610029220581055 
 batch loss: 28.172616958618164 
 batch loss: 28.52912139892578 
 batch loss: 28.454288482666016 
 batch loss: 28.78034019470215 
 batch loss: 28.834020614624023 
 batch loss: 28.211124420166016 
 batch loss: 28.86141014099121 
 batch loss: 28.556936264038086 
 batch loss: 28.60708236694336 
 batch loss: 28.506973266601562 
 batch loss: 28.777761459350586 
 batch loss: 29.062618255615234 
 batch loss: 28.179794311523438 
 batch loss: 28.36579132080078 
 batch loss: 28.287593841552734 
 batch loss: 28.13823699951172 
 batch loss: 28.171016693115234 
 batch loss: 28.01526641845703 
 batch loss: 28.044469833374023 
 batch loss: 28.04037857055664 
 batch loss: 28.151613235473633 
 Epoch 5 |Train loss: 31.552381491661073 |Validation loss: 28.42879190444946 
 batch loss: 29.00978660583496 
 batch loss: 27.933650970458984 
 batch loss: 28.182876586914062 
 batch loss: 28.584606170654297 
 batch loss: 27.9940128326416 
 batch loss: 27.73722267150879 
 batch loss: 27.714763641357422 
 batch loss: 27.646656036376953 
 batch loss: 27.986467361450195 
 batch loss: 27.628862380981445 
 batch loss: 27.529077529907227 
 batch loss: 27.72123908996582 
 batch loss: 27.307132720947266 
 batch loss: 27.4907169342041 
 batch loss: 27.778539657592773 
 batch loss: 27.35854721069336 
 batch loss: 27.465700149536133 
 batch loss: 27.207212448120117 
 batch loss: 27.26888084411621 
 batch loss: 27.04388999938965 
 batch loss: 27.072561264038086 
 batch loss: 27.107072830200195 
 batch loss: 27.058992385864258 
 batch loss: 26.99090003967285 
 batch loss: 27.36924934387207 
 batch loss: 27.444347381591797 
 batch loss: 27.474483489990234 
 batch loss: 26.761898040771484 
 batch loss: 26.898649215698242 
 batch loss: 26.89475440979004 
 batch loss: 26.91681480407715 
 batch loss: 26.91415786743164 
 batch loss: 26.62771987915039 
 batch loss: 26.653146743774414 
 batch loss: 26.70294189453125 
 batch loss: 27.035343170166016 
 batch loss: 26.423690795898438 
 batch loss: 26.643163681030273 
 batch loss: 26.56153678894043 
 batch loss: 26.19045066833496 
 batch loss: 26.15443229675293 
 batch loss: 26.137449264526367 
 batch loss: 26.066314697265625 
 batch loss: 26.478578567504883 
 batch loss: 26.07789421081543 
 batch loss: 25.926570892333984 
 batch loss: 26.1146240234375 
 batch loss: 25.902753829956055 
 batch loss: 25.924840927124023 
 batch loss: 25.71453857421875 
 batch loss: 25.9310359954834 
 batch loss: 25.65298080444336 
 batch loss: 25.86844825744629 
 batch loss: 25.537437438964844 
 batch loss: 25.549114227294922 
 batch loss: 25.708887100219727 
 batch loss: 25.479598999023438 
 batch loss: 25.52290916442871 
 batch loss: 25.38161277770996 
 batch loss: 25.57874870300293 
 batch loss: 25.34565544128418 
 batch loss: 25.62911033630371 
 batch loss: 25.107006072998047 
 batch loss: 25.14692497253418 
 batch loss: 25.333742141723633 
 batch loss: 24.98069953918457 
 batch loss: 25.055557250976562 
 batch loss: 25.036787033081055 
 batch loss: 24.979751586914062 
 batch loss: 25.05766487121582 
 batch loss: 24.836530685424805 
 batch loss: 24.88418960571289 
 batch loss: 25.072158813476562 
 batch loss: 24.77117347717285 
 batch loss: 24.855609893798828 
 batch loss: 24.791227340698242 
 batch loss: 24.742481231689453 
 batch loss: 24.70124626159668 
 batch loss: 24.842247009277344 
 batch loss: 24.64152717590332 
 batch loss: 24.77102279663086 
 batch loss: 24.70553970336914 
 batch loss: 24.637929916381836 
 batch loss: 24.649354934692383 
 batch loss: 24.81294059753418 
 batch loss: 24.718708038330078 
 batch loss: 24.85578155517578 
 batch loss: 24.87020492553711 
 batch loss: 25.165904998779297 
 batch loss: 25.09724998474121 
 batch loss: 25.093564987182617 
 batch loss: 24.633262634277344 
 batch loss: 25.076587677001953 
 batch loss: 24.845813751220703 
 batch loss: 25.100107192993164 
 batch loss: 24.834070205688477 
 batch loss: 24.90658950805664 
 batch loss: 24.56978416442871 
 batch loss: 24.58074188232422 
 batch loss: 24.600074768066406 
 Epoch 6 |Train loss: 26.360647225379942 |Validation loss: 24.826261711120605 
 batch loss: 24.730133056640625 
 batch loss: 25.302764892578125 
 batch loss: 24.705154418945312 
 batch loss: 24.7204647064209 
 batch loss: 24.7601318359375 
 batch loss: 24.781171798706055 
 batch loss: 24.40636444091797 
 batch loss: 24.680625915527344 
 batch loss: 24.603538513183594 
 batch loss: 24.655990600585938 
 batch loss: 24.42437744140625 
 batch loss: 24.473411560058594 
 batch loss: 24.41451644897461 
 batch loss: 24.496673583984375 
 batch loss: 24.23500633239746 
 batch loss: 24.28952980041504 
 batch loss: 24.14383888244629 
 batch loss: 24.1668643951416 
 batch loss: 24.116107940673828 
 batch loss: 25.121864318847656 
 batch loss: 23.999906539916992 
 batch loss: 24.35673713684082 
 batch loss: 24.195152282714844 
 batch loss: 24.08153533935547 
 batch loss: 24.012296676635742 
 batch loss: 24.02175521850586 
 batch loss: 24.014739990234375 
 batch loss: 24.1661376953125 
 batch loss: 23.86782455444336 
 batch loss: 23.89703941345215 
 batch loss: 23.915691375732422 
 batch loss: 23.804828643798828 
 batch loss: 23.71905517578125 
 batch loss: 23.741945266723633 
 batch loss: 23.71476173400879 
 batch loss: 23.68893814086914 
 batch loss: 23.56144142150879 
 batch loss: 23.713390350341797 
 batch loss: 23.54303550720215 
 batch loss: 23.57247543334961 
 batch loss: 23.581851959228516 
 batch loss: 23.505495071411133 
 batch loss: 23.458393096923828 
 batch loss: 23.406518936157227 
 batch loss: 23.655912399291992 
 batch loss: 23.427589416503906 
 batch loss: 23.413423538208008 
 batch loss: 23.31488800048828 
 batch loss: 23.243837356567383 
 batch loss: 23.209091186523438 
 batch loss: 23.36566734313965 
 batch loss: 23.272838592529297 
 batch loss: 23.339996337890625 
 batch loss: 23.264558792114258 
 batch loss: 23.106510162353516 
 batch loss: 23.066425323486328 
 batch loss: 23.250417709350586 
 batch loss: 23.101499557495117 
 batch loss: 22.90068817138672 
 batch loss: 23.012041091918945 
 batch loss: 22.906702041625977 
 batch loss: 22.84591293334961 
 batch loss: 22.900203704833984 
 batch loss: 22.807018280029297 
 batch loss: 22.684703826904297 
 batch loss: 22.816654205322266 
 batch loss: 22.916852951049805 
 batch loss: 22.692928314208984 
 batch loss: 22.598106384277344 
 batch loss: 22.709606170654297 
 batch loss: 22.623607635498047 
 batch loss: 22.713014602661133 
 batch loss: 22.610492706298828 
 batch loss: 22.589143753051758 
 batch loss: 22.474525451660156 
 batch loss: 22.411334991455078 
 batch loss: 22.73950958251953 
 batch loss: 22.400678634643555 
 batch loss: 22.386045455932617 
 batch loss: 22.399213790893555 
 batch loss: 22.413785934448242 
 batch loss: 22.35157585144043 
 batch loss: 22.370285034179688 
 batch loss: 22.363006591796875 
 batch loss: 22.40491485595703 
 batch loss: 22.44215202331543 
 batch loss: 22.547571182250977 
 batch loss: 22.585145950317383 
 batch loss: 22.645105361938477 
 batch loss: 22.55248260498047 
 batch loss: 22.49658966064453 
 batch loss: 22.376893997192383 
 batch loss: 22.29486846923828 
 batch loss: 22.350605010986328 
 batch loss: 22.39641571044922 
 batch loss: 22.692564010620117 
 batch loss: 22.317989349365234 
 batch loss: 22.229686737060547 
 batch loss: 22.39745330810547 
 batch loss: 22.455320358276367 
 Epoch 7 |Train loss: 23.599263978004455 |Validation loss: 22.434220600128175 
 batch loss: 22.40180015563965 
 batch loss: 22.342674255371094 
 batch loss: 22.525554656982422 
 batch loss: 22.472213745117188 
 batch loss: 22.188827514648438 
 batch loss: 22.37929916381836 
 batch loss: 22.220352172851562 
 batch loss: 22.171953201293945 
 batch loss: 22.11819839477539 
 batch loss: 22.091238021850586 
 batch loss: 22.239484786987305 
 batch loss: 22.079721450805664 
 batch loss: 22.25090217590332 
 batch loss: 21.95246696472168 
 batch loss: 21.900562286376953 
 batch loss: 21.96622657775879 
 batch loss: 21.927967071533203 
 batch loss: 21.951169967651367 
 batch loss: 22.138397216796875 
 batch loss: 22.030996322631836 
 batch loss: 21.864093780517578 
 batch loss: 22.00292205810547 
 batch loss: 21.76435661315918 
 batch loss: 21.842182159423828 
 batch loss: 21.726991653442383 
 batch loss: 21.813369750976562 
 batch loss: 21.831920623779297 
 batch loss: 21.75309944152832 
 batch loss: 21.844303131103516 
 batch loss: 21.696041107177734 
 batch loss: 21.6763916015625 
 batch loss: 21.651594161987305 
 batch loss: 21.59586524963379 
 batch loss: 21.70783805847168 
 batch loss: 21.520774841308594 
 batch loss: 21.55119514465332 
 batch loss: 21.63367462158203 
 batch loss: 21.678573608398438 
 batch loss: 21.524688720703125 
 batch loss: 21.435163497924805 
 batch loss: 21.50515365600586 
 batch loss: 21.504858016967773 
 batch loss: 21.519250869750977 
 batch loss: 21.407381057739258 
 batch loss: 21.43929672241211 
 batch loss: 21.335193634033203 
 batch loss: 21.304466247558594 
 batch loss: 21.409011840820312 
 batch loss: 21.27948760986328 
 batch loss: 21.731067657470703 
 batch loss: 21.32146644592285 
 batch loss: 21.59218978881836 
 batch loss: 21.298484802246094 
 batch loss: 21.155956268310547 
 batch loss: 21.106277465820312 
 batch loss: 21.22987174987793 
 batch loss: 21.210615158081055 
 batch loss: 21.258981704711914 
 batch loss: 21.317522048950195 
 batch loss: 21.174699783325195 
 batch loss: 20.999935150146484 
 batch loss: 21.021841049194336 
 batch loss: 21.06117820739746 
 batch loss: 21.004989624023438 
 batch loss: 20.897188186645508 
 batch loss: 20.882780075073242 
 batch loss: 20.90571403503418 
 batch loss: 20.98569679260254 
 batch loss: 20.92401695251465 
 batch loss: 20.915573120117188 
 batch loss: 20.96722412109375 
 batch loss: 20.99436378479004 
 batch loss: 20.824472427368164 
 batch loss: 20.790103912353516 
 batch loss: 20.754121780395508 
 batch loss: 20.79275131225586 
 batch loss: 20.679527282714844 
 batch loss: 20.714107513427734 
 batch loss: 20.75339698791504 
 batch loss: 20.771183013916016 
 batch loss: 20.72303581237793 
 batch loss: 20.62981414794922 
 batch loss: 20.7761287689209 
 batch loss: 20.65955924987793 
 batch loss: 20.66683578491211 
 batch loss: 20.690576553344727 
 batch loss: 20.654964447021484 
 batch loss: 20.6337833404541 
 batch loss: 20.93572425842285 
 batch loss: 20.8192138671875 
 batch loss: 20.760292053222656 
 batch loss: 20.762020111083984 
 batch loss: 20.71306800842285 
 batch loss: 20.663124084472656 
 batch loss: 20.697851181030273 
 batch loss: 20.826284408569336 
 batch loss: 20.62717628479004 
 batch loss: 20.682571411132812 
 batch loss: 20.81770133972168 
 batch loss: 20.619661331176758 
 Epoch 8 |Train loss: 21.527580547332764 |Validation loss: 20.71796932220459 
 batch loss: 20.611177444458008 
 batch loss: 20.57375717163086 
 batch loss: 20.626855850219727 
 batch loss: 20.51581573486328 
 batch loss: 20.589262008666992 
 batch loss: 20.566112518310547 
 batch loss: 20.536664962768555 
 batch loss: 20.60841941833496 
 batch loss: 20.55682373046875 
 batch loss: 20.403488159179688 
 batch loss: 20.56587028503418 
 batch loss: 20.574705123901367 
 batch loss: 20.439332962036133 
 batch loss: 20.574464797973633 
 batch loss: 20.66512680053711 
 batch loss: 20.369619369506836 
 batch loss: 20.654541015625 
 batch loss: 20.482486724853516 
 batch loss: 20.432374954223633 
 batch loss: 20.334447860717773 
 batch loss: 20.44091796875 
 batch loss: 20.414836883544922 
 batch loss: 20.57309913635254 
 batch loss: 20.395687103271484 
 batch loss: 20.39124870300293 
 batch loss: 20.454864501953125 
 batch loss: 20.24313735961914 
 batch loss: 20.31254005432129 
 batch loss: 20.127099990844727 
 batch loss: 20.072521209716797 
 batch loss: 20.185626983642578 
 batch loss: 20.063196182250977 
 batch loss: 20.095775604248047 
 batch loss: 20.194442749023438 
 batch loss: 20.193666458129883 
 batch loss: 19.936405181884766 
 batch loss: 20.05545425415039 
 batch loss: 20.567996978759766 
 batch loss: 20.10084342956543 
 batch loss: 20.59600257873535 
 batch loss: 20.086227416992188 
 batch loss: 20.029386520385742 
 batch loss: 19.974023818969727 
 batch loss: 19.933334350585938 
 batch loss: 19.971872329711914 
 batch loss: 20.037044525146484 
 batch loss: 19.93691635131836 
 batch loss: 19.81633758544922 
 batch loss: 20.23888397216797 
 batch loss: 19.881690979003906 
 batch loss: 19.88836669921875 
 batch loss: 19.842086791992188 
 batch loss: 19.742076873779297 
 batch loss: 19.77169418334961 
 batch loss: 19.875354766845703 
 batch loss: 20.089111328125 
 batch loss: 19.789562225341797 
 batch loss: 19.789743423461914 
 batch loss: 19.81599235534668 
 batch loss: 19.676353454589844 
 batch loss: 19.68417739868164 
 batch loss: 19.788402557373047 
 batch loss: 19.64829444885254 
 batch loss: 19.740083694458008 
 batch loss: 19.575069427490234 
 batch loss: 19.536603927612305 
 batch loss: 19.516746520996094 
 batch loss: 19.40544319152832 
 batch loss: 19.44713592529297 
 batch loss: 19.4627685546875 
 batch loss: 19.288127899169922 
 batch loss: 19.508745193481445 
 batch loss: 19.218420028686523 
 batch loss: 19.359703063964844 
 batch loss: 19.309398651123047 
 batch loss: 19.396825790405273 
 batch loss: 19.168434143066406 
 batch loss: 19.25062370300293 
 batch loss: 19.19156837463379 
 batch loss: 19.366487503051758 
 batch loss: 19.13519287109375 
 batch loss: 19.216718673706055 
 batch loss: 19.16761016845703 
 batch loss: 18.983997344970703 
 batch loss: 19.030879974365234 
 batch loss: 19.108396530151367 
 batch loss: 19.113126754760742 
 batch loss: 19.21117401123047 
 batch loss: 19.09694480895996 
 batch loss: 19.079103469848633 
 batch loss: 19.19564437866211 
 batch loss: 19.1080322265625 
 batch loss: 19.147972106933594 
 batch loss: 19.057903289794922 
 batch loss: 19.132488250732422 
 batch loss: 19.007896423339844 
 batch loss: 19.066207885742188 
 batch loss: 19.02057456970215 
 batch loss: 19.08238983154297 
 batch loss: 19.041397094726562 
 Epoch 9 |Train loss: 20.039322876930235 |Validation loss: 19.10018253326416 
 batch loss: 19.05989646911621 
 batch loss: 19.128509521484375 
 batch loss: 19.01455307006836 
 batch loss: 19.12912940979004 
 batch loss: 18.994251251220703 
 batch loss: 18.946537017822266 
 batch loss: 18.967193603515625 
 batch loss: 18.939699172973633 
 batch loss: 18.925878524780273 
 batch loss: 18.98590660095215 
 batch loss: 18.760738372802734 
 batch loss: 19.02591323852539 
 batch loss: 18.72537612915039 
 batch loss: 18.81883430480957 
 batch loss: 18.70003318786621 
 batch loss: 18.632017135620117 
 batch loss: 18.611623764038086 
 batch loss: 18.642608642578125 
 batch loss: 18.768186569213867 
 batch loss: 18.568153381347656 
 batch loss: 18.486854553222656 
 batch loss: 18.469223022460938 
 batch loss: 18.52308464050293 
 batch loss: 18.42835807800293 
 batch loss: 18.41815948486328 
 batch loss: 18.42403221130371 
 batch loss: 18.4736328125 
 batch loss: 18.335262298583984 
 batch loss: 18.300296783447266 
 batch loss: 18.2751522064209 
 batch loss: 18.388620376586914 
 batch loss: 18.227195739746094 
 batch loss: 18.212173461914062 
 batch loss: 18.23872947692871 
 batch loss: 18.164352416992188 
 batch loss: 18.20541763305664 
 batch loss: 18.171958923339844 
 batch loss: 18.109149932861328 
 batch loss: 18.11940574645996 
 batch loss: 18.211681365966797 
 batch loss: 18.117490768432617 
 batch loss: 18.031253814697266 
 batch loss: 18.243263244628906 
 batch loss: 17.96257209777832 
 batch loss: 18.035722732543945 
 batch loss: 17.920427322387695 
 batch loss: 18.016939163208008 
 batch loss: 17.97730255126953 
 batch loss: 17.978729248046875 
 batch loss: 17.81546974182129 
 batch loss: 17.819557189941406 
 batch loss: 17.73860740661621 
 batch loss: 17.734344482421875 
 batch loss: 17.784915924072266 
 batch loss: 17.749616622924805 
 batch loss: 17.708629608154297 
 batch loss: 17.673059463500977 
 batch loss: 17.736719131469727 
 batch loss: 17.62751007080078 
 batch loss: 17.633874893188477 
 batch loss: 17.539947509765625 
 batch loss: 17.566312789916992 
 batch loss: 17.48846435546875 
 batch loss: 17.51312255859375 
 batch loss: 17.533119201660156 
 batch loss: 17.457468032836914 
 batch loss: 17.4411563873291 
 batch loss: 17.451824188232422 
 batch loss: 17.37215805053711 
 batch loss: 17.33573341369629 
 batch loss: 17.396150588989258 
 batch loss: 17.32063865661621 
 batch loss: 17.248138427734375 
 batch loss: 17.188405990600586 
 batch loss: 17.085229873657227 
 batch loss: 17.099285125732422 
 batch loss: 17.102375030517578 
 batch loss: 17.1148624420166 
 batch loss: 17.035236358642578 
 batch loss: 16.993759155273438 
 batch loss: 16.97188949584961 
 batch loss: 16.910093307495117 
 batch loss: 17.018075942993164 
 batch loss: 16.92530059814453 
 batch loss: 16.96687126159668 
 batch loss: 17.042081832885742 
 batch loss: 16.947072982788086 
 batch loss: 16.919233322143555 
 batch loss: 17.04235076904297 
 batch loss: 17.025676727294922 
 batch loss: 16.98209571838379 
 batch loss: 17.026979446411133 
 batch loss: 16.986190795898438 
 batch loss: 17.081098556518555 
 batch loss: 17.150863647460938 
 batch loss: 16.959230422973633 
 batch loss: 17.05757713317871 
 batch loss: 17.086036682128906 
 batch loss: 16.931249618530273 
 batch loss: 16.931861877441406 
 Epoch 10 |Train loss: 18.088964676856996 |Validation loss: 16.99809150695801 
 batch loss: 16.981781005859375 
 batch loss: 16.98349952697754 
 batch loss: 17.07366943359375 
 batch loss: 17.02159881591797 
 batch loss: 17.15863800048828 
 batch loss: 16.932113647460938 
 batch loss: 16.923368453979492 
 batch loss: 16.846738815307617 
 batch loss: 16.83498191833496 
 batch loss: 16.820568084716797 
 batch loss: 16.822528839111328 
 batch loss: 16.829483032226562 
 batch loss: 16.746030807495117 
 batch loss: 16.789100646972656 
 batch loss: 16.755252838134766 
 batch loss: 16.80959129333496 
 batch loss: 16.84143829345703 
 batch loss: 16.741886138916016 
 batch loss: 16.664979934692383 
 batch loss: 16.69917106628418 
 batch loss: 16.931386947631836 
 batch loss: 16.62275505065918 
 batch loss: 16.59503936767578 
 batch loss: 16.61922836303711 
 batch loss: 16.589649200439453 
 batch loss: 16.631818771362305 
 batch loss: 16.528959274291992 
 batch loss: 16.638858795166016 
 batch loss: 16.826692581176758 
 batch loss: 16.493200302124023 
 batch loss: 16.764997482299805 
 batch loss: 16.432279586791992 
 batch loss: 16.33632469177246 
 batch loss: 16.366134643554688 
 batch loss: 16.38477897644043 
 batch loss: 16.435773849487305 
 batch loss: 16.273300170898438 
 batch loss: 16.371063232421875 
 batch loss: 16.168184280395508 
 batch loss: 16.095624923706055 
 batch loss: 16.06708526611328 
 batch loss: 16.12999725341797 
 batch loss: 16.040098190307617 
 batch loss: 16.0128116607666 
 batch loss: 15.947566032409668 
 batch loss: 16.07230567932129 
 batch loss: 15.992236137390137 
 batch loss: 15.967665672302246 
 batch loss: 15.761534690856934 
 batch loss: 15.801186561584473 
 batch loss: 15.76496696472168 
 batch loss: 15.664673805236816 
 batch loss: 15.595470428466797 
 batch loss: 15.574647903442383 
 batch loss: 15.512494087219238 
 batch loss: 15.420751571655273 
 batch loss: 15.508200645446777 
 batch loss: 15.529495239257812 
 batch loss: 15.401537895202637 
 batch loss: 15.293375015258789 
 batch loss: 15.274313926696777 
 batch loss: 15.204108238220215 
 batch loss: 15.12974739074707 
 batch loss: 15.13596248626709 
 batch loss: 15.038764953613281 
 batch loss: 15.095269203186035 
 batch loss: 15.175738334655762 
 batch loss: 15.086322784423828 
 batch loss: 15.168427467346191 
 batch loss: 14.992066383361816 
 batch loss: 14.828272819519043 
 batch loss: 14.944364547729492 
 batch loss: 14.95950698852539 
 batch loss: 14.836751937866211 
 batch loss: 14.77000904083252 
 batch loss: 14.857283592224121 
 batch loss: 14.730259895324707 
 batch loss: 14.8014554977417 
 batch loss: 14.838022232055664 
 batch loss: 14.793496131896973 
 batch loss: 14.769143104553223 
 batch loss: 14.711812973022461 
 batch loss: 14.922296524047852 
 batch loss: 14.705613136291504 
 batch loss: 14.685136795043945 
 batch loss: 14.86919116973877 
 batch loss: 14.812779426574707 
 batch loss: 14.808502197265625 
 batch loss: 14.825207710266113 
 batch loss: 14.778521537780762 
 batch loss: 14.813749313354492 
 batch loss: 14.74928092956543 
 batch loss: 14.768357276916504 
 batch loss: 14.67393970489502 
 batch loss: 14.769171714782715 
 batch loss: 14.697860717773438 
 batch loss: 14.851448059082031 
 batch loss: 14.634513854980469 
 batch loss: 14.79393482208252 
 batch loss: 14.717467308044434 
 Epoch 11 |Train loss: 16.026258945465088 |Validation loss: 14.7678964138031 
 batch loss: 14.647844314575195 
 batch loss: 14.69711685180664 
 batch loss: 14.759958267211914 
 batch loss: 14.620428085327148 
 batch loss: 14.602770805358887 
 batch loss: 14.57155990600586 
 batch loss: 14.821487426757812 
 batch loss: 14.6190767288208 
 batch loss: 14.94624137878418 
 batch loss: 14.60128402709961 
 batch loss: 14.727531433105469 
 batch loss: 14.9627685546875 
 batch loss: 14.537992477416992 
 batch loss: 14.802942276000977 
 batch loss: 14.636940956115723 
 batch loss: 14.653094291687012 
 batch loss: 14.617070198059082 
 batch loss: 14.551254272460938 
 batch loss: 14.497692108154297 
 batch loss: 14.57280445098877 
 batch loss: 14.496166229248047 
 batch loss: 14.516148567199707 
 batch loss: 14.530083656311035 
 batch loss: 14.495445251464844 
 batch loss: 14.415885925292969 
 batch loss: 14.401008605957031 
 batch loss: 14.377493858337402 
 batch loss: 14.408302307128906 
 batch loss: 14.360136032104492 
 batch loss: 14.338565826416016 
 batch loss: 14.315862655639648 
 batch loss: 14.298189163208008 
 batch loss: 14.367965698242188 
 batch loss: 14.244586944580078 
 batch loss: 14.281526565551758 
 batch loss: 14.207442283630371 
 batch loss: 14.248844146728516 
 batch loss: 14.220551490783691 
 batch loss: 14.234416007995605 
 batch loss: 14.189103126525879 
 batch loss: 14.817365646362305 
 batch loss: 14.333745002746582 
 batch loss: 14.176021575927734 
 batch loss: 14.304497718811035 
 batch loss: 14.261265754699707 
 batch loss: 14.084992408752441 
 batch loss: 14.115396499633789 
 batch loss: 14.098196029663086 
 batch loss: 14.038044929504395 
 batch loss: 14.272942543029785 
 batch loss: 14.008676528930664 
 batch loss: 14.068740844726562 
 batch loss: 14.138189315795898 
 batch loss: 14.055976867675781 
 batch loss: 13.975380897521973 
 batch loss: 13.964289665222168 
 batch loss: 13.9954833984375 
 batch loss: 14.030418395996094 
 batch loss: 13.98022174835205 
 batch loss: 13.993521690368652 
 batch loss: 13.975541114807129 
 batch loss: 13.932929039001465 
 batch loss: 14.103292465209961 
 batch loss: 14.065908432006836 
 batch loss: 13.83437728881836 
 batch loss: 13.83564281463623 
 batch loss: 13.815281867980957 
 batch loss: 13.770801544189453 
 batch loss: 13.785585403442383 
 batch loss: 14.334212303161621 
 batch loss: 13.834563255310059 
 batch loss: 13.940305709838867 
 batch loss: 13.795003890991211 
 batch loss: 13.822820663452148 
 batch loss: 13.882589340209961 
 batch loss: 13.877656936645508 
 batch loss: 13.780298233032227 
 batch loss: 13.820832252502441 
 batch loss: 13.814579010009766 
 batch loss: 13.726055145263672 
 batch loss: 13.717390060424805 
 batch loss: 13.701292037963867 
 batch loss: 13.719429969787598 
 batch loss: 13.646629333496094 
 batch loss: 13.764533042907715 
 batch loss: 13.780128479003906 
 batch loss: 13.766637802124023 
 batch loss: 13.885335922241211 
 batch loss: 13.833781242370605 
 batch loss: 13.80990219116211 
 batch loss: 13.75646686553955 
 batch loss: 13.813274383544922 
 batch loss: 13.804176330566406 
 batch loss: 13.622462272644043 
 batch loss: 13.64691162109375 
 batch loss: 13.80438232421875 
 batch loss: 13.7119140625 
 batch loss: 13.791594505310059 
 batch loss: 13.94406509399414 
 batch loss: 13.800143241882324 
 Epoch 12 |Train loss: 14.260715341567993 |Validation loss: 13.766022539138794 
 batch loss: 13.650941848754883 
 batch loss: 13.760801315307617 
 batch loss: 13.658548355102539 
 batch loss: 14.457136154174805 
 batch loss: 13.798429489135742 
 batch loss: 13.669154167175293 
 batch loss: 13.76071548461914 
 batch loss: 13.66265869140625 
 batch loss: 13.727458000183105 
 batch loss: 13.735875129699707 
 batch loss: 13.613786697387695 
 batch loss: 13.558223724365234 
 batch loss: 13.627843856811523 
 batch loss: 13.629082679748535 
 batch loss: 13.462913513183594 
 batch loss: 13.85982608795166 
 batch loss: 13.462674140930176 
 batch loss: 13.486931800842285 
 batch loss: 13.441146850585938 
 batch loss: 13.439834594726562 
 batch loss: 13.455018997192383 
 batch loss: 13.64980411529541 
 batch loss: 13.424495697021484 
 batch loss: 13.438613891601562 
 batch loss: 13.424118041992188 
 batch loss: 13.417224884033203 
 batch loss: 13.416214942932129 
 batch loss: 13.667550086975098 
 batch loss: 13.333284378051758 
 batch loss: 13.283116340637207 
 batch loss: 13.296114921569824 
 batch loss: 13.306998252868652 
 batch loss: 13.309189796447754 
 batch loss: 13.322854995727539 
 batch loss: 13.3281831741333 
 batch loss: 13.423086166381836 
 batch loss: 13.251802444458008 
 batch loss: 13.313082695007324 
 batch loss: 13.167994499206543 
 batch loss: 13.184947967529297 
 batch loss: 13.209141731262207 
 batch loss: 13.282978057861328 
 batch loss: 13.303762435913086 
 batch loss: 13.2152681350708 
 batch loss: 13.162922859191895 
 batch loss: 13.302632331848145 
 batch loss: 13.247987747192383 
 batch loss: 13.227043151855469 
 batch loss: 13.186747550964355 
 batch loss: 13.349227905273438 
 batch loss: 13.143485069274902 
 batch loss: 13.118739128112793 
 batch loss: 13.105402946472168 
 batch loss: 13.057267189025879 
 batch loss: 13.135791778564453 
 batch loss: 13.091699600219727 
 batch loss: 13.052094459533691 
 batch loss: 13.06834888458252 
 batch loss: 13.011812210083008 
 batch loss: 12.98701000213623 
 batch loss: 12.941004753112793 
 batch loss: 12.936380386352539 
 batch loss: 12.963078498840332 
 batch loss: 12.966466903686523 
 batch loss: 13.175936698913574 
 batch loss: 12.960114479064941 
 batch loss: 12.970553398132324 
 batch loss: 12.906484603881836 
 batch loss: 12.909745216369629 
 batch loss: 12.875771522521973 
 batch loss: 12.990729331970215 
 batch loss: 12.89641284942627 
 batch loss: 12.857810974121094 
 batch loss: 12.87525749206543 
 batch loss: 12.831321716308594 
 batch loss: 12.792028427124023 
 batch loss: 12.822358131408691 
 batch loss: 12.751286506652832 
 batch loss: 12.824308395385742 
 batch loss: 12.73422908782959 
 batch loss: 12.735544204711914 
 batch loss: 12.859125137329102 
 batch loss: 12.90319538116455 
 batch loss: 12.769126892089844 
 batch loss: 12.784334182739258 
 batch loss: 12.785374641418457 
 batch loss: 12.808099746704102 
 batch loss: 12.855096817016602 
 batch loss: 12.876029968261719 
 batch loss: 12.854972839355469 
 batch loss: 12.78710651397705 
 batch loss: 12.80780029296875 
 batch loss: 12.728739738464355 
 batch loss: 12.724124908447266 
 batch loss: 12.996127128601074 
 batch loss: 12.826783180236816 
 batch loss: 13.143415451049805 
 batch loss: 12.75950813293457 
 batch loss: 12.948421478271484 
 batch loss: 12.721282958984375 
 Epoch 13 |Train loss: 13.276479017734527 |Validation loss: 12.833710479736329 
 batch loss: 12.779132843017578 
 batch loss: 12.691250801086426 
 batch loss: 12.737582206726074 
 batch loss: 12.663785934448242 
 batch loss: 12.678147315979004 
 batch loss: 12.763340950012207 
 batch loss: 12.619343757629395 
 batch loss: 12.676310539245605 
 batch loss: 12.682193756103516 
 batch loss: 12.64587116241455 
 batch loss: 12.705696105957031 
 batch loss: 12.608440399169922 
 batch loss: 12.631115913391113 
 batch loss: 12.715348243713379 
 batch loss: 12.636747360229492 
 batch loss: 12.549866676330566 
 batch loss: 12.615937232971191 
 batch loss: 12.563345909118652 
 batch loss: 12.518083572387695 
 batch loss: 12.7896089553833 
 batch loss: 12.576400756835938 
 batch loss: 12.528054237365723 
 batch loss: 12.471376419067383 
 batch loss: 12.453044891357422 
 batch loss: 12.686858177185059 
 batch loss: 12.482034683227539 
 batch loss: 12.531451225280762 
 batch loss: 12.59936809539795 
 batch loss: 12.496140480041504 
 batch loss: 12.446617126464844 
 batch loss: 12.428616523742676 
 batch loss: 12.395894050598145 
 batch loss: 12.43398666381836 
 batch loss: 12.379236221313477 
 batch loss: 12.34486198425293 
 batch loss: 12.374841690063477 
 batch loss: 12.355473518371582 
 batch loss: 12.301867485046387 
 batch loss: 12.334248542785645 
 batch loss: 12.23891544342041 
 batch loss: 12.334488868713379 
 batch loss: 12.305793762207031 
 batch loss: 12.272624015808105 
 batch loss: 12.286039352416992 
 batch loss: 12.342802047729492 
 batch loss: 12.274024963378906 
 batch loss: 12.203088760375977 
 batch loss: 12.20870304107666 
 batch loss: 12.171001434326172 
 batch loss: 12.499794960021973 
 batch loss: 12.215784072875977 
 batch loss: 12.207709312438965 
 batch loss: 12.20838451385498 
 batch loss: 12.183829307556152 
 batch loss: 12.357888221740723 
 batch loss: 12.198481559753418 
 batch loss: 12.124299049377441 
 batch loss: 12.144660949707031 
 batch loss: 12.143447875976562 
 batch loss: 12.085668563842773 
 batch loss: 12.102890014648438 
 batch loss: 12.154013633728027 
 batch loss: 12.020445823669434 
 batch loss: 12.099806785583496 
 batch loss: 11.978289604187012 
 batch loss: 12.036025047302246 
 batch loss: 11.988037109375 
 batch loss: 11.970466613769531 
 batch loss: 11.986488342285156 
 batch loss: 11.965532302856445 
 batch loss: 11.964948654174805 
 batch loss: 11.958520889282227 
 batch loss: 11.94416332244873 
 batch loss: 11.895173072814941 
 batch loss: 11.91299057006836 
 batch loss: 11.90160846710205 
 batch loss: 11.99582290649414 
 batch loss: 11.897369384765625 
 batch loss: 11.862565994262695 
 batch loss: 11.822609901428223 
 batch loss: 11.84273624420166 
 batch loss: 11.81338119506836 
 batch loss: 11.833199501037598 
 batch loss: 11.82227897644043 
 batch loss: 11.851422309875488 
 batch loss: 11.830427169799805 
 batch loss: 11.861692428588867 
 batch loss: 11.817898750305176 
 batch loss: 11.838576316833496 
 batch loss: 11.841453552246094 
 batch loss: 11.91735553741455 
 batch loss: 11.85365104675293 
 batch loss: 11.906566619873047 
 batch loss: 11.795182228088379 
 batch loss: 11.81905460357666 
 batch loss: 11.8048734664917 
 batch loss: 11.821581840515137 
 batch loss: 11.859192848205566 
 batch loss: 11.985213279724121 
 batch loss: 11.856342315673828 
 Epoch 14 |Train loss: 12.329459011554718 |Validation loss: 11.848604011535645 
 batch loss: 11.84825611114502 
 batch loss: 11.882214546203613 
 batch loss: 11.77823257446289 
 batch loss: 11.820350646972656 
 batch loss: 11.859145164489746 
 batch loss: 11.778879165649414 
 batch loss: 11.758283615112305 
 batch loss: 11.740239143371582 
 batch loss: 11.719600677490234 
 batch loss: 11.767598152160645 
 batch loss: 11.682618141174316 
 batch loss: 11.626313209533691 
 batch loss: 11.616697311401367 
 batch loss: 11.604151725769043 
 batch loss: 11.586347579956055 
 batch loss: 11.551196098327637 
 batch loss: 11.558572769165039 
 batch loss: 11.502232551574707 
 batch loss: 11.583749771118164 
 batch loss: 11.61196517944336 
 batch loss: 11.544795036315918 
 batch loss: 11.467239379882812 
 batch loss: 11.49858283996582 
 batch loss: 11.450757026672363 
 batch loss: 11.404705047607422 
 batch loss: 11.50212287902832 
 batch loss: 11.448715209960938 
 batch loss: 11.455451011657715 
 batch loss: 11.442302703857422 
 batch loss: 11.416524887084961 
 batch loss: 11.34574031829834 
 batch loss: 11.342327117919922 
 batch loss: 11.329498291015625 
 batch loss: 11.361268997192383 
 batch loss: 11.33039665222168 
 batch loss: 11.300658226013184 
 batch loss: 11.28693962097168 
 batch loss: 11.264345169067383 
 batch loss: 11.268488883972168 
 batch loss: 11.248197555541992 
 batch loss: 11.28040885925293 
 batch loss: 11.245200157165527 
 batch loss: 11.199127197265625 
 batch loss: 11.269126892089844 
 batch loss: 11.212837219238281 
 batch loss: 11.136276245117188 
 batch loss: 11.168910026550293 
 batch loss: 11.137063026428223 
 batch loss: 11.128741264343262 
 batch loss: 11.149776458740234 
 batch loss: 11.191389083862305 
 batch loss: 11.147499084472656 
 batch loss: 11.173166275024414 
 batch loss: 11.088760375976562 
 batch loss: 11.093853950500488 
 batch loss: 11.082447052001953 
 batch loss: 11.088611602783203 
 batch loss: 11.083905220031738 
 batch loss: 11.10843276977539 
 batch loss: 11.031340599060059 
 batch loss: 11.02891731262207 
 batch loss: 11.044917106628418 
 batch loss: 10.969208717346191 
 batch loss: 10.974553108215332 
 batch loss: 11.037915229797363 
 batch loss: 11.00500774383545 
 batch loss: 11.04780387878418 
 batch loss: 11.080946922302246 
 batch loss: 11.001457214355469 
 batch loss: 10.961068153381348 
 batch loss: 10.966944694519043 
 batch loss: 10.93814754486084 
 batch loss: 10.970969200134277 
 batch loss: 10.918519020080566 
 batch loss: 10.879961967468262 
 batch loss: 10.908817291259766 
 batch loss: 10.88926887512207 
 batch loss: 10.918584823608398 
 batch loss: 10.870665550231934 
 batch loss: 10.894908905029297 
 batch loss: 10.87330436706543 
 batch loss: 10.884474754333496 
 batch loss: 10.912257194519043 
 batch loss: 10.866296768188477 
 batch loss: 10.880317687988281 
 batch loss: 10.901741981506348 
 batch loss: 10.900223731994629 
 batch loss: 10.8920259475708 
 batch loss: 10.93830394744873 
 batch loss: 10.874430656433105 
 batch loss: 10.8992919921875 
 batch loss: 10.859230041503906 
 batch loss: 10.98556137084961 
 batch loss: 10.889963150024414 
 batch loss: 10.859562873840332 
 batch loss: 10.864957809448242 
 batch loss: 10.912956237792969 
 batch loss: 10.864633560180664 
 batch loss: 10.856054306030273 
 batch loss: 10.883930206298828 
 Epoch 15 |Train loss: 11.298889470100402 |Validation loss: 10.889975929260254 
 batch loss: 10.85965633392334 
 batch loss: 10.92247486114502 
 batch loss: 10.973531723022461 
 batch loss: 10.902898788452148 
 batch loss: 10.886651039123535 
 batch loss: 10.885506629943848 
 batch loss: 10.920500755310059 
 batch loss: 10.900725364685059 
 batch loss: 10.868412971496582 
 batch loss: 10.867735862731934 
 batch loss: 10.808433532714844 
 batch loss: 10.829842567443848 
 batch loss: 10.828242301940918 
 batch loss: 10.778417587280273 
 batch loss: 10.86036205291748 
 batch loss: 10.804405212402344 
 batch loss: 10.801005363464355 
 batch loss: 10.746406555175781 
 batch loss: 10.789263725280762 
 batch loss: 10.73660659790039 
 batch loss: 10.739664077758789 
 batch loss: 10.734941482543945 
 batch loss: 10.737302780151367 
 batch loss: 10.74338436126709 
 batch loss: 10.72510814666748 
 batch loss: 10.745159149169922 
 batch loss: 10.705024719238281 
 batch loss: 10.68184757232666 
 batch loss: 10.696002960205078 
 batch loss: 10.703329086303711 
 batch loss: 10.648555755615234 
 batch loss: 10.751643180847168 
 batch loss: 10.64422607421875 
 batch loss: 10.670218467712402 
 batch loss: 10.627857208251953 
 batch loss: 10.634771347045898 
 batch loss: 10.67264461517334 
 batch loss: 10.634076118469238 
 batch loss: 10.634549140930176 
 batch loss: 10.630247116088867 
 batch loss: 10.616521835327148 
 batch loss: 10.596732139587402 
 batch loss: 10.597500801086426 
 batch loss: 10.6095609664917 
 batch loss: 10.58965015411377 
 batch loss: 10.587085723876953 
 batch loss: 10.573023796081543 
 batch loss: 10.57703685760498 
 batch loss: 10.572186470031738 
 batch loss: 10.609139442443848 
 batch loss: 10.533658027648926 
 batch loss: 10.527153968811035 
 batch loss: 10.543828964233398 
 batch loss: 10.51104736328125 
 batch loss: 10.528002738952637 
 batch loss: 10.530933380126953 
 batch loss: 10.5283784866333 
 batch loss: 10.513871192932129 
 batch loss: 10.469658851623535 
 batch loss: 10.497082710266113 
 batch loss: 10.480602264404297 
 batch loss: 10.505999565124512 
 batch loss: 10.48383617401123 
 batch loss: 10.464799880981445 
 batch loss: 10.458386421203613 
 batch loss: 10.442852973937988 
 batch loss: 10.418778419494629 
 batch loss: 10.460834503173828 
 batch loss: 10.406786918640137 
 batch loss: 10.428669929504395 
 batch loss: 10.432011604309082 
 batch loss: 10.395862579345703 
 batch loss: 10.440345764160156 
 batch loss: 10.397931098937988 
 batch loss: 10.4369478225708 
 batch loss: 10.387651443481445 
 batch loss: 10.37944507598877 
 batch loss: 10.398497581481934 
 batch loss: 10.359614372253418 
 batch loss: 10.346704483032227 
 batch loss: 10.369980812072754 
 batch loss: 10.36539363861084 
 batch loss: 10.391613006591797 
 batch loss: 10.357902526855469 
 batch loss: 10.372611045837402 
 batch loss: 10.378996849060059 
 batch loss: 10.378252983093262 
 batch loss: 10.370058059692383 
 batch loss: 10.375587463378906 
 batch loss: 10.352404594421387 
 batch loss: 10.376409530639648 
 batch loss: 10.368269920349121 
 batch loss: 10.346684455871582 
 batch loss: 10.382667541503906 
 batch loss: 10.36522388458252 
 batch loss: 10.354290962219238 
 batch loss: 10.363804817199707 
 batch loss: 10.377822875976562 
 batch loss: 10.42333984375 
 batch loss: 10.356525421142578 
 Epoch 16 |Train loss: 10.629628074169158 |Validation loss: 10.371392011642456 
 batch loss: 10.383610725402832 
 batch loss: 10.345052719116211 
 batch loss: 10.331717491149902 
 batch loss: 10.34644603729248 
 batch loss: 10.329324722290039 
 batch loss: 10.304604530334473 
 batch loss: 10.315537452697754 
 batch loss: 10.306818962097168 
 batch loss: 10.31364917755127 
 batch loss: 10.309959411621094 
 batch loss: 10.296134948730469 
 batch loss: 10.3341064453125 
 batch loss: 10.26136589050293 
 batch loss: 10.298345565795898 
 batch loss: 10.278231620788574 
 batch loss: 10.286956787109375 
 batch loss: 10.288220405578613 
 batch loss: 10.238584518432617 
 batch loss: 10.306527137756348 
 batch loss: 10.30588436126709 
 batch loss: 10.290603637695312 
 batch loss: 10.239158630371094 
 batch loss: 10.241900444030762 
 batch loss: 10.2682523727417 
 batch loss: 10.236855506896973 
 batch loss: 10.24565315246582 
 batch loss: 10.224687576293945 
 batch loss: 10.232699394226074 
 batch loss: 10.219938278198242 
 batch loss: 10.19382381439209 
 batch loss: 10.190340995788574 
 batch loss: 10.198063850402832 
 batch loss: 10.19255542755127 
 batch loss: 10.173728942871094 
 batch loss: 10.180359840393066 
 batch loss: 10.158682823181152 
 batch loss: 10.162152290344238 
 batch loss: 10.139809608459473 
 batch loss: 10.151596069335938 
 batch loss: 10.193496704101562 
 batch loss: 10.150847434997559 
 batch loss: 10.125394821166992 
 batch loss: 10.125598907470703 
 batch loss: 10.088383674621582 
 batch loss: 10.111968040466309 
 batch loss: 10.102359771728516 
 batch loss: 10.105502128601074 
 batch loss: 10.10326862335205 
 batch loss: 10.122995376586914 
 batch loss: 10.097953796386719 
 batch loss: 10.083900451660156 
 batch loss: 10.07965087890625 
 batch loss: 10.085994720458984 
 batch loss: 10.058510780334473 
 batch loss: 10.087607383728027 
 batch loss: 10.079694747924805 
 batch loss: 10.082879066467285 
 batch loss: 10.087971687316895 
 batch loss: 10.04039192199707 
 batch loss: 10.062013626098633 
 batch loss: 10.030450820922852 
 batch loss: 10.085437774658203 
 batch loss: 10.051464080810547 
 batch loss: 10.03256607055664 
 batch loss: 10.044172286987305 
 batch loss: 10.024137496948242 
 batch loss: 10.007357597351074 
 batch loss: 10.047918319702148 
 batch loss: 10.047362327575684 
 batch loss: 10.055874824523926 
 batch loss: 9.993006706237793 
 batch loss: 10.000288009643555 
 batch loss: 10.007087707519531 
 batch loss: 9.992715835571289 
 batch loss: 10.025003433227539 
 batch loss: 9.979355812072754 
 batch loss: 9.993274688720703 
 batch loss: 9.980351448059082 
 batch loss: 9.922764778137207 
 batch loss: 9.949186325073242 
 batch loss: 9.938849449157715 
 batch loss: 9.963530540466309 
 batch loss: 9.983406066894531 
 batch loss: 9.958181381225586 
 batch loss: 9.956332206726074 
 batch loss: 9.947824478149414 
 batch loss: 9.954645156860352 
 batch loss: 9.952558517456055 
 batch loss: 9.955854415893555 
 batch loss: 9.946992874145508 
 batch loss: 9.933094024658203 
 batch loss: 9.92665958404541 
 batch loss: 9.970928192138672 
 batch loss: 9.937901496887207 
 batch loss: 9.911392211914062 
 batch loss: 9.926447868347168 
 batch loss: 9.957929611206055 
 batch loss: 9.972600936889648 
 batch loss: 9.955958366394043 
 batch loss: 9.96529769897461 
 Epoch 17 |Train loss: 10.15585128068924 |Validation loss: 9.950819253921509 
 batch loss: 9.951159477233887 
 batch loss: 9.936017990112305 
 batch loss: 9.910608291625977 
 batch loss: 9.923088073730469 
 batch loss: 9.935563087463379 
 batch loss: 9.889481544494629 
 batch loss: 9.89322280883789 
 batch loss: 9.91385555267334 
 batch loss: 9.873209953308105 
 batch loss: 9.9052152633667 
 batch loss: 9.890621185302734 
 batch loss: 9.89146614074707 
 batch loss: 9.877477645874023 
 batch loss: 9.871637344360352 
 batch loss: 9.884845733642578 
 batch loss: 9.870213508605957 
 batch loss: 9.853763580322266 
 batch loss: 9.913243293762207 
 batch loss: 9.852846145629883 
 batch loss: 9.891106605529785 
 batch loss: 9.852280616760254 
 batch loss: 9.854911804199219 
 batch loss: 9.85267162322998 
 batch loss: 9.825711250305176 
 batch loss: 9.842970848083496 
 batch loss: 9.806580543518066 
 batch loss: 9.84052848815918 
 batch loss: 9.867652893066406 
 batch loss: 9.792580604553223 
 batch loss: 9.849764823913574 
 batch loss: 9.787459373474121 
 batch loss: 9.833934783935547 
 batch loss: 9.791422843933105 
 batch loss: 9.821751594543457 
 batch loss: 9.820284843444824 
 batch loss: 9.823548316955566 
 batch loss: 9.795732498168945 
 batch loss: 9.817255973815918 
 batch loss: 9.794730186462402 
 batch loss: 9.778297424316406 
 batch loss: 9.758103370666504 
 batch loss: 9.784987449645996 
 batch loss: 9.769538879394531 
 batch loss: 9.78752326965332 
 batch loss: 9.743110656738281 
 batch loss: 9.778478622436523 
 batch loss: 9.783968925476074 
 batch loss: 9.740137100219727 
 batch loss: 9.735316276550293 
 batch loss: 9.729508399963379 
 batch loss: 9.72917652130127 
 batch loss: 9.760010719299316 
 batch loss: 9.721552848815918 
 batch loss: 9.748456001281738 
 batch loss: 9.756747245788574 
 batch loss: 9.727265357971191 
 batch loss: 9.716083526611328 
 batch loss: 9.705039024353027 
 batch loss: 9.700067520141602 
 batch loss: 9.674220085144043 
 batch loss: 9.70835018157959 
 batch loss: 9.698418617248535 
 batch loss: 9.69528865814209 
 batch loss: 9.662775993347168 
 batch loss: 9.657526969909668 
 batch loss: 9.655728340148926 
 batch loss: 9.673432350158691 
 batch loss: 9.643125534057617 
 batch loss: 9.634443283081055 
 batch loss: 9.653458595275879 
 batch loss: 9.643868446350098 
 batch loss: 9.643441200256348 
 batch loss: 9.651177406311035 
 batch loss: 9.646425247192383 
 batch loss: 9.651488304138184 
 batch loss: 9.638341903686523 
 batch loss: 9.635759353637695 
 batch loss: 9.64202880859375 
 batch loss: 9.653827667236328 
 batch loss: 9.623836517333984 
 batch loss: 9.629646301269531 
 batch loss: 9.604047775268555 
 batch loss: 9.622968673706055 
 batch loss: 9.612895965576172 
 batch loss: 9.615208625793457 
 batch loss: 9.607802391052246 
 batch loss: 9.650822639465332 
 batch loss: 9.612027168273926 
 batch loss: 9.61185359954834 
 batch loss: 9.674169540405273 
 batch loss: 9.61096477508545 
 batch loss: 9.642480850219727 
 batch loss: 9.678744316101074 
 batch loss: 9.595489501953125 
 batch loss: 9.621437072753906 
 batch loss: 9.681599617004395 
 batch loss: 9.62563705444336 
 batch loss: 9.616337776184082 
 batch loss: 9.613929748535156 
 batch loss: 9.601069450378418 
 Epoch 18 |Train loss: 9.779259371757508 |Validation loss: 9.62645664215088 
 batch loss: 9.612497329711914 
 batch loss: 9.615982055664062 
 batch loss: 9.605175971984863 
 batch loss: 9.58694076538086 
 batch loss: 9.57613754272461 
 batch loss: 9.601133346557617 
 batch loss: 9.565987586975098 
 batch loss: 9.574731826782227 
 batch loss: 9.578044891357422 
 batch loss: 9.56846809387207 
 batch loss: 9.580533981323242 
 batch loss: 9.55908489227295 
 batch loss: 9.559560775756836 
 batch loss: 9.584712982177734 
 batch loss: 9.562448501586914 
 batch loss: 9.541340827941895 
 batch loss: 9.561543464660645 
 batch loss: 9.549607276916504 
 batch loss: 9.550128936767578 
 batch loss: 9.55660343170166 
 batch loss: 9.532252311706543 
 batch loss: 9.545707702636719 
 batch loss: 9.504264831542969 
 batch loss: 9.534825325012207 
 batch loss: 9.52054500579834 
 batch loss: 9.519651412963867 
 batch loss: 9.520472526550293 
 batch loss: 9.513794898986816 
 batch loss: 9.502613067626953 
 batch loss: 9.499760627746582 
 batch loss: 9.50567626953125 
 batch loss: 9.493457794189453 
 batch loss: 9.48554801940918 
 batch loss: 9.49986457824707 
 batch loss: 9.509745597839355 
 batch loss: 9.500272750854492 
 batch loss: 9.471455574035645 
 batch loss: 9.488932609558105 
 batch loss: 9.49972152709961 
 batch loss: 9.49011516571045 
 batch loss: 9.47852897644043 
 batch loss: 9.451309204101562 
 batch loss: 9.446002960205078 
 batch loss: 9.448569297790527 
 batch loss: 9.438139915466309 
 batch loss: 9.499120712280273 
 batch loss: 9.444275856018066 
 batch loss: 9.46536922454834 
 batch loss: 9.433496475219727 
 batch loss: 9.424256324768066 
 batch loss: 9.420494079589844 
 batch loss: 9.437607765197754 
 batch loss: 9.42195987701416 
 batch loss: 9.441289901733398 
 batch loss: 9.398030281066895 
 batch loss: 9.417661666870117 
 batch loss: 9.402746200561523 
 batch loss: 9.405730247497559 
 batch loss: 9.389546394348145 
 batch loss: 9.431687355041504 
 batch loss: 9.392007827758789 
 batch loss: 9.386274337768555 
 batch loss: 9.382339477539062 
 batch loss: 9.393767356872559 
 batch loss: 9.38002872467041 
 batch loss: 9.370384216308594 
 batch loss: 9.393525123596191 
 batch loss: 9.357917785644531 
 batch loss: 9.361602783203125 
 batch loss: 9.361173629760742 
 batch loss: 9.371009826660156 
 batch loss: 9.352478981018066 
 batch loss: 9.38627815246582 
 batch loss: 9.334888458251953 
 batch loss: 9.336854934692383 
 batch loss: 9.362555503845215 
 batch loss: 9.329845428466797 
 batch loss: 9.343221664428711 
 batch loss: 9.352432250976562 
 batch loss: 9.32702350616455 
 batch loss: 9.329252243041992 
 batch loss: 9.336323738098145 
 batch loss: 9.356754302978516 
 batch loss: 9.326638221740723 
 batch loss: 9.355124473571777 
 batch loss: 9.363131523132324 
 batch loss: 9.371052742004395 
 batch loss: 9.381606101989746 
 batch loss: 9.366568565368652 
 batch loss: 9.344972610473633 
 batch loss: 9.32938003540039 
 batch loss: 9.335314750671387 
 batch loss: 9.351075172424316 
 batch loss: 9.327605247497559 
 batch loss: 9.322232246398926 
 batch loss: 9.337958335876465 
 batch loss: 9.347241401672363 
 batch loss: 9.334244728088379 
 batch loss: 9.338533401489258 
 batch loss: 9.328937530517578 
 Epoch 19 |Train loss: 9.470009684562683 |Validation loss: 9.344197368621826 
