     
 ID: Net1_v1 
 infomation: baseline -- stack then extract feature 
 Epoch number: 20 
 Batch size: 2 
 =======================

Net_1(
  (feature_extract): FeatureNet(
    (net): Sequential(
      (0): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (1): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (2): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (3): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (4): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (5): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 320, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
    )
  )
  (disp_extracct): DispNet(
    (net): Sequential(
      (0): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (1): ConvTranspose2d_block(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (2): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (3): ConvTranspose2d_block(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (4): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (5): ConvTranspose2d_block(
        (net): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace)
        )
      )
      (6): Conv2d_block(
        (net): Sequential(
          (0): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): Softmax()
    )
  )
  (regression): DisparityRegression()
) batch loss: 94.80308532714844 
 batch loss: 92.91494750976562 
 batch loss: 91.3719253540039 
 batch loss: 90.41239929199219 
 batch loss: 88.85323333740234 
 batch loss: 85.96092987060547 
 batch loss: 86.80821228027344 
 batch loss: 84.80236053466797 
 batch loss: 83.03889465332031 
 batch loss: 82.15042877197266 
 batch loss: 80.05665588378906 
 batch loss: 78.71834564208984 
 batch loss: 78.75703430175781 
 batch loss: 78.0110855102539 
 batch loss: 77.1827621459961 
 batch loss: 76.80426788330078 
 batch loss: 76.2330551147461 
 batch loss: 76.44184112548828 
 batch loss: 75.33859252929688 
 batch loss: 75.35591125488281 
 batch loss: 74.95768737792969 
 batch loss: 74.79627990722656 
 batch loss: 75.15023803710938 
 batch loss: 74.37068176269531 
 batch loss: 74.05699157714844 
 batch loss: 73.71940612792969 
 batch loss: 73.38733673095703 
 batch loss: 74.20115661621094 
 batch loss: 73.20784759521484 
 batch loss: 73.20523071289062 
 batch loss: 72.7743148803711 
 batch loss: 72.79287719726562 
 batch loss: 72.71665954589844 
 batch loss: 72.5240478515625 
 batch loss: 72.62937927246094 
 batch loss: 72.82112121582031 
 batch loss: 72.18669128417969 
 batch loss: 72.02577209472656 
 batch loss: 72.02287292480469 
 batch loss: 71.9117202758789 
 batch loss: 71.92992401123047 
 batch loss: 71.55760955810547 
 batch loss: 71.66637420654297 
 batch loss: 71.51961517333984 
 batch loss: 71.21662139892578 
 batch loss: 71.20450592041016 
 batch loss: 71.21737670898438 
 batch loss: 70.98514556884766 
 batch loss: 70.75497436523438 
 batch loss: 70.61798858642578 
 batch loss: 70.59894561767578 
 batch loss: 70.59547424316406 
 batch loss: 70.40253448486328 
 batch loss: 70.24535369873047 
 batch loss: 70.11083221435547 
 batch loss: 70.02601623535156 
 batch loss: 71.4260025024414 
 batch loss: 69.98434448242188 
 batch loss: 69.67340850830078 
 batch loss: 69.63709259033203 
 batch loss: 69.49835205078125 
 batch loss: 69.55598449707031 
 batch loss: 69.35746765136719 
 batch loss: 69.3198013305664 
 batch loss: 69.22621154785156 
 batch loss: 69.10258483886719 
 batch loss: 69.10213470458984 
 batch loss: 68.73638153076172 
 batch loss: 68.7587890625 
 batch loss: 68.64886474609375 
 batch loss: 68.5528564453125 
 batch loss: 68.43435668945312 
 batch loss: 68.35694885253906 
 batch loss: 68.21553039550781 
 batch loss: 68.22805786132812 
 batch loss: 67.99822998046875 
 batch loss: 67.8433837890625 
 batch loss: 67.87510681152344 
 batch loss: 67.79283905029297 
 batch loss: 67.65359497070312 
 batch loss: 67.54491424560547 
 batch loss: 67.54922485351562 
 batch loss: 67.60570526123047 
 batch loss: 67.57951354980469 
 batch loss: 67.56283569335938 
 batch loss: 67.55960083007812 
 batch loss: 67.58726501464844 
 batch loss: 67.62683868408203 
 batch loss: 67.59209442138672 
 batch loss: 67.52745819091797 
 batch loss: 67.5655517578125 
 batch loss: 67.51510620117188 
 batch loss: 67.5107192993164 
 batch loss: 67.55461883544922 
 batch loss: 67.5152816772461 
 batch loss: 67.49203491210938 
 batch loss: 67.50255584716797 
 batch loss: 67.57139587402344 
 batch loss: 67.48933410644531 
 batch loss: 67.52787780761719 
 Epoch 0 |Train loss: 73.83877372741699 |Validation loss: 67.54899635314942 
 batch loss: 67.51617431640625 
 batch loss: 67.38993072509766 
 batch loss: 67.33753204345703 
 batch loss: 67.2551498413086 
 batch loss: 67.11419677734375 
 batch loss: 67.04907989501953 
 batch loss: 66.81048583984375 
 batch loss: 66.85802459716797 
 batch loss: 66.75778198242188 
 batch loss: 66.68112182617188 
 batch loss: 66.66704559326172 
 batch loss: 66.4587173461914 
 batch loss: 66.35816192626953 
 batch loss: 66.2926025390625 
 batch loss: 66.12934112548828 
 batch loss: 66.13761138916016 
 batch loss: 65.98194122314453 
 batch loss: 65.94709014892578 
 batch loss: 65.7625732421875 
 batch loss: 65.72234344482422 
 batch loss: 65.64978790283203 
 batch loss: 65.4677505493164 
 batch loss: 65.3549575805664 
 batch loss: 65.29396057128906 
 batch loss: 65.26058197021484 
 batch loss: 65.15321350097656 
 batch loss: 65.05809020996094 
 batch loss: 64.92655944824219 
 batch loss: 64.80455780029297 
 batch loss: 64.82068634033203 
 batch loss: 64.61260223388672 
 batch loss: 64.58680725097656 
 batch loss: 64.46312713623047 
 batch loss: 64.38782501220703 
 batch loss: 64.29217529296875 
 batch loss: 64.19254302978516 
 batch loss: 64.0435791015625 
 batch loss: 63.9749870300293 
 batch loss: 63.88813400268555 
 batch loss: 63.84595489501953 
 batch loss: 63.67510986328125 
 batch loss: 63.60538101196289 
 batch loss: 63.52603530883789 
 batch loss: 63.3261833190918 
 batch loss: 63.318233489990234 
 batch loss: 63.195648193359375 
 batch loss: 63.150428771972656 
 batch loss: 63.04694747924805 
 batch loss: 62.8842658996582 
 batch loss: 62.82758712768555 
 batch loss: 62.74577331542969 
 batch loss: 62.62119674682617 
 batch loss: 62.48293685913086 
 batch loss: 62.454158782958984 
 batch loss: 62.30851364135742 
 batch loss: 62.25601577758789 
 batch loss: 62.1493034362793 
 batch loss: 62.02519989013672 
 batch loss: 61.91688919067383 
 batch loss: 61.82535934448242 
 batch loss: 61.750083923339844 
 batch loss: 61.626502990722656 
 batch loss: 61.4935188293457 
 batch loss: 61.340240478515625 
 batch loss: 61.2474479675293 
 batch loss: 61.06078338623047 
 batch loss: 60.92523193359375 
 batch loss: 60.70786666870117 
 batch loss: 60.54283142089844 
 batch loss: 60.39439392089844 
 batch loss: 60.21656036376953 
 batch loss: 60.14442825317383 
 batch loss: 59.97319030761719 
 batch loss: 59.79680633544922 
 batch loss: 59.63401794433594 
 batch loss: 59.46465301513672 
 batch loss: 59.2765998840332 
 batch loss: 59.109066009521484 
 batch loss: 59.554805755615234 
 batch loss: 59.03520965576172 
 batch loss: 58.830039978027344 
 batch loss: 58.911033630371094 
 batch loss: 58.98686981201172 
 batch loss: 58.899452209472656 
 batch loss: 58.91360855102539 
 batch loss: 58.926822662353516 
 batch loss: 58.80046081542969 
 batch loss: 58.98400115966797 
 batch loss: 58.98861312866211 
 batch loss: 58.814117431640625 
 batch loss: 58.78923034667969 
 batch loss: 58.774871826171875 
 batch loss: 58.968223571777344 
 batch loss: 58.93059158325195 
 batch loss: 58.97104263305664 
 batch loss: 58.7998161315918 
 batch loss: 58.78630065917969 
 batch loss: 58.84004211425781 
 batch loss: 58.81822204589844 
 batch loss: 58.865421295166016 
 Epoch 1 |Train loss: 63.61175241470337 |Validation loss: 58.87993907928467 
 batch loss: 58.860836029052734 
 batch loss: 58.580562591552734 
 batch loss: 58.57644271850586 
 batch loss: 58.37314987182617 
 batch loss: 58.2098274230957 
 batch loss: 58.08554458618164 
 batch loss: 58.09273910522461 
 batch loss: 58.09721755981445 
 batch loss: 57.782806396484375 
 batch loss: 57.686954498291016 
 batch loss: 57.5674934387207 
 batch loss: 57.41434860229492 
 batch loss: 57.438629150390625 
 batch loss: 57.275447845458984 
 batch loss: 57.131587982177734 
 batch loss: 57.166343688964844 
 batch loss: 57.03586959838867 
 batch loss: 57.11940002441406 
 batch loss: 56.82499694824219 
 batch loss: 56.75513458251953 
 batch loss: 56.94773483276367 
 batch loss: 56.60887908935547 
 batch loss: 56.4425163269043 
 batch loss: 56.39073181152344 
 batch loss: 56.257164001464844 
 batch loss: 56.14418029785156 
 batch loss: 56.181026458740234 
 batch loss: 55.98710250854492 
 batch loss: 56.03369903564453 
 batch loss: 55.835357666015625 
 batch loss: 55.72956466674805 
 batch loss: 55.854278564453125 
 batch loss: 55.473812103271484 
 batch loss: 55.35728073120117 
 batch loss: 55.2874870300293 
 batch loss: 55.17427444458008 
 batch loss: 55.091861724853516 
 batch loss: 54.995243072509766 
 batch loss: 54.89581298828125 
 batch loss: 54.74372863769531 
 batch loss: 54.6556396484375 
 batch loss: 54.5710563659668 
 batch loss: 54.463523864746094 
 batch loss: 54.38204574584961 
 batch loss: 54.208946228027344 
 batch loss: 54.2041015625 
 batch loss: 53.9949836730957 
 batch loss: 53.97010040283203 
 batch loss: 53.90547180175781 
 batch loss: 53.84027862548828 
 batch loss: 53.632652282714844 
 batch loss: 53.5561637878418 
 batch loss: 53.44338607788086 
 batch loss: 53.320125579833984 
 batch loss: 53.12922668457031 
 batch loss: 53.076560974121094 
 batch loss: 52.92504119873047 
 batch loss: 52.92948532104492 
 batch loss: 52.798221588134766 
 batch loss: 52.65739822387695 
 batch loss: 52.50191116333008 
 batch loss: 52.60591506958008 
 batch loss: 52.244083404541016 
 batch loss: 52.155418395996094 
 batch loss: 51.91347885131836 
 batch loss: 51.758365631103516 
 batch loss: 51.632850646972656 
 batch loss: 51.51435089111328 
 batch loss: 51.42555236816406 
 batch loss: 51.25751876831055 
 batch loss: 51.00774383544922 
 batch loss: 51.00271987915039 
 batch loss: 50.74266815185547 
 batch loss: 50.655029296875 
 batch loss: 50.47468566894531 
 batch loss: 50.41046142578125 
 batch loss: 50.16139602661133 
 batch loss: 50.04502868652344 
 batch loss: 49.89242172241211 
 batch loss: 49.690914154052734 
 batch loss: 49.53265380859375 
 batch loss: 49.49900817871094 
 batch loss: 49.597312927246094 
 batch loss: 49.51272964477539 
 batch loss: 49.57931900024414 
 batch loss: 49.68815231323242 
 batch loss: 49.666194915771484 
 batch loss: 49.443668365478516 
 batch loss: 49.50425720214844 
 batch loss: 49.5893669128418 
 batch loss: 49.495845794677734 
 batch loss: 49.49329376220703 
 batch loss: 49.46811294555664 
 batch loss: 49.64521026611328 
 batch loss: 49.63208770751953 
 batch loss: 49.54692840576172 
 batch loss: 49.695281982421875 
 batch loss: 49.69072723388672 
 batch loss: 49.74989318847656 
 batch loss: 49.49064254760742 
 Epoch 2 |Train loss: 54.57829990386963 |Validation loss: 49.57603435516357 
 batch loss: 49.587459564208984 
 batch loss: 49.30327224731445 
 batch loss: 49.2037239074707 
 batch loss: 49.07877731323242 
 batch loss: 49.011253356933594 
 batch loss: 48.585269927978516 
 batch loss: 48.15383529663086 
 batch loss: 48.11052703857422 
 batch loss: 48.00587844848633 
 batch loss: 47.38389205932617 
 batch loss: 48.20436477661133 
 batch loss: 47.753074645996094 
 batch loss: 49.46234130859375 
 batch loss: 47.62689971923828 
 batch loss: 47.492889404296875 
 batch loss: 47.93150329589844 
 batch loss: 47.090423583984375 
 batch loss: 46.8239631652832 
 batch loss: 46.61016082763672 
 batch loss: 46.9649543762207 
 batch loss: 46.40485382080078 
 batch loss: 46.53630065917969 
 batch loss: 46.10639953613281 
 batch loss: 45.91676712036133 
 batch loss: 45.815677642822266 
 batch loss: 45.650146484375 
 batch loss: 45.49773025512695 
 batch loss: 45.39812088012695 
 batch loss: 45.34062194824219 
 batch loss: 45.08210754394531 
 batch loss: 45.04304122924805 
 batch loss: 44.90263748168945 
 batch loss: 44.83148193359375 
 batch loss: 44.714111328125 
 batch loss: 44.57646942138672 
 batch loss: 44.34187316894531 
 batch loss: 44.388790130615234 
 batch loss: 44.268009185791016 
 batch loss: 44.23969650268555 
 batch loss: 44.181148529052734 
 batch loss: 44.198211669921875 
 batch loss: 44.58614730834961 
 batch loss: 43.86639404296875 
 batch loss: 44.43142318725586 
 batch loss: 44.8661994934082 
 batch loss: 43.946449279785156 
 batch loss: 43.997657775878906 
 batch loss: 43.557186126708984 
 batch loss: 43.44218063354492 
 batch loss: 43.265811920166016 
 batch loss: 43.337650299072266 
 batch loss: 43.20264434814453 
 batch loss: 43.076210021972656 
 batch loss: 42.91032028198242 
 batch loss: 43.08926010131836 
 batch loss: 42.83830642700195 
 batch loss: 42.80562210083008 
 batch loss: 42.652896881103516 
 batch loss: 42.5144157409668 
 batch loss: 42.41988754272461 
 batch loss: 42.39166259765625 
 batch loss: 42.29023361206055 
 batch loss: 42.31100845336914 
 batch loss: 42.1440544128418 
 batch loss: 42.05422592163086 
 batch loss: 41.91404724121094 
 batch loss: 41.89148712158203 
 batch loss: 41.948455810546875 
 batch loss: 41.59760284423828 
 batch loss: 41.64173889160156 
 batch loss: 41.47208023071289 
 batch loss: 41.52031326293945 
 batch loss: 41.345916748046875 
 batch loss: 41.23051834106445 
 batch loss: 41.161949157714844 
 batch loss: 41.06541442871094 
 batch loss: 41.15071105957031 
 batch loss: 41.15122985839844 
 batch loss: 41.39743423461914 
 batch loss: 41.015384674072266 
 batch loss: 41.06924819946289 
 batch loss: 41.368045806884766 
 batch loss: 41.049034118652344 
 batch loss: 41.096900939941406 
 batch loss: 41.26748275756836 
 batch loss: 41.41720199584961 
 batch loss: 40.94145584106445 
 batch loss: 40.90409851074219 
 batch loss: 40.94065856933594 
 batch loss: 40.99884033203125 
 batch loss: 41.1710205078125 
 batch loss: 40.97709655761719 
 batch loss: 40.90955352783203 
 batch loss: 41.0694465637207 
 batch loss: 41.023658752441406 
 batch loss: 41.32176208496094 
 batch loss: 40.952972412109375 
 batch loss: 41.189598083496094 
 batch loss: 42.106101989746094 
 batch loss: 41.8178596496582 
 Epoch 3 |Train loss: 44.591509914398195 |Validation loss: 41.17960186004639 
 batch loss: 41.147396087646484 
 batch loss: 40.994384765625 
 batch loss: 40.617191314697266 
 batch loss: 40.597537994384766 
 batch loss: 40.90424728393555 
 batch loss: 40.51392364501953 
 batch loss: 40.19026565551758 
 batch loss: 40.246864318847656 
 batch loss: 40.33269500732422 
 batch loss: 40.195045471191406 
 batch loss: 40.322486877441406 
 batch loss: 40.41084289550781 
 batch loss: 40.45100784301758 
 batch loss: 40.863059997558594 
 batch loss: 39.816871643066406 
 batch loss: 39.853755950927734 
 batch loss: 39.692955017089844 
 batch loss: 39.60804748535156 
 batch loss: 39.613304138183594 
 batch loss: 39.529930114746094 
 batch loss: 39.29439926147461 
 batch loss: 39.419456481933594 
 batch loss: 39.274173736572266 
 batch loss: 39.482330322265625 
 batch loss: 38.8813591003418 
 batch loss: 39.0116081237793 
 batch loss: 38.923458099365234 
 batch loss: 38.90003967285156 
 batch loss: 38.81990432739258 
 batch loss: 38.6824951171875 
 batch loss: 38.62828063964844 
 batch loss: 38.46733093261719 
 batch loss: 38.4227409362793 
 batch loss: 38.33386993408203 
 batch loss: 38.2756462097168 
 batch loss: 38.4058723449707 
 batch loss: 38.56031036376953 
 batch loss: 38.54417419433594 
 batch loss: 38.40065383911133 
 batch loss: 38.31399917602539 
 batch loss: 38.016258239746094 
 batch loss: 38.15821838378906 
 batch loss: 37.75211715698242 
 batch loss: 37.54473114013672 
 batch loss: 37.54958724975586 
 batch loss: 37.6028938293457 
 batch loss: 37.596961975097656 
 batch loss: 37.39914321899414 
 batch loss: 37.361141204833984 
 batch loss: 37.2955436706543 
 batch loss: 37.59013366699219 
 batch loss: 37.26192092895508 
 batch loss: 37.2747688293457 
 batch loss: 37.35959243774414 
 batch loss: 36.965599060058594 
 batch loss: 37.07325744628906 
 batch loss: 37.04597091674805 
 batch loss: 36.92444610595703 
 batch loss: 36.838134765625 
 batch loss: 36.88589859008789 
 batch loss: 36.738468170166016 
 batch loss: 36.729774475097656 
 batch loss: 36.6048469543457 
 batch loss: 36.6788330078125 
 batch loss: 36.50680160522461 
 batch loss: 36.67808151245117 
 batch loss: 36.436668395996094 
 batch loss: 36.287811279296875 
 batch loss: 36.215824127197266 
 batch loss: 36.08274459838867 
 batch loss: 35.99578857421875 
 batch loss: 36.011512756347656 
 batch loss: 36.01549530029297 
 batch loss: 35.798423767089844 
 batch loss: 35.89324951171875 
 batch loss: 35.753849029541016 
 batch loss: 35.74135208129883 
 batch loss: 35.6400260925293 
 batch loss: 35.54379653930664 
 batch loss: 35.573646545410156 
 batch loss: 35.54761505126953 
 batch loss: 35.56615447998047 
 batch loss: 35.67134475708008 
 batch loss: 35.6352653503418 
 batch loss: 35.5478630065918 
 batch loss: 35.6094970703125 
 batch loss: 35.590576171875 
 batch loss: 35.5111198425293 
 batch loss: 35.633853912353516 
 batch loss: 35.54289627075195 
 batch loss: 35.46936798095703 
 batch loss: 35.65101623535156 
 batch loss: 35.43132400512695 
 batch loss: 35.46177673339844 
 batch loss: 35.82951736450195 
 batch loss: 35.72939682006836 
 batch loss: 35.6168327331543 
 batch loss: 35.700904846191406 
 batch loss: 36.067562103271484 
 batch loss: 35.435951232910156 
 Epoch 4 |Train loss: 38.14209036827087 |Validation loss: 35.61249179840088 
 batch loss: 35.419437408447266 
 batch loss: 35.63408660888672 
 batch loss: 35.69492721557617 
 batch loss: 35.38078308105469 
 batch loss: 35.49715805053711 
 batch loss: 35.292152404785156 
 batch loss: 35.19964599609375 
 batch loss: 35.16284942626953 
 batch loss: 35.74195861816406 
 batch loss: 35.14274978637695 
 batch loss: 34.909217834472656 
 batch loss: 35.162044525146484 
 batch loss: 34.8337287902832 
 batch loss: 34.897972106933594 
 batch loss: 34.707176208496094 
 batch loss: 34.99198913574219 
 batch loss: 34.57164764404297 
 batch loss: 34.34321212768555 
 batch loss: 34.33791732788086 
 batch loss: 34.224761962890625 
 batch loss: 34.022674560546875 
 batch loss: 34.029876708984375 
 batch loss: 33.848995208740234 
 batch loss: 33.97334289550781 
 batch loss: 33.806640625 
 batch loss: 33.61716079711914 
 batch loss: 33.491451263427734 
 batch loss: 33.38560104370117 
 batch loss: 33.245243072509766 
 batch loss: 33.22773361206055 
 batch loss: 33.08717346191406 
 batch loss: 33.21726989746094 
 batch loss: 33.19548416137695 
 batch loss: 33.37984085083008 
 batch loss: 32.794490814208984 
 batch loss: 33.04472732543945 
 batch loss: 32.62647247314453 
 batch loss: 32.7991828918457 
 batch loss: 32.572303771972656 
 batch loss: 32.30714416503906 
 batch loss: 32.39480972290039 
 batch loss: 32.2647819519043 
 batch loss: 32.24185562133789 
 batch loss: 32.033905029296875 
 batch loss: 31.981014251708984 
 batch loss: 32.23922348022461 
 batch loss: 32.55030822753906 
 batch loss: 32.0369758605957 
 batch loss: 32.35026550292969 
 batch loss: 32.06454849243164 
 batch loss: 31.616119384765625 
 batch loss: 31.723228454589844 
 batch loss: 31.447351455688477 
 batch loss: 31.615211486816406 
 batch loss: 31.371732711791992 
 batch loss: 31.26300621032715 
 batch loss: 31.22034454345703 
 batch loss: 31.297151565551758 
 batch loss: 31.127920150756836 
 batch loss: 31.085081100463867 
 batch loss: 30.926443099975586 
 batch loss: 31.151037216186523 
 batch loss: 31.049901962280273 
 batch loss: 30.78241729736328 
 batch loss: 30.861726760864258 
 batch loss: 30.800703048706055 
 batch loss: 30.896486282348633 
 batch loss: 30.77065658569336 
 batch loss: 30.630294799804688 
 batch loss: 30.45858383178711 
 batch loss: 30.441242218017578 
 batch loss: 30.380504608154297 
 batch loss: 30.358171463012695 
 batch loss: 30.471464157104492 
 batch loss: 30.531085968017578 
 batch loss: 30.403587341308594 
 batch loss: 31.518075942993164 
 batch loss: 30.28010368347168 
 batch loss: 30.235633850097656 
 batch loss: 30.02686309814453 
 batch loss: 30.252702713012695 
 batch loss: 30.022323608398438 
 batch loss: 30.335220336914062 
 batch loss: 30.134902954101562 
 batch loss: 30.34650230407715 
 batch loss: 30.047744750976562 
 batch loss: 30.197830200195312 
 batch loss: 30.137428283691406 
 batch loss: 30.109962463378906 
 batch loss: 30.143068313598633 
 batch loss: 29.994327545166016 
 batch loss: 29.96900177001953 
 batch loss: 30.171085357666016 
 batch loss: 30.13315773010254 
 batch loss: 30.68621253967285 
 batch loss: 30.032470703125 
 batch loss: 30.43568229675293 
 batch loss: 30.001564025878906 
 batch loss: 30.2447509765625 
 batch loss: 29.996665954589844 
 Epoch 5 |Train loss: 32.69647555351257 |Validation loss: 30.169630241394042 
 batch loss: 29.950057983398438 
 batch loss: 29.95330238342285 
 batch loss: 30.100446701049805 
 batch loss: 29.926742553710938 
 batch loss: 29.974504470825195 
 batch loss: 29.697200775146484 
 batch loss: 29.733884811401367 
 batch loss: 30.011211395263672 
 batch loss: 29.722309112548828 
 batch loss: 29.7430419921875 
 batch loss: 29.923080444335938 
 batch loss: 29.844852447509766 
 batch loss: 29.82122230529785 
 batch loss: 29.577917098999023 
 batch loss: 29.548912048339844 
 batch loss: 29.766639709472656 
 batch loss: 29.599994659423828 
 batch loss: 29.710163116455078 
 batch loss: 29.508052825927734 
 batch loss: 29.380104064941406 
 batch loss: 29.681636810302734 
 batch loss: 29.117420196533203 
 batch loss: 29.414505004882812 
 batch loss: 29.24790382385254 
 batch loss: 29.413738250732422 
 batch loss: 29.178604125976562 
 batch loss: 29.096036911010742 
 batch loss: 29.12491798400879 
 batch loss: 29.014392852783203 
 batch loss: 29.043170928955078 
 batch loss: 28.961984634399414 
 batch loss: 28.86980628967285 
 batch loss: 28.87860107421875 
 batch loss: 28.981731414794922 
 batch loss: 28.935619354248047 
 batch loss: 28.91238021850586 
 batch loss: 28.6052188873291 
 batch loss: 29.462461471557617 
 batch loss: 29.43546485900879 
 batch loss: 28.799711227416992 
 batch loss: 28.756696701049805 
 batch loss: 28.85185432434082 
 batch loss: 29.033212661743164 
 batch loss: 28.568544387817383 
 batch loss: 28.613597869873047 
 batch loss: 28.64724349975586 
 batch loss: 28.657732009887695 
 batch loss: 28.610017776489258 
 batch loss: 28.37539291381836 
 batch loss: 28.740619659423828 
 batch loss: 28.585691452026367 
 batch loss: 28.26895523071289 
 batch loss: 28.345684051513672 
 batch loss: 28.22899627685547 
 batch loss: 28.230587005615234 
 batch loss: 28.18733024597168 
 batch loss: 28.181814193725586 
 batch loss: 28.13336753845215 
 batch loss: 28.450424194335938 
 batch loss: 28.31203269958496 
 batch loss: 28.062593460083008 
 batch loss: 28.026081085205078 
 batch loss: 28.08837890625 
 batch loss: 27.919845581054688 
 batch loss: 28.060176849365234 
 batch loss: 27.86673355102539 
 batch loss: 27.823963165283203 
 batch loss: 27.831295013427734 
 batch loss: 27.92238426208496 
 batch loss: 27.74906349182129 
 batch loss: 27.9882755279541 
 batch loss: 27.78776741027832 
 batch loss: 27.771839141845703 
 batch loss: 27.77235221862793 
 batch loss: 27.713407516479492 
 batch loss: 27.696399688720703 
 batch loss: 27.634918212890625 
 batch loss: 27.606220245361328 
 batch loss: 27.762235641479492 
 batch loss: 27.452529907226562 
 batch loss: 27.66120719909668 
 batch loss: 27.41800880432129 
 batch loss: 27.54505157470703 
 batch loss: 27.531200408935547 
 batch loss: 27.428115844726562 
 batch loss: 27.606393814086914 
 batch loss: 27.73299789428711 
 batch loss: 27.546770095825195 
 batch loss: 27.391115188598633 
 batch loss: 27.511844635009766 
 batch loss: 27.433029174804688 
 batch loss: 27.425132751464844 
 batch loss: 27.37985610961914 
 batch loss: 27.61056900024414 
 batch loss: 27.543437957763672 
 batch loss: 27.45879364013672 
 batch loss: 27.707212448120117 
 batch loss: 27.397079467773438 
 batch loss: 27.671289443969727 
 batch loss: 27.48649024963379 
 Epoch 6 |Train loss: 28.799815034866334 |Validation loss: 27.52427978515625 
 batch loss: 27.525188446044922 
 batch loss: 27.39733123779297 
 batch loss: 27.59407615661621 
 batch loss: 27.397775650024414 
 batch loss: 27.538442611694336 
 batch loss: 28.450490951538086 
 batch loss: 27.48232650756836 
 batch loss: 27.51567840576172 
 batch loss: 27.481454849243164 
 batch loss: 27.47899627685547 
 batch loss: 27.348909378051758 
 batch loss: 27.580913543701172 
 batch loss: 27.23600196838379 
 batch loss: 27.13711166381836 
 batch loss: 27.12831687927246 
 batch loss: 27.19644546508789 
 batch loss: 27.029380798339844 
 batch loss: 26.92626953125 
 batch loss: 26.868526458740234 
 batch loss: 26.78903579711914 
 batch loss: 26.78542709350586 
 batch loss: 26.99395179748535 
 batch loss: 27.08871078491211 
 batch loss: 26.940990447998047 
 batch loss: 26.900131225585938 
 batch loss: 27.02984619140625 
 batch loss: 26.915292739868164 
 batch loss: 26.840110778808594 
 batch loss: 26.866384506225586 
 batch loss: 26.64388656616211 
 batch loss: 26.61487579345703 
 batch loss: 26.791051864624023 
 batch loss: 26.95267105102539 
 batch loss: 26.634449005126953 
 batch loss: 26.50153350830078 
 batch loss: 26.503026962280273 
 batch loss: 26.619672775268555 
 batch loss: 26.43260955810547 
 batch loss: 26.514446258544922 
 batch loss: 26.40401840209961 
 batch loss: 26.299463272094727 
 batch loss: 26.170263290405273 
 batch loss: 26.170454025268555 
 batch loss: 26.050886154174805 
 batch loss: 26.15854263305664 
 batch loss: 26.342510223388672 
 batch loss: 26.312776565551758 
 batch loss: 25.954206466674805 
 batch loss: 25.866168975830078 
 batch loss: 26.044843673706055 
 batch loss: 25.916902542114258 
 batch loss: 25.918062210083008 
 batch loss: 25.73684310913086 
 batch loss: 26.030515670776367 
 batch loss: 25.672443389892578 
 batch loss: 25.70301055908203 
 batch loss: 25.58266830444336 
 batch loss: 25.75412368774414 
 batch loss: 25.54634666442871 
 batch loss: 25.55463218688965 
 batch loss: 25.724689483642578 
 batch loss: 25.464923858642578 
 batch loss: 25.573862075805664 
 batch loss: 25.436437606811523 
 batch loss: 25.50309944152832 
 batch loss: 25.7933349609375 
 batch loss: 25.392906188964844 
 batch loss: 25.486238479614258 
 batch loss: 25.519372940063477 
 batch loss: 25.457107543945312 
 batch loss: 25.279970169067383 
 batch loss: 25.225788116455078 
 batch loss: 25.157419204711914 
 batch loss: 25.07497787475586 
 batch loss: 25.20966148376465 
 batch loss: 25.294015884399414 
 batch loss: 25.10658073425293 
 batch loss: 25.059555053710938 
 batch loss: 25.080711364746094 
 batch loss: 24.935367584228516 
 batch loss: 24.83432960510254 
 batch loss: 24.895299911499023 
 batch loss: 24.9337158203125 
 batch loss: 24.9794864654541 
 batch loss: 24.980039596557617 
 batch loss: 24.918575286865234 
 batch loss: 24.940488815307617 
 batch loss: 24.894155502319336 
 batch loss: 24.89495277404785 
 batch loss: 25.034189224243164 
 batch loss: 24.90875816345215 
 batch loss: 24.84000587463379 
 batch loss: 24.96917152404785 
 batch loss: 24.96839141845703 
 batch loss: 25.18954849243164 
 batch loss: 24.805177688598633 
 batch loss: 24.82502555847168 
 batch loss: 25.182470321655273 
 batch loss: 25.07524299621582 
 batch loss: 24.966733932495117 
 Epoch 7 |Train loss: 26.345468044281006 |Validation loss: 24.9517879486084 
 batch loss: 24.94759750366211 
 batch loss: 24.75887107849121 
 batch loss: 25.05634307861328 
 batch loss: 24.80816650390625 
 batch loss: 24.761554718017578 
 batch loss: 24.763992309570312 
 batch loss: 24.708660125732422 
 batch loss: 24.67078399658203 
 batch loss: 24.812908172607422 
 batch loss: 24.54839324951172 
 batch loss: 24.783910751342773 
 batch loss: 24.70802116394043 
 batch loss: 24.746822357177734 
 batch loss: 24.482208251953125 
 batch loss: 24.597431182861328 
 batch loss: 24.597900390625 
 batch loss: 24.47142791748047 
 batch loss: 24.588394165039062 
 batch loss: 24.382076263427734 
 batch loss: 24.296688079833984 
 batch loss: 24.483057022094727 
 batch loss: 24.297985076904297 
 batch loss: 24.274686813354492 
 batch loss: 24.18168067932129 
 batch loss: 24.205224990844727 
 batch loss: 24.14621925354004 
 batch loss: 24.200458526611328 
 batch loss: 24.092529296875 
 batch loss: 24.923465728759766 
 batch loss: 24.43258285522461 
 batch loss: 25.148054122924805 
 batch loss: 24.602497100830078 
 batch loss: 24.53251075744629 
 batch loss: 24.472551345825195 
 batch loss: 24.3008975982666 
 batch loss: 24.490407943725586 
 batch loss: 24.351102828979492 
 batch loss: 24.33584976196289 
 batch loss: 24.131031036376953 
 batch loss: 24.180147171020508 
 batch loss: 24.09203338623047 
 batch loss: 23.99851417541504 
 batch loss: 24.156740188598633 
 batch loss: 23.946575164794922 
 batch loss: 23.791135787963867 
 batch loss: 24.102251052856445 
 batch loss: 23.703575134277344 
 batch loss: 23.876428604125977 
 batch loss: 23.679841995239258 
 batch loss: 23.561044692993164 
 batch loss: 23.603038787841797 
 batch loss: 23.491453170776367 
 batch loss: 23.720468521118164 
 batch loss: 23.360450744628906 
 batch loss: 23.650453567504883 
 batch loss: 23.266950607299805 
 batch loss: 23.2640323638916 
 batch loss: 23.190269470214844 
 batch loss: 23.356178283691406 
 batch loss: 23.038211822509766 
 batch loss: 23.218971252441406 
 batch loss: 23.09243392944336 
 batch loss: 22.955982208251953 
 batch loss: 22.99162483215332 
 batch loss: 22.993459701538086 
 batch loss: 22.92255210876465 
 batch loss: 22.852388381958008 
 batch loss: 22.78021812438965 
 batch loss: 22.76409149169922 
 batch loss: 22.653963088989258 
 batch loss: 22.757320404052734 
 batch loss: 22.539648056030273 
 batch loss: 22.928401947021484 
 batch loss: 22.482046127319336 
 batch loss: 22.70938491821289 
 batch loss: 22.47017478942871 
 batch loss: 22.55832290649414 
 batch loss: 22.438772201538086 
 batch loss: 22.238574981689453 
 batch loss: 22.271038055419922 
 batch loss: 22.260881423950195 
 batch loss: 22.260622024536133 
 batch loss: 22.379478454589844 
 batch loss: 22.457128524780273 
 batch loss: 22.235158920288086 
 batch loss: 22.462846755981445 
 batch loss: 22.363069534301758 
 batch loss: 22.56074333190918 
 batch loss: 22.307188034057617 
 batch loss: 22.420503616333008 
 batch loss: 22.421377182006836 
 batch loss: 22.29900360107422 
 batch loss: 22.310672760009766 
 batch loss: 22.34071159362793 
 batch loss: 22.576282501220703 
 batch loss: 22.307064056396484 
 batch loss: 22.181886672973633 
 batch loss: 22.438690185546875 
 batch loss: 22.475019454956055 
 batch loss: 22.345470428466797 
 Epoch 8 |Train loss: 23.859301352500914 |Validation loss: 22.370189952850343 
 batch loss: 22.163532257080078 
 batch loss: 22.44683837890625 
 batch loss: 22.26343536376953 
 batch loss: 22.722806930541992 
 batch loss: 22.425933837890625 
 batch loss: 22.228307723999023 
 batch loss: 22.389795303344727 
 batch loss: 22.202030181884766 
 batch loss: 22.369651794433594 
 batch loss: 22.175853729248047 
 batch loss: 22.081270217895508 
 batch loss: 22.169546127319336 
 batch loss: 22.14729881286621 
 batch loss: 21.96498680114746 
 batch loss: 21.9681453704834 
 batch loss: 22.017595291137695 
 batch loss: 21.943212509155273 
 batch loss: 21.93500518798828 
 batch loss: 21.950777053833008 
 batch loss: 21.822858810424805 
 batch loss: 21.760725021362305 
 batch loss: 21.83772850036621 
 batch loss: 21.75218963623047 
 batch loss: 21.739376068115234 
 batch loss: 21.81998634338379 
 batch loss: 21.732995986938477 
 batch loss: 21.65708351135254 
 batch loss: 21.617721557617188 
 batch loss: 21.59372329711914 
 batch loss: 21.476749420166016 
 batch loss: 21.827293395996094 
 batch loss: 21.476543426513672 
 batch loss: 21.67047882080078 
 batch loss: 21.83797836303711 
 batch loss: 21.41048812866211 
 batch loss: 21.43262481689453 
 batch loss: 21.391124725341797 
 batch loss: 21.531761169433594 
 batch loss: 21.342758178710938 
 batch loss: 21.583023071289062 
 batch loss: 21.34287452697754 
 batch loss: 21.460906982421875 
 batch loss: 21.317874908447266 
 batch loss: 21.27708625793457 
 batch loss: 21.260698318481445 
 batch loss: 21.27364158630371 
 batch loss: 21.235305786132812 
 batch loss: 21.195690155029297 
 batch loss: 21.185176849365234 
 batch loss: 21.132291793823242 
 batch loss: 21.142927169799805 
 batch loss: 21.174118041992188 
 batch loss: 21.166946411132812 
 batch loss: 21.25066566467285 
 batch loss: 21.29735565185547 
 batch loss: 21.197416305541992 
 batch loss: 21.043115615844727 
 batch loss: 21.060441970825195 
 batch loss: 20.96063995361328 
 batch loss: 20.95731544494629 
 batch loss: 21.05620002746582 
 batch loss: 21.0914363861084 
 batch loss: 21.070133209228516 
 batch loss: 20.918575286865234 
 batch loss: 20.98419761657715 
 batch loss: 20.908733367919922 
 batch loss: 20.96186065673828 
 batch loss: 20.898256301879883 
 batch loss: 21.059751510620117 
 batch loss: 20.87523651123047 
 batch loss: 20.801828384399414 
 batch loss: 20.93762969970703 
 batch loss: 20.80838394165039 
 batch loss: 20.823694229125977 
 batch loss: 20.87689971923828 
 batch loss: 20.733678817749023 
 batch loss: 20.747901916503906 
 batch loss: 20.775571823120117 
 batch loss: 20.72979736328125 
 batch loss: 20.876628875732422 
 batch loss: 20.652292251586914 
 batch loss: 20.751306533813477 
 batch loss: 20.666690826416016 
 batch loss: 20.646804809570312 
 batch loss: 20.808189392089844 
 batch loss: 20.655040740966797 
 batch loss: 20.68833351135254 
 batch loss: 20.79483985900879 
 batch loss: 20.654510498046875 
 batch loss: 20.688472747802734 
 batch loss: 20.62152862548828 
 batch loss: 20.758468627929688 
 batch loss: 20.670740127563477 
 batch loss: 20.607877731323242 
 batch loss: 20.798364639282227 
 batch loss: 20.691471099853516 
 batch loss: 20.868335723876953 
 batch loss: 20.71302032470703 
 batch loss: 20.837661743164062 
 batch loss: 20.91692352294922 
 Epoch 9 |Train loss: 21.47187650203705 |Validation loss: 20.7245436668396 
 batch loss: 20.690038681030273 
 batch loss: 20.63313102722168 
 batch loss: 20.79923439025879 
 batch loss: 20.760143280029297 
 batch loss: 20.680339813232422 
 batch loss: 20.815279006958008 
 batch loss: 20.597734451293945 
 batch loss: 20.579893112182617 
 batch loss: 20.618051528930664 
 batch loss: 20.8647518157959 
 batch loss: 20.530292510986328 
 batch loss: 20.556861877441406 
 batch loss: 20.58962059020996 
 batch loss: 20.585437774658203 
 batch loss: 20.49549102783203 
 batch loss: 20.74112892150879 
 batch loss: 20.486480712890625 
 batch loss: 20.464370727539062 
 batch loss: 20.469274520874023 
 batch loss: 20.608291625976562 
 batch loss: 20.486709594726562 
 batch loss: 20.67404556274414 
 batch loss: 20.486156463623047 
 batch loss: 20.308488845825195 
 batch loss: 20.33036994934082 
 batch loss: 20.312225341796875 
 batch loss: 20.38501739501953 
 batch loss: 20.34467124938965 
 batch loss: 20.237545013427734 
 batch loss: 20.25181770324707 
 batch loss: 20.258132934570312 
 batch loss: 20.26152229309082 
 batch loss: 20.27981948852539 
 batch loss: 20.221004486083984 
 batch loss: 20.220321655273438 
 batch loss: 20.069805145263672 
 batch loss: 20.05746078491211 
 batch loss: 20.018041610717773 
 batch loss: 20.078662872314453 
 batch loss: 20.05292320251465 
 batch loss: 20.04600715637207 
 batch loss: 20.062896728515625 
 batch loss: 19.958311080932617 
 batch loss: 19.96870231628418 
 batch loss: 20.009193420410156 
 batch loss: 19.997411727905273 
 batch loss: 19.90943145751953 
 batch loss: 20.145267486572266 
 batch loss: 20.02053451538086 
 batch loss: 19.98870086669922 
 batch loss: 19.821643829345703 
 batch loss: 19.8559513092041 
 batch loss: 19.895055770874023 
 batch loss: 19.945240020751953 
 batch loss: 19.886699676513672 
 batch loss: 19.80754852294922 
 batch loss: 19.850923538208008 
 batch loss: 19.7481689453125 
 batch loss: 19.820323944091797 
 batch loss: 19.811721801757812 
 batch loss: 20.02010154724121 
 batch loss: 19.8382511138916 
 batch loss: 19.822797775268555 
 batch loss: 19.74872589111328 
 batch loss: 19.780485153198242 
 batch loss: 19.8061580657959 
 batch loss: 19.70360565185547 
 batch loss: 19.71788787841797 
 batch loss: 19.683576583862305 
 batch loss: 19.62300682067871 
 batch loss: 19.6182861328125 
 batch loss: 19.632417678833008 
 batch loss: 19.603275299072266 
 batch loss: 19.551193237304688 
 batch loss: 19.536338806152344 
 batch loss: 19.636844635009766 
 batch loss: 19.55304718017578 
 batch loss: 19.650856018066406 
 batch loss: 19.623178482055664 
 batch loss: 19.59485626220703 
 batch loss: 19.522357940673828 
 batch loss: 19.575376510620117 
 batch loss: 19.533931732177734 
 batch loss: 19.583053588867188 
 batch loss: 19.53023338317871 
 batch loss: 19.619285583496094 
 batch loss: 19.505332946777344 
 batch loss: 19.515119552612305 
 batch loss: 19.494848251342773 
 batch loss: 19.475727081298828 
 batch loss: 19.549001693725586 
 batch loss: 19.538103103637695 
 batch loss: 19.48529815673828 
 batch loss: 19.597469329833984 
 batch loss: 19.490615844726562 
 batch loss: 19.455631256103516 
 batch loss: 19.510066986083984 
 batch loss: 19.537303924560547 
 batch loss: 19.483688354492188 
 batch loss: 19.565080642700195 
 Epoch 10 |Train loss: 20.127440166473388 |Validation loss: 19.528376293182372 
 batch loss: 19.47337532043457 
 batch loss: 19.497283935546875 
 batch loss: 19.502601623535156 
 batch loss: 19.46233367919922 
 batch loss: 19.425172805786133 
 batch loss: 19.540563583374023 
 batch loss: 19.390487670898438 
 batch loss: 19.365257263183594 
 batch loss: 19.33987808227539 
 batch loss: 19.36652946472168 
 batch loss: 19.405643463134766 
 batch loss: 19.33127212524414 
 batch loss: 19.351367950439453 
 batch loss: 19.306196212768555 
 batch loss: 19.283458709716797 
 batch loss: 19.255165100097656 
 batch loss: 19.281999588012695 
 batch loss: 19.24812889099121 
 batch loss: 19.23171615600586 
 batch loss: 19.23104476928711 
 batch loss: 19.190898895263672 
 batch loss: 19.27118492126465 
 batch loss: 19.299028396606445 
 batch loss: 19.141250610351562 
 batch loss: 19.134784698486328 
 batch loss: 19.179044723510742 
 batch loss: 19.133899688720703 
 batch loss: 19.261388778686523 
 batch loss: 19.11933135986328 
 batch loss: 19.205188751220703 
 batch loss: 19.188154220581055 
 batch loss: 19.086639404296875 
 batch loss: 19.15956687927246 
 batch loss: 19.069040298461914 
 batch loss: 19.07871437072754 
 batch loss: 18.955280303955078 
 batch loss: 18.9050235748291 
 batch loss: 18.85694694519043 
 batch loss: 18.833690643310547 
 batch loss: 18.801681518554688 
 batch loss: 18.808940887451172 
 batch loss: 18.809677124023438 
 batch loss: 18.748136520385742 
 batch loss: 18.818618774414062 
 batch loss: 18.72249984741211 
 batch loss: 18.65106964111328 
 batch loss: 18.620925903320312 
 batch loss: 18.832014083862305 
 batch loss: 18.556516647338867 
 batch loss: 18.66940689086914 
 batch loss: 18.684202194213867 
 batch loss: 18.794477462768555 
 batch loss: 18.525529861450195 
 batch loss: 18.541723251342773 
 batch loss: 18.518362045288086 
 batch loss: 18.48101806640625 
 batch loss: 18.552978515625 
 batch loss: 18.420394897460938 
 batch loss: 18.50887107849121 
 batch loss: 18.431684494018555 
 batch loss: 18.4130916595459 
 batch loss: 18.37553596496582 
 batch loss: 18.403959274291992 
 batch loss: 18.437829971313477 
 batch loss: 18.317697525024414 
 batch loss: 18.318038940429688 
 batch loss: 18.317581176757812 
 batch loss: 18.319246292114258 
 batch loss: 18.254117965698242 
 batch loss: 18.498716354370117 
 batch loss: 18.263023376464844 
 batch loss: 18.316368103027344 
 batch loss: 18.23798370361328 
 batch loss: 18.294313430786133 
 batch loss: 18.21024513244629 
 batch loss: 18.160484313964844 
 batch loss: 18.18343162536621 
 batch loss: 18.151195526123047 
 batch loss: 18.09707260131836 
 batch loss: 18.101999282836914 
 batch loss: 18.108945846557617 
 batch loss: 18.202259063720703 
 batch loss: 18.082782745361328 
 batch loss: 18.08452033996582 
 batch loss: 18.214292526245117 
 batch loss: 18.152456283569336 
 batch loss: 18.091428756713867 
 batch loss: 18.090225219726562 
 batch loss: 18.102218627929688 
 batch loss: 18.105783462524414 
 batch loss: 18.15176010131836 
 batch loss: 18.082014083862305 
 batch loss: 18.09069061279297 
 batch loss: 18.058605194091797 
 batch loss: 18.20332908630371 
 batch loss: 18.111160278320312 
 batch loss: 18.095369338989258 
 batch loss: 18.13330841064453 
 batch loss: 18.329452514648438 
 batch loss: 18.15424346923828 
 Epoch 11 |Train loss: 18.84411494731903 |Validation loss: 18.132242298126222 
 batch loss: 18.09090805053711 
 batch loss: 18.068979263305664 
 batch loss: 18.163286209106445 
 batch loss: 18.057838439941406 
 batch loss: 18.063528060913086 
 batch loss: 18.140127182006836 
 batch loss: 18.027414321899414 
 batch loss: 17.998504638671875 
 batch loss: 17.94271469116211 
 batch loss: 17.943065643310547 
 batch loss: 18.08922576904297 
 batch loss: 17.92205047607422 
 batch loss: 17.899730682373047 
 batch loss: 17.87001609802246 
 batch loss: 17.926950454711914 
 batch loss: 17.941844940185547 
 batch loss: 17.816606521606445 
 batch loss: 18.039113998413086 
 batch loss: 17.939762115478516 
 batch loss: 17.87498664855957 
 batch loss: 17.814966201782227 
 batch loss: 17.837533950805664 
 batch loss: 17.748291015625 
 batch loss: 17.751646041870117 
 batch loss: 17.732568740844727 
 batch loss: 17.784786224365234 
 batch loss: 17.7567138671875 
 batch loss: 17.641124725341797 
 batch loss: 17.705307006835938 
 batch loss: 17.674190521240234 
 batch loss: 17.627702713012695 
 batch loss: 17.625919342041016 
 batch loss: 17.64626121520996 
 batch loss: 17.63096809387207 
 batch loss: 17.734275817871094 
 batch loss: 17.57673454284668 
 batch loss: 17.655988693237305 
 batch loss: 17.571720123291016 
 batch loss: 17.5689697265625 
 batch loss: 17.524503707885742 
 batch loss: 17.568893432617188 
 batch loss: 17.454317092895508 
 batch loss: 17.409332275390625 
 batch loss: 17.44935417175293 
 batch loss: 17.456134796142578 
 batch loss: 17.506303787231445 
 batch loss: 17.407306671142578 
 batch loss: 17.36882972717285 
 batch loss: 17.350610733032227 
 batch loss: 17.40854263305664 
 batch loss: 17.2828311920166 
 batch loss: 17.29754066467285 
 batch loss: 17.349781036376953 
 batch loss: 17.246557235717773 
 batch loss: 17.29058265686035 
 batch loss: 17.240886688232422 
 batch loss: 17.160486221313477 
 batch loss: 17.281896591186523 
 batch loss: 17.176864624023438 
 batch loss: 17.118616104125977 
 batch loss: 17.188796997070312 
 batch loss: 17.078107833862305 
 batch loss: 17.04331398010254 
 batch loss: 17.152780532836914 
 batch loss: 17.088821411132812 
 batch loss: 17.06968879699707 
 batch loss: 17.107147216796875 
 batch loss: 16.98185157775879 
 batch loss: 17.00108528137207 
 batch loss: 16.999521255493164 
 batch loss: 16.991300582885742 
 batch loss: 16.962688446044922 
 batch loss: 16.91390609741211 
 batch loss: 16.946022033691406 
 batch loss: 16.894119262695312 
 batch loss: 16.883621215820312 
 batch loss: 16.85209083557129 
 batch loss: 16.87201690673828 
 batch loss: 16.82874870300293 
 batch loss: 16.793684005737305 
 batch loss: 16.90313720703125 
 batch loss: 16.819774627685547 
 batch loss: 16.890979766845703 
 batch loss: 16.798301696777344 
 batch loss: 16.84676170349121 
 batch loss: 16.845718383789062 
 batch loss: 16.829843521118164 
 batch loss: 16.898529052734375 
 batch loss: 16.85871696472168 
 batch loss: 16.777328491210938 
 batch loss: 16.86695671081543 
 batch loss: 16.873456954956055 
 batch loss: 16.845972061157227 
 batch loss: 16.853893280029297 
 batch loss: 16.885318756103516 
 batch loss: 16.762516021728516 
 batch loss: 16.79429817199707 
 batch loss: 16.83498764038086 
 batch loss: 16.811843872070312 
 batch loss: 16.966773986816406 
 Epoch 12 |Train loss: 17.49877259731293 |Validation loss: 16.848255443573 
 batch loss: 16.928739547729492 
 batch loss: 16.910907745361328 
 batch loss: 16.77667999267578 
 batch loss: 16.74245262145996 
 batch loss: 16.773548126220703 
 batch loss: 16.73012924194336 
 batch loss: 16.755598068237305 
 batch loss: 16.638519287109375 
 batch loss: 16.73466682434082 
 batch loss: 16.715730667114258 
 batch loss: 16.600563049316406 
 batch loss: 16.72849464416504 
 batch loss: 16.6248722076416 
 batch loss: 16.64772605895996 
 batch loss: 16.655380249023438 
 batch loss: 16.573719024658203 
 batch loss: 16.61009407043457 
 batch loss: 16.4713191986084 
 batch loss: 16.570079803466797 
 batch loss: 16.488540649414062 
 batch loss: 16.448406219482422 
 batch loss: 16.495101928710938 
 batch loss: 16.414836883544922 
 batch loss: 16.443096160888672 
 batch loss: 16.370563507080078 
 batch loss: 16.394561767578125 
 batch loss: 16.382293701171875 
 batch loss: 16.35455322265625 
 batch loss: 16.425867080688477 
 batch loss: 16.331729888916016 
 batch loss: 16.420289993286133 
 batch loss: 16.407970428466797 
 batch loss: 16.3847713470459 
 batch loss: 16.36367416381836 
 batch loss: 16.4168758392334 
 batch loss: 16.3243465423584 
 batch loss: 16.262744903564453 
 batch loss: 16.23945426940918 
 batch loss: 16.295751571655273 
 batch loss: 16.252132415771484 
 batch loss: 16.400287628173828 
 batch loss: 16.27065086364746 
 batch loss: 16.208433151245117 
 batch loss: 16.22623062133789 
 batch loss: 16.153684616088867 
 batch loss: 16.18043327331543 
 batch loss: 16.124706268310547 
 batch loss: 16.175935745239258 
 batch loss: 16.21944808959961 
 batch loss: 16.13558578491211 
 batch loss: 16.108600616455078 
 batch loss: 16.068416595458984 
 batch loss: 16.03972053527832 
 batch loss: 16.107728958129883 
 batch loss: 16.180307388305664 
 batch loss: 16.084922790527344 
 batch loss: 16.044157028198242 
 batch loss: 16.058059692382812 
 batch loss: 15.983720779418945 
 batch loss: 15.976186752319336 
 batch loss: 16.106531143188477 
 batch loss: 16.037817001342773 
 batch loss: 16.01970100402832 
 batch loss: 16.003536224365234 
 batch loss: 15.953752517700195 
 batch loss: 16.091861724853516 
 batch loss: 15.998682022094727 
 batch loss: 15.895127296447754 
 batch loss: 15.925209999084473 
 batch loss: 15.912039756774902 
 batch loss: 15.952322006225586 
 batch loss: 15.94753360748291 
 batch loss: 15.843456268310547 
 batch loss: 15.823878288269043 
 batch loss: 15.872232437133789 
 batch loss: 15.936553955078125 
 batch loss: 15.861136436462402 
 batch loss: 15.865653991699219 
 batch loss: 15.769482612609863 
 batch loss: 15.786309242248535 
 batch loss: 15.768692016601562 
 batch loss: 15.794686317443848 
 batch loss: 15.768159866333008 
 batch loss: 15.787867546081543 
 batch loss: 15.753002166748047 
 batch loss: 15.79361343383789 
 batch loss: 15.86099910736084 
 batch loss: 15.814855575561523 
 batch loss: 15.795405387878418 
 batch loss: 15.949560165405273 
 batch loss: 15.781941413879395 
 batch loss: 15.775066375732422 
 batch loss: 15.743663787841797 
 batch loss: 15.766728401184082 
 batch loss: 15.770113945007324 
 batch loss: 15.74434757232666 
 batch loss: 15.76878833770752 
 batch loss: 15.802557945251465 
 batch loss: 15.7781400680542 
 batch loss: 15.72688102722168 
 Epoch 13 |Train loss: 16.280710220336914 |Validation loss: 15.787253522872925 
 batch loss: 15.709081649780273 
 batch loss: 15.779728889465332 
 batch loss: 15.817901611328125 
 batch loss: 15.732694625854492 
 batch loss: 15.72218132019043 
 batch loss: 15.74838638305664 
 batch loss: 15.688085556030273 
 batch loss: 15.722491264343262 
 batch loss: 15.662774085998535 
 batch loss: 15.628872871398926 
 batch loss: 15.709431648254395 
 batch loss: 15.688482284545898 
 batch loss: 15.62812614440918 
 batch loss: 15.630780220031738 
 batch loss: 15.61337661743164 
 batch loss: 15.672866821289062 
 batch loss: 15.556279182434082 
 batch loss: 15.543522834777832 
 batch loss: 15.539249420166016 
 batch loss: 15.519469261169434 
 batch loss: 15.514029502868652 
 batch loss: 15.472570419311523 
 batch loss: 15.466251373291016 
 batch loss: 15.486531257629395 
 batch loss: 15.540474891662598 
 batch loss: 15.487610816955566 
 batch loss: 15.609774589538574 
 batch loss: 15.462461471557617 
 batch loss: 15.447940826416016 
 batch loss: 15.443188667297363 
 batch loss: 15.49447250366211 
 batch loss: 15.434341430664062 
 batch loss: 15.507797241210938 
 batch loss: 15.381253242492676 
 batch loss: 15.473187446594238 
 batch loss: 15.364728927612305 
 batch loss: 15.433599472045898 
 batch loss: 15.344053268432617 
 batch loss: 15.435437202453613 
 batch loss: 15.340842247009277 
 batch loss: 15.350783348083496 
 batch loss: 15.327898025512695 
 batch loss: 15.329353332519531 
 batch loss: 15.36915111541748 
 batch loss: 15.327984809875488 
 batch loss: 15.336859703063965 
 batch loss: 15.277957916259766 
 batch loss: 15.241957664489746 
 batch loss: 15.267146110534668 
 batch loss: 15.278149604797363 
 batch loss: 15.19503402709961 
 batch loss: 15.240205764770508 
 batch loss: 15.222527503967285 
 batch loss: 15.291358947753906 
 batch loss: 15.172171592712402 
 batch loss: 15.168163299560547 
 batch loss: 15.188759803771973 
 batch loss: 15.138554573059082 
 batch loss: 15.19367790222168 
 batch loss: 15.194620132446289 
 batch loss: 15.133333206176758 
 batch loss: 15.11985969543457 
 batch loss: 15.111186981201172 
 batch loss: 15.122291564941406 
 batch loss: 15.120420455932617 
 batch loss: 15.075484275817871 
 batch loss: 15.051922798156738 
 batch loss: 15.096111297607422 
 batch loss: 15.038168907165527 
 batch loss: 15.022796630859375 
 batch loss: 14.991652488708496 
 batch loss: 14.974422454833984 
 batch loss: 15.017208099365234 
 batch loss: 14.976821899414062 
 batch loss: 15.032707214355469 
 batch loss: 14.950439453125 
 batch loss: 14.95425796508789 
 batch loss: 14.957810401916504 
 batch loss: 14.96548843383789 
 batch loss: 15.007694244384766 
 batch loss: 14.896076202392578 
 batch loss: 14.921138763427734 
 batch loss: 14.92811393737793 
 batch loss: 14.962042808532715 
 batch loss: 14.935759544372559 
 batch loss: 14.941428184509277 
 batch loss: 14.98490047454834 
 batch loss: 14.929976463317871 
 batch loss: 14.955114364624023 
 batch loss: 15.03777027130127 
 batch loss: 14.9489107131958 
 batch loss: 14.962366104125977 
 batch loss: 14.916808128356934 
 batch loss: 14.901301383972168 
 batch loss: 14.939676284790039 
 batch loss: 14.916893005371094 
 batch loss: 14.919610977172852 
 batch loss: 14.905673027038574 
 batch loss: 14.954675674438477 
 batch loss: 14.923003196716309 
 Epoch 14 |Train loss: 15.353584039211274 |Validation loss: 14.939061975479126 
 batch loss: 14.924837112426758 
 batch loss: 14.916854858398438 
 batch loss: 14.904969215393066 
 batch loss: 14.901403427124023 
 batch loss: 14.917534828186035 
 batch loss: 14.909674644470215 
 batch loss: 14.879705429077148 
 batch loss: 14.875783920288086 
 batch loss: 14.951878547668457 
 batch loss: 14.839628219604492 
 batch loss: 14.850099563598633 
 batch loss: 14.834366798400879 
 batch loss: 14.860404968261719 
 batch loss: 14.785053253173828 
 batch loss: 14.830540657043457 
 batch loss: 14.798855781555176 
 batch loss: 14.782235145568848 
 batch loss: 14.770545959472656 
 batch loss: 14.746195793151855 
 batch loss: 14.724601745605469 
 batch loss: 14.720207214355469 
 batch loss: 14.725351333618164 
 batch loss: 14.884902000427246 
 batch loss: 14.701931953430176 
 batch loss: 14.727699279785156 
 batch loss: 14.749002456665039 
 batch loss: 14.684998512268066 
 batch loss: 14.683574676513672 
 batch loss: 14.667326927185059 
 batch loss: 14.676971435546875 
 batch loss: 14.681289672851562 
 batch loss: 14.635415077209473 
 batch loss: 14.62712574005127 
 batch loss: 14.607741355895996 
 batch loss: 14.594733238220215 
 batch loss: 14.606067657470703 
 batch loss: 14.673440933227539 
 batch loss: 14.592772483825684 
 batch loss: 14.550043106079102 
 batch loss: 14.54526424407959 
 batch loss: 14.579843521118164 
 batch loss: 14.52334976196289 
 batch loss: 14.575481414794922 
 batch loss: 14.488085746765137 
 batch loss: 14.492498397827148 
 batch loss: 14.498812675476074 
 batch loss: 14.455035209655762 
 batch loss: 14.459931373596191 
 batch loss: 14.436694145202637 
 batch loss: 14.426154136657715 
 batch loss: 14.3917236328125 
 batch loss: 14.428267478942871 
 batch loss: 14.409616470336914 
 batch loss: 14.408499717712402 
 batch loss: 14.373905181884766 
 batch loss: 14.384696006774902 
 batch loss: 14.37601375579834 
 batch loss: 14.329142570495605 
 batch loss: 14.29304313659668 
 batch loss: 14.329058647155762 
 batch loss: 14.301738739013672 
 batch loss: 14.339468955993652 
 batch loss: 14.30792236328125 
 batch loss: 14.299236297607422 
 batch loss: 14.297857284545898 
 batch loss: 14.356551170349121 
 batch loss: 14.334442138671875 
 batch loss: 14.327712059020996 
 batch loss: 14.290724754333496 
 batch loss: 14.283493041992188 
 batch loss: 14.308259010314941 
 batch loss: 14.305325508117676 
 batch loss: 14.255797386169434 
 batch loss: 14.297751426696777 
 batch loss: 14.235424041748047 
 batch loss: 14.217538833618164 
 batch loss: 14.201420783996582 
 batch loss: 14.185087203979492 
 batch loss: 14.1948881149292 
 batch loss: 14.166499137878418 
 batch loss: 14.138733863830566 
 batch loss: 14.119789123535156 
 batch loss: 14.126352310180664 
 batch loss: 14.095549583435059 
 batch loss: 14.151890754699707 
 batch loss: 14.136170387268066 
 batch loss: 14.127667427062988 
 batch loss: 14.123722076416016 
 batch loss: 14.128595352172852 
 batch loss: 14.118605613708496 
 batch loss: 14.13101577758789 
 batch loss: 14.117616653442383 
 batch loss: 14.123762130737305 
 batch loss: 14.116707801818848 
 batch loss: 14.089930534362793 
 batch loss: 14.124757766723633 
 batch loss: 14.14877700805664 
 batch loss: 14.15807056427002 
 batch loss: 14.263608932495117 
 batch loss: 14.140589714050293 
 Epoch 15 |Train loss: 14.556350255012513 |Validation loss: 14.134095668792725 
 batch loss: 14.154245376586914 
 batch loss: 14.165054321289062 
 batch loss: 14.133398056030273 
 batch loss: 14.07870864868164 
 batch loss: 14.080538749694824 
 batch loss: 14.091684341430664 
 batch loss: 14.085762977600098 
 batch loss: 14.061934471130371 
 batch loss: 14.145466804504395 
 batch loss: 14.050390243530273 
 batch loss: 14.02313232421875 
 batch loss: 14.0199613571167 
 batch loss: 14.020142555236816 
 batch loss: 13.982352256774902 
 batch loss: 13.987605094909668 
 batch loss: 13.989365577697754 
 batch loss: 13.960866928100586 
 batch loss: 13.945348739624023 
 batch loss: 13.917830467224121 
 batch loss: 13.932838439941406 
 batch loss: 13.92996883392334 
 batch loss: 13.939295768737793 
 batch loss: 13.912700653076172 
 batch loss: 13.874349594116211 
 batch loss: 13.887250900268555 
 batch loss: 13.876501083374023 
 batch loss: 13.899154663085938 
 batch loss: 13.880632400512695 
 batch loss: 13.82702350616455 
 batch loss: 13.840126037597656 
 batch loss: 13.841544151306152 
 batch loss: 13.850171089172363 
 batch loss: 13.839127540588379 
 batch loss: 13.853611946105957 
 batch loss: 13.804396629333496 
 batch loss: 13.80064868927002 
 batch loss: 13.822897911071777 
 batch loss: 13.782018661499023 
 batch loss: 13.747845649719238 
 batch loss: 13.79985237121582 
 batch loss: 13.75805377960205 
 batch loss: 13.77447509765625 
 batch loss: 13.757613182067871 
 batch loss: 13.747143745422363 
 batch loss: 13.748205184936523 
 batch loss: 13.730446815490723 
 batch loss: 13.71939468383789 
 batch loss: 13.707337379455566 
 batch loss: 13.682272911071777 
 batch loss: 13.683629035949707 
 batch loss: 13.676547050476074 
 batch loss: 13.689443588256836 
 batch loss: 13.875469207763672 
 batch loss: 13.720941543579102 
 batch loss: 13.819246292114258 
 batch loss: 13.73835563659668 
 batch loss: 13.789886474609375 
 batch loss: 13.818367958068848 
 batch loss: 13.773263931274414 
 batch loss: 13.76471996307373 
 batch loss: 13.817288398742676 
 batch loss: 13.762711524963379 
 batch loss: 13.702208518981934 
 batch loss: 13.709181785583496 
 batch loss: 13.708858489990234 
 batch loss: 13.650506019592285 
 batch loss: 13.6522216796875 
 batch loss: 13.6359224319458 
 batch loss: 13.697172164916992 
 batch loss: 13.636941909790039 
 batch loss: 13.628189086914062 
 batch loss: 13.62229061126709 
 batch loss: 13.636236190795898 
 batch loss: 13.596641540527344 
 batch loss: 13.55488109588623 
 batch loss: 13.565445899963379 
 batch loss: 13.587608337402344 
 batch loss: 13.55460262298584 
 batch loss: 13.531310081481934 
 batch loss: 13.514250755310059 
 batch loss: 13.493585586547852 
 batch loss: 13.54916763305664 
 batch loss: 13.5007905960083 
 batch loss: 13.492863655090332 
 batch loss: 13.517943382263184 
 batch loss: 13.52757740020752 
 batch loss: 13.531107902526855 
 batch loss: 13.529853820800781 
 batch loss: 13.537919044494629 
 batch loss: 13.485391616821289 
 batch loss: 13.461752891540527 
 batch loss: 13.498617172241211 
 batch loss: 13.559684753417969 
 batch loss: 13.51152515411377 
 batch loss: 13.55179500579834 
 batch loss: 13.530062675476074 
 batch loss: 13.54641342163086 
 batch loss: 13.490243911743164 
 batch loss: 13.524263381958008 
 batch loss: 13.507240295410156 
 Epoch 16 |Train loss: 13.819687855243682 |Validation loss: 13.517389965057372 
 batch loss: 13.5112886428833 
 batch loss: 13.506206512451172 
 batch loss: 13.52295970916748 
 batch loss: 13.539093971252441 
 batch loss: 13.517560958862305 
 batch loss: 13.508354187011719 
 batch loss: 13.495563507080078 
 batch loss: 13.464624404907227 
 batch loss: 13.458799362182617 
 batch loss: 13.433236122131348 
 batch loss: 13.428445816040039 
 batch loss: 13.440465927124023 
 batch loss: 13.426751136779785 
 batch loss: 13.442180633544922 
 batch loss: 13.420052528381348 
 batch loss: 13.488993644714355 
 batch loss: 13.43808364868164 
 batch loss: 13.411723136901855 
 batch loss: 13.424038887023926 
 batch loss: 13.392204284667969 
 batch loss: 13.444560050964355 
 batch loss: 13.359883308410645 
 batch loss: 13.36336898803711 
 batch loss: 13.358235359191895 
 batch loss: 13.456130981445312 
 batch loss: 13.363533020019531 
 batch loss: 13.372485160827637 
 batch loss: 13.27716064453125 
 batch loss: 13.330631256103516 
 batch loss: 13.324664115905762 
 batch loss: 13.325308799743652 
 batch loss: 13.294179916381836 
 batch loss: 13.316593170166016 
 batch loss: 13.271549224853516 
 batch loss: 13.26340103149414 
 batch loss: 13.248663902282715 
 batch loss: 13.243627548217773 
 batch loss: 13.260504722595215 
 batch loss: 13.251498222351074 
 batch loss: 13.253657341003418 
 batch loss: 13.200671195983887 
 batch loss: 13.199978828430176 
 batch loss: 13.197685241699219 
 batch loss: 13.313762664794922 
 batch loss: 13.176111221313477 
 batch loss: 13.190412521362305 
 batch loss: 13.270264625549316 
 batch loss: 13.155902862548828 
 batch loss: 13.155366897583008 
 batch loss: 13.187712669372559 
 batch loss: 13.118257522583008 
 batch loss: 13.130836486816406 
 batch loss: 13.062318801879883 
 batch loss: 13.039628028869629 
 batch loss: 13.00540542602539 
 batch loss: 13.060210227966309 
 batch loss: 12.953938484191895 
 batch loss: 12.912874221801758 
 batch loss: 12.898383140563965 
 batch loss: 12.901887893676758 
 batch loss: 12.842377662658691 
 batch loss: 12.815112113952637 
 batch loss: 12.811113357543945 
 batch loss: 12.737833976745605 
 batch loss: 12.712069511413574 
 batch loss: 12.701314926147461 
 batch loss: 12.690645217895508 
 batch loss: 12.684517860412598 
 batch loss: 12.580554008483887 
 batch loss: 12.50925350189209 
 batch loss: 12.474489212036133 
 batch loss: 12.458817481994629 
 batch loss: 12.453397750854492 
 batch loss: 12.375082015991211 
 batch loss: 12.357870101928711 
 batch loss: 12.316040992736816 
 batch loss: 12.315199851989746 
 batch loss: 12.273733139038086 
 batch loss: 12.275341033935547 
 batch loss: 12.238551139831543 
 batch loss: 12.232928276062012 
 batch loss: 12.282000541687012 
 batch loss: 12.221041679382324 
 batch loss: 12.249329566955566 
 batch loss: 12.241625785827637 
 batch loss: 12.25320816040039 
 batch loss: 12.259842872619629 
 batch loss: 12.294145584106445 
 batch loss: 12.31446647644043 
 batch loss: 12.373340606689453 
 batch loss: 12.289379119873047 
 batch loss: 12.26684284210205 
 batch loss: 12.269780158996582 
 batch loss: 12.228277206420898 
 batch loss: 12.216588020324707 
 batch loss: 12.346415519714355 
 batch loss: 12.215733528137207 
 batch loss: 12.304468154907227 
 batch loss: 12.290630340576172 
 batch loss: 12.229028701782227 
 Epoch 17 |Train loss: 13.105064845085144 |Validation loss: 12.268953657150268 
 batch loss: 12.334251403808594 
 batch loss: 12.223441123962402 
 batch loss: 12.21103286743164 
 batch loss: 12.210282325744629 
 batch loss: 12.160370826721191 
 batch loss: 12.155071258544922 
 batch loss: 12.14599323272705 
 batch loss: 12.128623962402344 
 batch loss: 12.161478996276855 
 batch loss: 12.145851135253906 
 batch loss: 12.149553298950195 
 batch loss: 12.134953498840332 
 batch loss: 12.126105308532715 
 batch loss: 12.12570571899414 
 batch loss: 12.129075050354004 
 batch loss: 12.05944538116455 
 batch loss: 12.037211418151855 
 batch loss: 12.050219535827637 
 batch loss: 12.03100872039795 
 batch loss: 12.095869064331055 
 batch loss: 12.047867774963379 
 batch loss: 12.054532051086426 
 batch loss: 11.91651439666748 
 batch loss: 11.923569679260254 
 batch loss: 11.91551399230957 
 batch loss: 11.840797424316406 
 batch loss: 11.864109992980957 
 batch loss: 11.81525993347168 
 batch loss: 11.824760437011719 
 batch loss: 11.761809349060059 
 batch loss: 11.762104988098145 
 batch loss: 11.702841758728027 
 batch loss: 11.683328628540039 
 batch loss: 11.700189590454102 
 batch loss: 11.674277305603027 
 batch loss: 11.672977447509766 
 batch loss: 11.66585636138916 
 batch loss: 11.656105041503906 
 batch loss: 11.620417594909668 
 batch loss: 11.648530960083008 
 batch loss: 11.614203453063965 
 batch loss: 11.616597175598145 
 batch loss: 11.594042778015137 
 batch loss: 11.600797653198242 
 batch loss: 11.580202102661133 
 batch loss: 11.564729690551758 
 batch loss: 11.5459623336792 
 batch loss: 11.538294792175293 
 batch loss: 11.5151948928833 
 batch loss: 11.495088577270508 
 batch loss: 11.596964836120605 
 batch loss: 11.46551513671875 
 batch loss: 11.472962379455566 
 batch loss: 11.40150260925293 
 batch loss: 11.441864013671875 
 batch loss: 11.343419075012207 
 batch loss: 11.348419189453125 
 batch loss: 11.438114166259766 
 batch loss: 11.336792945861816 
 batch loss: 11.305319786071777 
 batch loss: 11.322365760803223 
 batch loss: 11.325727462768555 
 batch loss: 11.3297119140625 
 batch loss: 11.364846229553223 
 batch loss: 11.300167083740234 
 batch loss: 11.271066665649414 
 batch loss: 11.266474723815918 
 batch loss: 11.288409233093262 
 batch loss: 11.21867561340332 
 batch loss: 11.244303703308105 
 batch loss: 11.188508987426758 
 batch loss: 11.210309028625488 
 batch loss: 11.169648170471191 
 batch loss: 11.14384651184082 
 batch loss: 11.16171932220459 
 batch loss: 11.150943756103516 
 batch loss: 11.14046859741211 
 batch loss: 11.139344215393066 
 batch loss: 11.119131088256836 
 batch loss: 11.090508460998535 
 batch loss: 11.07981014251709 
 batch loss: 11.10461139678955 
 batch loss: 11.100774765014648 
 batch loss: 11.0736083984375 
 batch loss: 11.077509880065918 
 batch loss: 11.07494831085205 
 batch loss: 11.090180397033691 
 batch loss: 11.118102073669434 
 batch loss: 11.101727485656738 
 batch loss: 11.097908020019531 
 batch loss: 11.087336540222168 
 batch loss: 11.133902549743652 
 batch loss: 11.10590934753418 
 batch loss: 11.079086303710938 
 batch loss: 11.109145164489746 
 batch loss: 11.054378509521484 
 batch loss: 11.069781303405762 
 batch loss: 11.08487606048584 
 batch loss: 11.09512996673584 
 batch loss: 11.093669891357422 
 Epoch 18 |Train loss: 11.660363411903381 |Validation loss: 11.091619825363159 
 batch loss: 11.060050964355469 
 batch loss: 11.067992210388184 
 batch loss: 11.085606575012207 
 batch loss: 11.05810546875 
 batch loss: 11.01427936553955 
 batch loss: 11.038233757019043 
 batch loss: 11.00799560546875 
 batch loss: 11.029509544372559 
 batch loss: 10.975594520568848 
 batch loss: 10.974883079528809 
 batch loss: 10.956432342529297 
 batch loss: 10.973201751708984 
 batch loss: 10.939271926879883 
 batch loss: 10.947779655456543 
 batch loss: 10.95640754699707 
 batch loss: 10.911275863647461 
 batch loss: 10.923830032348633 
 batch loss: 10.889280319213867 
 batch loss: 10.907424926757812 
 batch loss: 10.87611198425293 
 batch loss: 10.888447761535645 
 batch loss: 10.871593475341797 
 batch loss: 10.855964660644531 
 batch loss: 10.854704856872559 
 batch loss: 10.840502738952637 
 batch loss: 10.816306114196777 
 batch loss: 10.834484100341797 
 batch loss: 10.822017669677734 
 batch loss: 10.821331977844238 
 batch loss: 10.809717178344727 
 batch loss: 10.770791053771973 
 batch loss: 10.823607444763184 
 batch loss: 10.755027770996094 
 batch loss: 10.75306510925293 
 batch loss: 10.814291000366211 
 batch loss: 10.744216918945312 
 batch loss: 10.712066650390625 
 batch loss: 10.704900741577148 
 batch loss: 10.752774238586426 
 batch loss: 10.686198234558105 
 batch loss: 10.688172340393066 
 batch loss: 10.68204402923584 
 batch loss: 10.693689346313477 
 batch loss: 10.683854103088379 
 batch loss: 10.662631034851074 
 batch loss: 10.65458869934082 
 batch loss: 10.695026397705078 
 batch loss: 10.644829750061035 
 batch loss: 10.675573348999023 
 batch loss: 10.646672248840332 
 batch loss: 10.620800018310547 
 batch loss: 10.622690200805664 
 batch loss: 10.621183395385742 
 batch loss: 10.583657264709473 
 batch loss: 10.580901145935059 
 batch loss: 10.590171813964844 
 batch loss: 10.564793586730957 
 batch loss: 10.575153350830078 
 batch loss: 10.568281173706055 
 batch loss: 10.550602912902832 
 batch loss: 10.531787872314453 
 batch loss: 10.541479110717773 
 batch loss: 10.531672477722168 
 batch loss: 10.515120506286621 
 batch loss: 10.486417770385742 
 batch loss: 10.500533103942871 
 batch loss: 10.457682609558105 
 batch loss: 10.449113845825195 
 batch loss: 10.466827392578125 
 batch loss: 10.446550369262695 
 batch loss: 10.457413673400879 
 batch loss: 10.428221702575684 
 batch loss: 10.458722114562988 
 batch loss: 10.425914764404297 
 batch loss: 10.409820556640625 
 batch loss: 10.409872055053711 
 batch loss: 10.417061805725098 
 batch loss: 10.390357971191406 
 batch loss: 10.371049880981445 
 batch loss: 10.367770195007324 
 batch loss: 10.346051216125488 
 batch loss: 10.363261222839355 
 batch loss: 10.342663764953613 
 batch loss: 10.338252067565918 
 batch loss: 10.369219779968262 
 batch loss: 10.367402076721191 
 batch loss: 10.351264953613281 
 batch loss: 10.367298126220703 
 batch loss: 10.378432273864746 
 batch loss: 10.343255996704102 
 batch loss: 10.334053039550781 
 batch loss: 10.340211868286133 
 batch loss: 10.346226692199707 
 batch loss: 10.35973834991455 
 batch loss: 10.376358032226562 
 batch loss: 10.352896690368652 
 batch loss: 10.343745231628418 
 batch loss: 10.362813949584961 
 batch loss: 10.384796142578125 
 batch loss: 10.345235824584961 
 Epoch 19 |Train loss: 10.714924788475036 |Validation loss: 10.355658864974975 
